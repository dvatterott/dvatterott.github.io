<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Dan Vatterott]]></title>
  <link href="https://danvatterott.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="https://danvatterott.com/"/>
  <updated>2022-02-25T14:33:11-06:00</updated>
  <id>https://danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Posting Collections as Hive Tables]]></title>
    <link href="https://danvatterott.com/blog/2020/08/10/posting-collections-as-hive-tables/"/>
    <updated>2020-08-10T20:03:43-05:00</updated>
    <id>https://danvatterott.com/blog/2020/08/10/posting-collections-as-hive-tables</id>
    <content type="html"><![CDATA[<p>I was recently asked to post a series of parquet collection as tables so analysts could query them in SQL. This should be straight forward, but it took me awhile to figure out. Hopefully, you find this post before spending too much time on such an easy task.</p>

<p>You should use the <a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-table.html"><code>CREATE TABLE</code></a>. This is pretty straight forward. By creating a permanent table (rather than a temp table), you can use a database name. Also, by using a table (rather than  a view), you can load the data from an s3 location.</p>

<p>Next, you can specify the table’s schema. Again, this is pretty straight forward. Columns used to partition the data should be declared here.</p>

<p>Next, you can specify how the data is stored (below, I use Parquet) and how the data is partitioned (below, there are two partitioning columns).</p>

<p>Finally, you specify the data’s location.</p>

<p>The part that really threw me for a loop here is that I wasn’t done yet! You need one more command so that Spark can go examine the partitions - <a href="https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-repair-table.html"><code>MSCK REPAIR TABLE</code></a>. Also please note that this command needs to be re-run whenever a partition is added.</p>

<p>{% codeblock lang:python %}
spark.sql(“””
CREATE TABLE my_db.my_table (
(example_key INT, example_col STRING, example_string STRING, example_date STRING)
)
USING PARQUET
PARTITIONED BY (example_string, example_date)
LOCATION ‘s3://my.example.bucket/my_db/my_table/’
“””
spark.sql(“MSCK REPAIR TABLE my_db.my_table”)
{% endcodeblock %}</p>

<p>Hope this post saves you some time!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Balancing Model Weights in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark/"/>
    <updated>2019-11-18T18:57:03-06:00</updated>
    <id>https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark</id>
    <content type="html"><![CDATA[<p><a href="https://www.jeremyjordan.me/imbalanced-data/">Imbalanced classes</a> is a common problem. Scikit-learn provides an easy fix - “balancing” class weights. This makes models more likely to predict the less common classes (e.g., <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">logistic regression</a>).</p>

<p>The PySpark ML API doesn’t have this same functionality, so in this blog post, I describe how to balance class weights yourself.</p>

<p>{% codeblock lang:python %}
import numpy as np
import pandas as pd
from itertools import chain
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Generate some random data and put the data in a Spark DataFrame. Note that the input variables are not predictive. The model will behave randomly. This is okay, since I am not interested in model accuracy.</p>

<p>{% codeblock lang:python %}
X = np.random.normal(0, 1, (10000, 10))</p>

<p>y = np.ones(X.shape[0]).astype(int)
y[:1000] = 0
np.random.shuffle(y)</p>

<p>print(np.mean(y)) # 0.9</p>

<p>X = np.append(X, y.reshape((10000, 1)), 1)</p>

<p>DF = spark.createDataFrame(pd.DataFrame(X))
DF = DF.withColumnRenamed(“10”, “y”)
{% endcodeblock %}</p>

<p>Here’s how Scikit-learn computes class weights when “balanced” weights are requested.</p>

<p>{% codeblock lang:python %}
# class weight
# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
# n_samples / (n_classes * np.bincount(y)).</p>

<p>class_weights = {i: ii for i, ii in zip(np.unique(y), len(y) / (len(np.unique(y)) * np.bincount(y)))}
print(class_weights) # {0: 5.0, 1: 0.5555555555555556}
{% endcodeblock %}</p>

<p>Here’s how we can compute “balanced” weights with data from a PySpark DataFrame.</p>

<p>{% codeblock lang:python %}
y_collect = DF.select(“y”).groupBy(“y”).count().collect()
unique_y = [x[“y”] for x in y_collect]
total_y = sum([x[“count”] for x in y_collect])
unique_y_count = len(y_collect)
bin_count = [x[“count”] for x in y_collect]</p>

<p>class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}
print(class_weights_spark) # {0.0: 5.0, 1.0: 0.5555555555555556}
{% endcodeblock %}</p>

<p>PySpark needs to have a weight assigned to each instance (i.e., row) in the training set. I create a mapping to apply a weight to each training instance.</p>

<p>{% codeblock lang:python %}
mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])</p>

<p>DF = DF.withColumn(“weight”, mapping_expr.getItem(F.col(“y”)))
{% endcodeblock %}</p>

<p>I assemble all the input features into a vector.</p>

<p>{% codeblock lang:python %}
assembler = VectorAssembler(inputCols=[str(x) for x in range(10)], outputCol=”features”)</p>

<p>DF = assembler.transform(DF).drop(*[str(x) for x in range(10)])
{% endcodeblock %}</p>

<p>And train a logistic regression. Without the instance weights, the model predicts all instances as the frequent class.</p>

<p>{% codeblock lang:python %}
lr = LogisticRegression(featuresCol=”features”, labelCol=”y”)
lrModel = lr.fit(DF)
lrModel.transform(DF).agg(F.mean(“prediction”)).show()
{% endcodeblock %}</p>

<pre><code>+---------------+
|avg(prediction)|
+---------------+
|            1.0|
+---------------+
</code></pre>

<p>With the weights, the model assigns half the instances to each class (even the less commmon one).</p>

<p>{% codeblock lang:python %}
lr = LogisticRegression(featuresCol=”features”, labelCol=”y”, weightCol=”weight”)
lrModel = lr.fit(DF)
lrModel.transform(DF).agg(F.mean(“prediction”)).show()
{% endcodeblock %}</p>

<pre><code>+---------------+
|avg(prediction)|
+---------------+
|         0.5089|
+---------------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a CDF in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/08/26/creating-a-cdf-in-pyspark/"/>
    <updated>2019-08-26T19:36:15-05:00</updated>
    <id>https://danvatterott.com/blog/2019/08/26/creating-a-cdf-in-pyspark</id>
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">CDFs</a> are a useful tool for understanding your data. This tutorial will demonstrate how to create a CDF in PySpark.</p>

<p>I start by creating normally distributed, fake data.</p>

<p>{% codeblock lang:python %}
import numpy as np
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark.sql.window import Window</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)</p>

<p>a = (sc.parallelize([(float(x),) for x in np.random.normal(0, 1, 1000)]).toDF([‘X’]))
a.limit(5).show() 
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
 </tr>
 <tr>
   <td>1.3162087724709406</td>
 </tr>
 <tr>
   <td>-0.9226127327757598</td>
 </tr>
 <tr>
   <td>0.5388249247619141</td>
 </tr>
 <tr>
   <td>-0.38263792383896356</td>
 </tr>
 <tr>
   <td>0.20584675505779562</td>
 </tr>
</table>

<p>To create the CDF I need to use a <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window">window</a> function to order the data. I can then use <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.percent_rank">percent_rank</a> to retrieve the percentile associated with each value.</p>

<p>The only trick here is I round the column of interest to make sure I don’t retrieve too much data onto the master node (not a concern here, but always good to think about).</p>

<p>After rounding, I group by the variable of interest, again, to limit the amount of data returned.</p>

<p>{% codeblock lang:python %}
win = Window.orderBy(‘X’)</p>

<p>output = (a
          .withColumn(‘cumulative_probability’, F.percent_rank().over(win))
          .withColumn(“X”, F.round(F.col(“X”), 1))
          .groupBy(“X”)
          .agg(F.max(“cumulative_probability”).alias(“cumulative_probability”),F.count(‘*’).alias(“my_count”)))</p>

<p>output.limit(5).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
   <th>cumulative_probability</th>
   <th>my_count</th>
 </tr>
 <tr>
   <td>-3.5</td>
   <td>0.0</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-3.3</td>
   <td>0.001001001001001001</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.9</td>
   <td>0.002002002002002002</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.8</td>
   <td>0.003003003003003003</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.7</td>
   <td>0.004004004004004004</td>
   <td>1</td>
 </tr>
</table>

<p>A CDF should report the percent of data less than or <em>equal</em> to the specified value. The data returned above is the percent of data less than the specified value. We need to fix this by shifting the data up.</p>

<p>To shift the data, I will use the function, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lead">lead</a>.</p>

<p>{% codeblock lang:python %}
output = (a
          .withColumn(‘cumulative_probability’, F.percent_rank().over(win))
          .withColumn(“X”, F.round(F.col(“X”), 1))
          .groupBy(“X”)
          .agg(F.max(“cumulative_probability”).alias(“cumulative_probability”),F.count(‘*’).alias(“my_count”))
          .withColumn(“cumulative_probability”, F.lead(F.col(“cumulative_probability”)).over(win))
          .fillna(1, subset=[“cumulative_probability”]))</p>

<p>output.limit(5).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
   <th>cumulative_probability</th>
   <th>my_count</th>
 </tr>
 <tr>
   <td>-3.5</td>
   <td>0.001001001001001001</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-3.3</td>
   <td>0.002002002002002002</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.9</td>
   <td>0.003003003003003003</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.8</td>
   <td>0.004004004004004004</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.7</td>
   <td>0.005005005005005005</td>
   <td>1</td>
 </tr>
</table>

<p>There we go! A CDF of the data! I hope you find this helpful!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Limiting Cardinality With a PySpark Custom Transformer]]></title>
    <link href="https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer/"/>
    <updated>2019-07-12T06:30:28-05:00</updated>
    <id>https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer</id>
    <content type="html"><![CDATA[<p>When onehot-encoding columns in pyspark, <a href="https://livebook.datascienceheroes.com/data-preparation.html#high_cardinality_descriptive_stats">column cardinality</a> can become a problem. The size of the data often leads to an enourmous number of unique values. If a minority of the values are common and the majority of the values are rare, you might want to represent the rare values as a single group. Note that this might not be appropriate for your problem. <a href="https://livebook.datascienceheroes.com/data-preparation.html#analysis-for-predictive-modeling">Here’s</a> some nice text describing the costs and benefits of this approach. In the following blog post I describe how to implement this solution.</p>

<p>I begin by importing the necessary libraries and creating a spark session.</p>

<p>{% codeblock lang:python %}
import string
import random
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark import keyword_only
from pyspark.ml.pipeline import Transformer
from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param</p>

<p>random.seed(1)</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Next create the custom transformer. This class inherits from the <code>Transformer</code>, <code>HasInputCol</code>, and <code>HasOutputCol</code> classes. I also call an additional parameter <code>n</code> which controls the maximum cardinality allowed in the tranformed column. Because I have the additional parameter, I need some methods for calling and setting this paramter (<code>setN</code> and <code>getN</code>). Finally, there’s <code>_tranform</code> which limits the cardinality of the desired column (set by <code>inputCol</code> parameter). This tranformation method simply takes the desired column and changes all values greater than <code>n</code> to <code>n</code>. It outputs a column named by the <code>outputCol</code> parameter.</p>

<p>{% codeblock lang:python %}
class LimitCardinality(Transformer, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):  
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")  
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):  
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):  
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):  
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col)))) {% endcodeblock %}
</code></pre>

<p>Now that we have the tranformer, I will create some data and apply the transformer to it. I want categorical data, so I will randomly draw letters of the alphabet. The only trick is I’ve made some letters of the alphabet much more common than other ones.</p>

<p>{% codeblock lang:python %}</p>

<p>letter_pool = string.ascii_letters[:26]
letter_pool += ‘‘.join([x*y for x, y in zip(letter_pool[:5], range(100,50,-10))])</p>

<p>a = sc.parallelize([[x, random.choice(letter_pool)] for x in range(1000)]).toDF([“id”, “category”])
a.limit(5).show()
# +—+——–+                                                                <br />
# | id|category|
# +—+——–+
# |  0|       a|
# |  1|       c|
# |  2|       e|
# |  3|       e|
# |  4|       a|
# +—+——–+
{% endcodeblock %}</p>

<p>Take a look at the data.</p>

<p>{% codeblock lang:python %}
(a
 .groupBy(“category”)
 .agg(F.count(“*”).alias(“category_count”))
 .orderBy(F.col(“category_count”).desc())
 .limit(20)
 .show())
# +——–+————–+                                                     <br />
# |category|category_count|
# +——–+————–+
# |       b|           221|
# |       a|           217|
# |       c|           197|
# |       d|           162|
# |       e|           149|
# |       k|             5|
# |       p|             5|
# |       u|             5|
# |       f|             4|
# |       l|             3|
# |       g|             3|
# |       m|             3|
# |       o|             3|
# |       y|             3|
# |       j|             3|
# |       x|             2|
# |       n|             2|
# |       h|             2|
# |       i|             2|
# |       q|             2|
# +——–+————–+
{% endcodeblock %}</p>

<p>Now to apply the new class <code>LimitCardinality</code> after <code>StringIndexer</code> which maps each category (starting with the most common category) to numbers. This means the most common letter will be 1. <code>LimitCardinality</code> then sets the max value of <code>StringIndexer</code>’s output to <code>n</code>. <code>OneHotEncoderEstimator</code> one-hot encodes <code>LimitCardinality</code>’s output. I wrap <code>StringIndexer</code>, <code>LimitCardinality</code>, and <code>OneHotEncoderEstimator</code> into a single pipeline so that I can fit/transform the dataset at one time.</p>

<p>Note that <code>LimitCardinality</code> needs additional code in order to be saved to disk.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer
from pyspark.ml import Pipeline</p>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, stringOrderType=”frequencyDesc”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, n=10)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|          (10,[2],[1.0])|
# |  2|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  3|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+</p>

<p>fit_pipeline.transform(a).limit(5).filter(F.col(“category”) == “n”).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# | 35|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# |458|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>A quick improvement to <code>LimitCardinality</code> would be to set a column’s cardinality so that X% of rows retain their category values and 100-X% receive the default value (rather than arbitrarily selecting a cardinality limit). I implement this below. Note that <code>LimitCardinalityModel</code> is identical to the original <code>LimitCardinality</code>. The new <code>LimitCardinality</code> has a <code>_fit</code> method rather than <code>_transform</code> and this method determines a column’s cardinality.</p>

<p>In the <code>_fit</code> method I find the proportion of columns that are required to describe the requested amount of data.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.pipeline import Estimator, Model</p>

<p>class LimitCardinality(Estimator, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, proportion=None):
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.proportion = Param(self, "proportion", "Cardinality upper limit as a proportion of data.")
    self._setDefault(proportion=0.75)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, proportion=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setProportion(self, value):
    """Set cardinality limit as proportion of data."""
    return self._set(proportion=value)

def getProportion(self):
    """Get cardinality limit as proportion of data."""
    return self.getOrDefault(self.proportion)

def _fit(self, dataframe):
    """Fit transformer."""
    pandas_df = dataframe.groupBy(self.getInputCol()).agg(F.count("*").alias("my_count")).toPandas()
    n = sum((pandas_df
             .sort_values("my_count", ascending=False)
             .cumsum()["my_count"] / sum(pandas_df["my_count"])
            ) &lt; self.getProportion())
    return LimitCardinalityModel(inputCol=self.getInputCol(), outputCol=self.getOutputCol(), n=n)
</code></pre>

<p>class LimitCardinalityModel(Model, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):
    """Initialize."""
    super(LimitCardinalityModel, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col))))
</code></pre>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, handleInvalid=”skip”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, proportion=0.75)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|           (3,[2],[1.0])|
# |  2|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  3|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>There are <a href="https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159">other options</a> for dealing with high cardinality columns such as using a clustering or a <a href="https://tech.instacart.com/predicting-real-time-availability-of-200-million-grocery-items-in-us-canada-stores-61f43a16eafe">mean encoding</a> scheme.</p>

<p>Hope you find this useful and reach out if you have any questions.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Complex Aggregations in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/02/05/complex-aggregations-in-pyspark/"/>
    <updated>2019-02-05T19:09:32-06:00</updated>
    <id>https://danvatterott.com/blog/2019/02/05/complex-aggregations-in-pyspark</id>
    <content type="html"><![CDATA[<p>I’ve touched on this in <a href="https://danvatterott.com/blog/2018/09/06/python-aggregate-udfs-in-pyspark/">past posts</a>, but wanted to write a post specifically describing the power of what I call complex aggregations in PySpark.</p>

<p>The idea is that you have have a data request which initially seems to require multiple different queries, but using ‘complex aggregations’ you can create the requested data using a single query (and a single shuffle).</p>

<p>Let’s say you have a dataset like the following. You have one column (id) which is a unique key for each user, another column (group) which expresses the group that each user belongs to, and finally (value) which expresses the value of each customer. I apologize for the contrived example.</p>

<p>{% codeblock lang:python %}
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql import SparkSession
from pyspark import SparkContext</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)</p>

<p>a = sc.parallelize([[1, ‘a’, 5.1],
                    [2, ‘b’, 2.6],
                    [3, ‘b’, 3.4],
                    [4, ‘c’, 1.7]]).toDF([‘id’, ‘group’, ‘value’])
a.show()          <br />
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>group</th>
   <th>value</th>
 </tr>
 <tr>
   <td>1</td>
   <td>'a'</td>
   <td>5.1</td>
 </tr>
 <tr>
   <td>2</td>
   <td>'b'</td>
   <td>2.6</td>
 </tr>
 <tr>
   <td>3</td>
   <td>'b'</td>
   <td>3.4</td>
 </tr>
 <tr>
   <td>4</td>
   <td>'c'</td>
   <td>1.7</td>
 </tr>
</table>

<p>Let’s say someone wants the average value of group a, b, and c, <em>AND</em> the average value of users in group a <em>OR</em> b, the average value of users in group b <em>OR</em> c AND the value of users in group a <em>OR</em> c. Adds a wrinkle, right? The ‘or’ clauses prevent us from using a simple groupby, and we don’t want to have to write 4 different queries.</p>

<p>Using complex aggregations, we can access all these different conditions in a single query.</p>

<p>{% codeblock lang:python %}</p>

<p>final_data = (a
              .agg(
                F.avg(F.when(F.col(‘group’) == ‘a’, F.col(‘value’)).otherwise(None)).alias(‘group_a_avg’),
                F.avg(F.when(F.col(‘group’) == ‘b’, F.col(‘value’)).otherwise(None)).alias(‘group_b_avg’),
                F.avg(F.when(F.col(‘group’) == ‘c’, F.col(‘value’)).otherwise(None)).alias(‘group_c_avg’),
                F.avg((F.when(F.col(‘group’) == ‘a’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘b’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_ab_avg’),
                F.avg((F.when(F.col(‘group’) == ‘b’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘c’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_bc_avg’),
                F.avg((F.when(F.col(‘group’) == ‘a’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘c’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_ac_avg’),
                )
              )</p>

<p>final_data.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>group_a_avg</th>
   <th>group_b_avg</th>
   <th>group_c_avg</th>
   <th>group_ab_avg</th>
   <th>group_ac_avg</th>
   <th>group_bc_avg</th>
 </tr>
 <tr>
   <td>5.1</td>
   <td>3.0</td>
   <td>1.7</td>
   <td>3.7</td>
   <td>3.4</td>
   <td>2.6</td>
 </tr>
</table>

<p>They key here is using  <code>when</code> to filter different data in and out of different aggregations.</p>

<p>This approach can be quite concise when used with python list comprehensions. I’ll rewrite the query above, but using a list comprehension.</p>

<p>{% codeblock lang:python %}
from itertools import combinations</p>

<p>groups  = [‘a’, ‘b’, ‘c’]
combos = [x for x in combinations(groups,  2)]
print(combos)
#[(‘a’, ‘b’), (‘a’, ‘c’), (‘b’, ‘c’)]</p>

<p>single_group = [F.avg(F.when(F.col(‘group’) == x, F.col(‘value’)).otherwise(None)).alias(‘group_%s_avg’ % x) for x in groups]
double_group = [F.avg(F.when(F.col(‘group’) == x, F.col(‘value’)).when(F.col(‘group’)==y, F.col(‘value’)).otherwise(None)).alias(‘group_%s%s_avg’ % (x, y)) for x, y in combos]
final_data = a.agg(*single_group + double_group)
final_data.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>group_a_avg</th>
   <th>group_b_avg</th>
   <th>group_c_avg</th>
   <th>group_ab_avg</th>
   <th>group_ac_avg</th>
   <th>group_bc_avg</th>
 </tr>
 <tr>
   <td>5.1</td>
   <td>3.0</td>
   <td>1.7</td>
   <td>3.7</td>
   <td>3.4</td>
   <td>2.6</td>
 </tr>
</table>

<p>Voila! Hope you find this little trick helpful! Let me know if you have any questions or comments.</p>
]]></content>
  </entry>
  
</feed>
