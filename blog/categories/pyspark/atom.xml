<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Pyspark | Dan Vatterott]]></title>
  <link href="https://danvatterott.com/blog/categories/pyspark/atom.xml" rel="self"/>
  <link href="https://danvatterott.com/"/>
  <updated>2022-02-25T14:33:11-06:00</updated>
  <id>https://danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Balancing Model Weights in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark/"/>
    <updated>2019-11-18T18:57:03-06:00</updated>
    <id>https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark</id>
    <content type="html"><![CDATA[<p><a href="https://www.jeremyjordan.me/imbalanced-data/">Imbalanced classes</a> is a common problem. Scikit-learn provides an easy fix - “balancing” class weights. This makes models more likely to predict the less common classes (e.g., <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">logistic regression</a>).</p>

<p>The PySpark ML API doesn’t have this same functionality, so in this blog post, I describe how to balance class weights yourself.</p>

<p>{% codeblock lang:python %}
import numpy as np
import pandas as pd
from itertools import chain
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Generate some random data and put the data in a Spark DataFrame. Note that the input variables are not predictive. The model will behave randomly. This is okay, since I am not interested in model accuracy.</p>

<p>{% codeblock lang:python %}
X = np.random.normal(0, 1, (10000, 10))</p>

<p>y = np.ones(X.shape[0]).astype(int)
y[:1000] = 0
np.random.shuffle(y)</p>

<p>print(np.mean(y)) # 0.9</p>

<p>X = np.append(X, y.reshape((10000, 1)), 1)</p>

<p>DF = spark.createDataFrame(pd.DataFrame(X))
DF = DF.withColumnRenamed(“10”, “y”)
{% endcodeblock %}</p>

<p>Here’s how Scikit-learn computes class weights when “balanced” weights are requested.</p>

<p>{% codeblock lang:python %}
# class weight
# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
# n_samples / (n_classes * np.bincount(y)).</p>

<p>class_weights = {i: ii for i, ii in zip(np.unique(y), len(y) / (len(np.unique(y)) * np.bincount(y)))}
print(class_weights) # {0: 5.0, 1: 0.5555555555555556}
{% endcodeblock %}</p>

<p>Here’s how we can compute “balanced” weights with data from a PySpark DataFrame.</p>

<p>{% codeblock lang:python %}
y_collect = DF.select(“y”).groupBy(“y”).count().collect()
unique_y = [x[“y”] for x in y_collect]
total_y = sum([x[“count”] for x in y_collect])
unique_y_count = len(y_collect)
bin_count = [x[“count”] for x in y_collect]</p>

<p>class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}
print(class_weights_spark) # {0.0: 5.0, 1.0: 0.5555555555555556}
{% endcodeblock %}</p>

<p>PySpark needs to have a weight assigned to each instance (i.e., row) in the training set. I create a mapping to apply a weight to each training instance.</p>

<p>{% codeblock lang:python %}
mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])</p>

<p>DF = DF.withColumn(“weight”, mapping_expr.getItem(F.col(“y”)))
{% endcodeblock %}</p>

<p>I assemble all the input features into a vector.</p>

<p>{% codeblock lang:python %}
assembler = VectorAssembler(inputCols=[str(x) for x in range(10)], outputCol=”features”)</p>

<p>DF = assembler.transform(DF).drop(*[str(x) for x in range(10)])
{% endcodeblock %}</p>

<p>And train a logistic regression. Without the instance weights, the model predicts all instances as the frequent class.</p>

<p>{% codeblock lang:python %}
lr = LogisticRegression(featuresCol=”features”, labelCol=”y”)
lrModel = lr.fit(DF)
lrModel.transform(DF).agg(F.mean(“prediction”)).show()
{% endcodeblock %}</p>

<pre><code>+---------------+
|avg(prediction)|
+---------------+
|            1.0|
+---------------+
</code></pre>

<p>With the weights, the model assigns half the instances to each class (even the less commmon one).</p>

<p>{% codeblock lang:python %}
lr = LogisticRegression(featuresCol=”features”, labelCol=”y”, weightCol=”weight”)
lrModel = lr.fit(DF)
lrModel.transform(DF).agg(F.mean(“prediction”)).show()
{% endcodeblock %}</p>

<pre><code>+---------------+
|avg(prediction)|
+---------------+
|         0.5089|
+---------------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a CDF in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/08/26/creating-a-cdf-in-pyspark/"/>
    <updated>2019-08-26T19:36:15-05:00</updated>
    <id>https://danvatterott.com/blog/2019/08/26/creating-a-cdf-in-pyspark</id>
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">CDFs</a> are a useful tool for understanding your data. This tutorial will demonstrate how to create a CDF in PySpark.</p>

<p>I start by creating normally distributed, fake data.</p>

<p>{% codeblock lang:python %}
import numpy as np
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark.sql.window import Window</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)</p>

<p>a = (sc.parallelize([(float(x),) for x in np.random.normal(0, 1, 1000)]).toDF([‘X’]))
a.limit(5).show() 
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
 </tr>
 <tr>
   <td>1.3162087724709406</td>
 </tr>
 <tr>
   <td>-0.9226127327757598</td>
 </tr>
 <tr>
   <td>0.5388249247619141</td>
 </tr>
 <tr>
   <td>-0.38263792383896356</td>
 </tr>
 <tr>
   <td>0.20584675505779562</td>
 </tr>
</table>

<p>To create the CDF I need to use a <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window">window</a> function to order the data. I can then use <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.percent_rank">percent_rank</a> to retrieve the percentile associated with each value.</p>

<p>The only trick here is I round the column of interest to make sure I don’t retrieve too much data onto the master node (not a concern here, but always good to think about).</p>

<p>After rounding, I group by the variable of interest, again, to limit the amount of data returned.</p>

<p>{% codeblock lang:python %}
win = Window.orderBy(‘X’)</p>

<p>output = (a
          .withColumn(‘cumulative_probability’, F.percent_rank().over(win))
          .withColumn(“X”, F.round(F.col(“X”), 1))
          .groupBy(“X”)
          .agg(F.max(“cumulative_probability”).alias(“cumulative_probability”),F.count(‘*’).alias(“my_count”)))</p>

<p>output.limit(5).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
   <th>cumulative_probability</th>
   <th>my_count</th>
 </tr>
 <tr>
   <td>-3.5</td>
   <td>0.0</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-3.3</td>
   <td>0.001001001001001001</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.9</td>
   <td>0.002002002002002002</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.8</td>
   <td>0.003003003003003003</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.7</td>
   <td>0.004004004004004004</td>
   <td>1</td>
 </tr>
</table>

<p>A CDF should report the percent of data less than or <em>equal</em> to the specified value. The data returned above is the percent of data less than the specified value. We need to fix this by shifting the data up.</p>

<p>To shift the data, I will use the function, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lead">lead</a>.</p>

<p>{% codeblock lang:python %}
output = (a
          .withColumn(‘cumulative_probability’, F.percent_rank().over(win))
          .withColumn(“X”, F.round(F.col(“X”), 1))
          .groupBy(“X”)
          .agg(F.max(“cumulative_probability”).alias(“cumulative_probability”),F.count(‘*’).alias(“my_count”))
          .withColumn(“cumulative_probability”, F.lead(F.col(“cumulative_probability”)).over(win))
          .fillna(1, subset=[“cumulative_probability”]))</p>

<p>output.limit(5).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
   <th>cumulative_probability</th>
   <th>my_count</th>
 </tr>
 <tr>
   <td>-3.5</td>
   <td>0.001001001001001001</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-3.3</td>
   <td>0.002002002002002002</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.9</td>
   <td>0.003003003003003003</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.8</td>
   <td>0.004004004004004004</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.7</td>
   <td>0.005005005005005005</td>
   <td>1</td>
 </tr>
</table>

<p>There we go! A CDF of the data! I hope you find this helpful!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Limiting Cardinality With a PySpark Custom Transformer]]></title>
    <link href="https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer/"/>
    <updated>2019-07-12T06:30:28-05:00</updated>
    <id>https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer</id>
    <content type="html"><![CDATA[<p>When onehot-encoding columns in pyspark, <a href="https://livebook.datascienceheroes.com/data-preparation.html#high_cardinality_descriptive_stats">column cardinality</a> can become a problem. The size of the data often leads to an enourmous number of unique values. If a minority of the values are common and the majority of the values are rare, you might want to represent the rare values as a single group. Note that this might not be appropriate for your problem. <a href="https://livebook.datascienceheroes.com/data-preparation.html#analysis-for-predictive-modeling">Here’s</a> some nice text describing the costs and benefits of this approach. In the following blog post I describe how to implement this solution.</p>

<p>I begin by importing the necessary libraries and creating a spark session.</p>

<p>{% codeblock lang:python %}
import string
import random
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark import keyword_only
from pyspark.ml.pipeline import Transformer
from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param</p>

<p>random.seed(1)</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Next create the custom transformer. This class inherits from the <code>Transformer</code>, <code>HasInputCol</code>, and <code>HasOutputCol</code> classes. I also call an additional parameter <code>n</code> which controls the maximum cardinality allowed in the tranformed column. Because I have the additional parameter, I need some methods for calling and setting this paramter (<code>setN</code> and <code>getN</code>). Finally, there’s <code>_tranform</code> which limits the cardinality of the desired column (set by <code>inputCol</code> parameter). This tranformation method simply takes the desired column and changes all values greater than <code>n</code> to <code>n</code>. It outputs a column named by the <code>outputCol</code> parameter.</p>

<p>{% codeblock lang:python %}
class LimitCardinality(Transformer, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):  
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")  
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):  
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):  
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):  
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col)))) {% endcodeblock %}
</code></pre>

<p>Now that we have the tranformer, I will create some data and apply the transformer to it. I want categorical data, so I will randomly draw letters of the alphabet. The only trick is I’ve made some letters of the alphabet much more common than other ones.</p>

<p>{% codeblock lang:python %}</p>

<p>letter_pool = string.ascii_letters[:26]
letter_pool += ‘‘.join([x*y for x, y in zip(letter_pool[:5], range(100,50,-10))])</p>

<p>a = sc.parallelize([[x, random.choice(letter_pool)] for x in range(1000)]).toDF([“id”, “category”])
a.limit(5).show()
# +—+——–+                                                                <br />
# | id|category|
# +—+——–+
# |  0|       a|
# |  1|       c|
# |  2|       e|
# |  3|       e|
# |  4|       a|
# +—+——–+
{% endcodeblock %}</p>

<p>Take a look at the data.</p>

<p>{% codeblock lang:python %}
(a
 .groupBy(“category”)
 .agg(F.count(“*”).alias(“category_count”))
 .orderBy(F.col(“category_count”).desc())
 .limit(20)
 .show())
# +——–+————–+                                                     <br />
# |category|category_count|
# +——–+————–+
# |       b|           221|
# |       a|           217|
# |       c|           197|
# |       d|           162|
# |       e|           149|
# |       k|             5|
# |       p|             5|
# |       u|             5|
# |       f|             4|
# |       l|             3|
# |       g|             3|
# |       m|             3|
# |       o|             3|
# |       y|             3|
# |       j|             3|
# |       x|             2|
# |       n|             2|
# |       h|             2|
# |       i|             2|
# |       q|             2|
# +——–+————–+
{% endcodeblock %}</p>

<p>Now to apply the new class <code>LimitCardinality</code> after <code>StringIndexer</code> which maps each category (starting with the most common category) to numbers. This means the most common letter will be 1. <code>LimitCardinality</code> then sets the max value of <code>StringIndexer</code>’s output to <code>n</code>. <code>OneHotEncoderEstimator</code> one-hot encodes <code>LimitCardinality</code>’s output. I wrap <code>StringIndexer</code>, <code>LimitCardinality</code>, and <code>OneHotEncoderEstimator</code> into a single pipeline so that I can fit/transform the dataset at one time.</p>

<p>Note that <code>LimitCardinality</code> needs additional code in order to be saved to disk.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer
from pyspark.ml import Pipeline</p>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, stringOrderType=”frequencyDesc”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, n=10)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|          (10,[2],[1.0])|
# |  2|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  3|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+</p>

<p>fit_pipeline.transform(a).limit(5).filter(F.col(“category”) == “n”).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# | 35|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# |458|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>A quick improvement to <code>LimitCardinality</code> would be to set a column’s cardinality so that X% of rows retain their category values and 100-X% receive the default value (rather than arbitrarily selecting a cardinality limit). I implement this below. Note that <code>LimitCardinalityModel</code> is identical to the original <code>LimitCardinality</code>. The new <code>LimitCardinality</code> has a <code>_fit</code> method rather than <code>_transform</code> and this method determines a column’s cardinality.</p>

<p>In the <code>_fit</code> method I find the proportion of columns that are required to describe the requested amount of data.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.pipeline import Estimator, Model</p>

<p>class LimitCardinality(Estimator, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, proportion=None):
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.proportion = Param(self, "proportion", "Cardinality upper limit as a proportion of data.")
    self._setDefault(proportion=0.75)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, proportion=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setProportion(self, value):
    """Set cardinality limit as proportion of data."""
    return self._set(proportion=value)

def getProportion(self):
    """Get cardinality limit as proportion of data."""
    return self.getOrDefault(self.proportion)

def _fit(self, dataframe):
    """Fit transformer."""
    pandas_df = dataframe.groupBy(self.getInputCol()).agg(F.count("*").alias("my_count")).toPandas()
    n = sum((pandas_df
             .sort_values("my_count", ascending=False)
             .cumsum()["my_count"] / sum(pandas_df["my_count"])
            ) &lt; self.getProportion())
    return LimitCardinalityModel(inputCol=self.getInputCol(), outputCol=self.getOutputCol(), n=n)
</code></pre>

<p>class LimitCardinalityModel(Model, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):
    """Initialize."""
    super(LimitCardinalityModel, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col))))
</code></pre>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, handleInvalid=”skip”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, proportion=0.75)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|           (3,[2],[1.0])|
# |  2|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  3|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>There are <a href="https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159">other options</a> for dealing with high cardinality columns such as using a clustering or a <a href="https://tech.instacart.com/predicting-real-time-availability-of-200-million-grocery-items-in-us-canada-stores-61f43a16eafe">mean encoding</a> scheme.</p>

<p>Hope you find this useful and reach out if you have any questions.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Complex Aggregations in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/02/05/complex-aggregations-in-pyspark/"/>
    <updated>2019-02-05T19:09:32-06:00</updated>
    <id>https://danvatterott.com/blog/2019/02/05/complex-aggregations-in-pyspark</id>
    <content type="html"><![CDATA[<p>I’ve touched on this in <a href="https://danvatterott.com/blog/2018/09/06/python-aggregate-udfs-in-pyspark/">past posts</a>, but wanted to write a post specifically describing the power of what I call complex aggregations in PySpark.</p>

<p>The idea is that you have have a data request which initially seems to require multiple different queries, but using ‘complex aggregations’ you can create the requested data using a single query (and a single shuffle).</p>

<p>Let’s say you have a dataset like the following. You have one column (id) which is a unique key for each user, another column (group) which expresses the group that each user belongs to, and finally (value) which expresses the value of each customer. I apologize for the contrived example.</p>

<p>{% codeblock lang:python %}
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql import SparkSession
from pyspark import SparkContext</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)</p>

<p>a = sc.parallelize([[1, ‘a’, 5.1],
                    [2, ‘b’, 2.6],
                    [3, ‘b’, 3.4],
                    [4, ‘c’, 1.7]]).toDF([‘id’, ‘group’, ‘value’])
a.show()          <br />
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>group</th>
   <th>value</th>
 </tr>
 <tr>
   <td>1</td>
   <td>'a'</td>
   <td>5.1</td>
 </tr>
 <tr>
   <td>2</td>
   <td>'b'</td>
   <td>2.6</td>
 </tr>
 <tr>
   <td>3</td>
   <td>'b'</td>
   <td>3.4</td>
 </tr>
 <tr>
   <td>4</td>
   <td>'c'</td>
   <td>1.7</td>
 </tr>
</table>

<p>Let’s say someone wants the average value of group a, b, and c, <em>AND</em> the average value of users in group a <em>OR</em> b, the average value of users in group b <em>OR</em> c AND the value of users in group a <em>OR</em> c. Adds a wrinkle, right? The ‘or’ clauses prevent us from using a simple groupby, and we don’t want to have to write 4 different queries.</p>

<p>Using complex aggregations, we can access all these different conditions in a single query.</p>

<p>{% codeblock lang:python %}</p>

<p>final_data = (a
              .agg(
                F.avg(F.when(F.col(‘group’) == ‘a’, F.col(‘value’)).otherwise(None)).alias(‘group_a_avg’),
                F.avg(F.when(F.col(‘group’) == ‘b’, F.col(‘value’)).otherwise(None)).alias(‘group_b_avg’),
                F.avg(F.when(F.col(‘group’) == ‘c’, F.col(‘value’)).otherwise(None)).alias(‘group_c_avg’),
                F.avg((F.when(F.col(‘group’) == ‘a’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘b’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_ab_avg’),
                F.avg((F.when(F.col(‘group’) == ‘b’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘c’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_bc_avg’),
                F.avg((F.when(F.col(‘group’) == ‘a’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘c’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_ac_avg’),
                )
              )</p>

<p>final_data.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>group_a_avg</th>
   <th>group_b_avg</th>
   <th>group_c_avg</th>
   <th>group_ab_avg</th>
   <th>group_ac_avg</th>
   <th>group_bc_avg</th>
 </tr>
 <tr>
   <td>5.1</td>
   <td>3.0</td>
   <td>1.7</td>
   <td>3.7</td>
   <td>3.4</td>
   <td>2.6</td>
 </tr>
</table>

<p>They key here is using  <code>when</code> to filter different data in and out of different aggregations.</p>

<p>This approach can be quite concise when used with python list comprehensions. I’ll rewrite the query above, but using a list comprehension.</p>

<p>{% codeblock lang:python %}
from itertools import combinations</p>

<p>groups  = [‘a’, ‘b’, ‘c’]
combos = [x for x in combinations(groups,  2)]
print(combos)
#[(‘a’, ‘b’), (‘a’, ‘c’), (‘b’, ‘c’)]</p>

<p>single_group = [F.avg(F.when(F.col(‘group’) == x, F.col(‘value’)).otherwise(None)).alias(‘group_%s_avg’ % x) for x in groups]
double_group = [F.avg(F.when(F.col(‘group’) == x, F.col(‘value’)).when(F.col(‘group’)==y, F.col(‘value’)).otherwise(None)).alias(‘group_%s%s_avg’ % (x, y)) for x, y in combos]
final_data = a.agg(*single_group + double_group)
final_data.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>group_a_avg</th>
   <th>group_b_avg</th>
   <th>group_c_avg</th>
   <th>group_ab_avg</th>
   <th>group_ac_avg</th>
   <th>group_bc_avg</th>
 </tr>
 <tr>
   <td>5.1</td>
   <td>3.0</td>
   <td>1.7</td>
   <td>3.7</td>
   <td>3.4</td>
   <td>2.6</td>
 </tr>
</table>

<p>Voila! Hope you find this little trick helpful! Let me know if you have any questions or comments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a Survival Function in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2018/12/07/survival-function-in-pyspark/"/>
    <updated>2018-12-07T21:13:48-06:00</updated>
    <id>https://danvatterott.com/blog/2018/12/07/survival-function-in-pyspark</id>
    <content type="html"><![CDATA[<p>Traditionally, <a href="https://en.wikipedia.org/wiki/Survival_function">survival functions</a> have been used in medical research to visualize the proportion of people who remain alive following a treatment. I often use them to understand the length of time between users creating and cancelling their subscription accounts.</p>

<p>Here, I describe how to create a survival function using PySpark. This is not a post about creating a <a href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator">Kaplan-Meier estimator</a> or fitting mathematical functions to survival functions. Instead, I demonstrate how to acquire the data necessary for plotting a survival function.</p>

<p>I begin by creating a SparkContext.</p>

<p>{% codeblock lang:python %}
from pyspark.sql import SparkSession
from pyspark import SparkContext
sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Next, I load fake data into a Spark Dataframe. This is the data we will use in this example. Each row is a different user and the Dataframe has columns describing start and end dates for each user. <code>start_date</code> represents when a user created their account and <code>end_date</code> represents when a user canceled their account.</p>

<p>{% codeblock lang:python %}
from pyspark.sql import functions as F
from pyspark.sql import types as T</p>

<p>user_table = (sc.parallelize([[1, ‘2018-11-01’, ‘2018-11-03’],
                              [2, ‘2018-01-01’, ‘2018-08-17’],
                              [3, ‘2017-12-31’, ‘2018-01-06’],
                              [4, ‘2018-11-15’, ‘2018-11-16’],
                              [5, ‘2018-04-02’, ‘2018-04-12’]])
              .toDF([‘id’, ‘start_date’, ‘end_date’])
             )
user_table.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>start_date</th>
   <th>end_date</th>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
 </tr>
 <tr>
   <td>2</td>
   <td>2018-01-01</td>
   <td>2018-08-17</td>
 </tr>
 <tr>
   <td>3</td>
   <td>2017-12-31</td>
   <td>2018-01-06</td>
 </tr>
 <tr>
   <td>4</td>
   <td>2018-11-15</td>
   <td>2018-11-16</td>
 </tr>
 <tr>
   <td>5</td>
   <td>2018-04-02</td>
   <td>2018-04-12</td>
 </tr>
</table>

<p>I use <code>start_date</code> and <code>end_date</code> to determine how many days each user was active following their <code>start_date</code>.</p>

<p>{% codeblock lang:python %}
days_till_cancel = (user_table
                    .withColumn(‘days_till_cancel’, F.datediff(F.col(‘end_date’), F.col(‘start_date’)))
                   )</p>

<p>days_till_cancel.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>start_date</th>
   <th>end_date</th>
   <th>days_till_cancel</th>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
 </tr>
 <tr>
   <td>2</td>
   <td>2018-01-01</td>
   <td>2018-08-17</td>
   <td>228</td>
 </tr>
 <tr>
   <td>3</td>
   <td>2017-12-31</td>
   <td>2018-01-06</td>
   <td>6</td>
 </tr>
 <tr>
   <td>4</td>
   <td>2018-11-15</td>
   <td>2018-11-16</td>
   <td>1</td>
 </tr>
 <tr>
   <td>5</td>
   <td>2018-04-02</td>
   <td>2018-04-12</td>
   <td>10</td>
 </tr>
</table>

<p>I use a <a href="https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.functions.udf">Python UDF</a> to create a vector of the numbers 0 through 13 representing our <em>period of interest</em>. The start date of our <em>period of interest</em> is a user’s <code>start_date</code>. The end date of our <em>period of interest</em> is 13 days following a user’s <code>start_date</code>. I chose 13 days as the <em>period of interest</em> for no particular reason.</p>

<p>I use <a href="https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.functions.explode">explode</a> to expand the numbers in each vector (i.e., 0-&gt;13) into different rows. Each user now has a row for each day in the <em>period of interest</em>.</p>

<p>I describe one user’s data below.</p>

<p>{% codeblock lang:python %}
create_day_list = F.udf(lambda: [i for i in range(0, 14)], T.ArrayType(T.IntegerType()))</p>

<p>relevant_days = (days_till_cancel
                 .withColumn(‘day_list’, create_day_list())
                 .withColumn(‘day’, F.explode(F.col(‘day_list’)))
                 .drop(‘day_list’)
                )</p>

<p>relevant_days.filter(F.col(‘id’) == 1).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>start_date</th>
   <th>end_date</th>
   <th>days_till_cancel</th>
   <th>day</th>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>1</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>2</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>3</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>4</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>5</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>6</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>7</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>8</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>9</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>10</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>11</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>12</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>13</td>
 </tr>
</table>

<p>We want the proportion of users who are active <em>X</em> days after <code>start_date</code>. I create a column <code>active</code> which represents whether users are active or not. I initially assign each user a 1 in each row (1 represents active). I then overwrite 1s with 0s after a user is no longer active. I determine that a user is no longer active by comparing the values in <code>day</code> and <code>days_till_cancel</code>. When <code>day</code> is greater than <code>days_till_cancel</code>, the user is no longer active.</p>

<p>I describe one user’s data below.</p>

<p>{% codeblock lang:python %}
days_active = (relevant_days
               .withColumn(‘active’, F.lit(1))
               .withColumn(‘active’, F.when(F.col(‘day’) &gt;= F.col(‘days_till_cancel’), 0).otherwise(F.col(‘active’)))
              )</p>

<p>days_active.filter(F.col(‘id’) == 1).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>start_date</th>
   <th>end_date</th>
   <th>days_till_cancel</th>
   <th>day</th>
   <th>active</th>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>0</td>
   <td>1</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>1</td>
   <td>1</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>2</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>3</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>4</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>5</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>6</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>7</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>8</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>9</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>10</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>11</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>12</td>
   <td>0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2018-11-01</td>
   <td>2018-11-03</td>
   <td>2</td>
   <td>13</td>
   <td>0</td>
 </tr>
</table>

<p>Finally, to acquire the survival function data, I group by <code>day</code> (days following <code>start_date</code>) and average the value in <code>active</code>. This provides us with the proportion of users who are active <em>X</em> days after <code>start_date</code>.</p>

<p>{% codeblock lang:python %}
survival_curve = (days_active
                  .groupby(‘day’)
                  .agg(
                      F.count(‘*’).alias(‘user_count’),
                      F.avg(‘active’).alias(‘percent_active’),
                  )
                  .orderBy(‘day’)
                 )</p>

<p>survival_curve.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>day</th>
   <th>user_count</th>
   <th>percent_active</th>
 </tr>
 <tr>
   <td>0</td>
   <td>5</td>
   <td>1.0</td>
 </tr>
 <tr>
   <td>1</td>
   <td>5</td>
   <td>0.8</td>
 </tr>
 <tr>
   <td>2</td>
   <td>5</td>
   <td>0.6</td>
 </tr>
 <tr>
   <td>3</td>
   <td>5</td>
   <td>0.6</td>
 </tr>
 <tr>
   <td>4</td>
   <td>5</td>
   <td>0.6</td>
 </tr>
 <tr>
   <td>5</td>
   <td>5</td>
   <td>0.6</td>
 </tr>
 <tr>
   <td>6</td>
   <td>5</td>
   <td>0.4</td>
 </tr>
 <tr>
   <td>7</td>
   <td>5</td>
   <td>0.4</td>
 </tr>
 <tr>
   <td>8</td>
   <td>5</td>
   <td>0.4</td>
 </tr>
 <tr>
   <td>9</td>
   <td>5</td>
   <td>0.4</td>
 </tr>
 <tr>
   <td>10</td>
   <td>5</td>
   <td>0.2</td>
 </tr>
 <tr>
   <td>11</td>
   <td>5</td>
   <td>0.2</td>
 </tr>
 <tr>
   <td>12</td>
   <td>5</td>
   <td>0.2</td>
 </tr>
 <tr>
   <td>13</td>
   <td>5</td>
   <td>0.2</td>
 </tr>
</table>
]]></content>
  </entry>
  
</feed>
