<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine Learning | Dan Vatterott]]></title>
  <link href="http://www.danvatterott.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://www.danvatterott.com/"/>
  <updated>2017-03-16T22:06:48-04:00</updated>
  <id>http://www.danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Attention in a Convolutional Neural Net]]></title>
    <link href="http://www.danvatterott.com/blog/2016/09/20/attention-in-a-convolutional-neural-net/"/>
    <updated>2016-09-20T19:51:01-04:00</updated>
    <id>http://www.danvatterott.com/blog/2016/09/20/attention-in-a-convolutional-neural-net</id>
    <content type="html"><![CDATA[<p>This summer I had the pleasure of attending the <a href="http://cbmm.mit.edu/">Brains, Minds, and Machines</a> summer course at the <a href="http://www.mbl.edu/">Marine Biology Laboratory</a>. While there, I saw cool research, met awesome scientists, and completed an independent project. In this blog post, I describe my project.</p>

<p>In 2012, Krizhevsky et al. released a <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">convolutional neural network</a> that completely blew away the field at the <a href="http://www.image-net.org/">imagenet challenge</a>. This model is called “Alexnet,” and 2012 marks the beginning of neural networks’ resurgence in the machine learning community.</p>

<p>Alexnet’s domination was not only exciting for the machine learning community. It was also exciting for the visual neuroscience community whose descriptions of the visual system closely matched alexnet (e.g., <a href="http://maxlab.neuro.georgetown.edu/hmax">HMAX</a>). <a href="http://mcgovern.mit.edu/principal-investigators/james-dicarlo">Jim DiCarlo</a> gave an awesome talk at the summer course describing his research comparing the output of neurons in the visual system and the output of “neurons” in alexnet (you can find the article <a href="https://physics.ucsd.edu/neurophysics/courses/physics_171/DeCarlo_reprint.pdf">here</a>).</p>

<p><img src="/images/BMM_CNN/visual_system_models.png" /></p>

<p>I find the similarities between the visual system and convolutional neural networks exciting, but check out the depictions of alexnet and the visual system above. Alexnet is depicted in the upper image. The visual system is depicted in the lower image. Comparing the two images is not fair, but the visual system is obviously vastly more complex than alexnet.</p>

<p>In my project, I applied a known complexity of the biological visual system to a convolutional neural network. Specifically, I incoporated visual attention into the network. <a href="https://en.wikipedia.org/wiki/Biased_Competition_Theory">Visual attention</a> refers to our ability to focus cognitive processing onto a subset of the environment. Check out <a href="https://www.youtube.com/watch?v=vJG698U2Mvo">this video</a> for an incredibly 90s demonstration of visual attention.</p>

<p>In this post, I demonstrate that implementing a basic version of visual attention in a convolutional neural net improves performance of the CNN, but only when classifying noisy images, and not when classifying relatively noiseless images.</p>

<p>Code for everything described in this post can be found on <a href="https://github.com/dvatterott/BMM_attentional_CNN">my github page</a>. In creating this model, I cribbed code from both <a href="http://jacobcv.blogspot.com/2016/08/class-activation-maps-in-keras.html">Jacob Gildenblat</a> and <a href="https://github.com/heuritech/convnets-keras">this implementation of alexnet</a>.</p>

<p>I implemented my model using the <a href="https://keras.io/">Keras library</a> with a <a href="https://theano.readthedocs.io/en/latest/">Theano backend</a>, and I tested my model on the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST database</a>. The MNIST database is composed of images of handwritten numbers. The task is to design a model that can accurately guess what number is written in the image. This is a relatively easy task, and the <a href="http://yann.lecun.com/exdb/mnist/">best models are over 99% accurate</a>.</p>

<p>I chose MNIST because its an easy problem, which allows me to use a small network. A small network is both easy to train and easy to understand, which is good for an exploratory project like this one.</p>

<p><img src="/images/BMM_CNN/att_model2.png" /></p>

<p>Above, I depict my model. This model has two <a href="http://cs231n.github.io/convolutional-networks/">convolutional layers</a>. Following the convolutional layers is a feature averaging layer which borrows methods from a <a href="http://cnnlocalization.csail.mit.edu/">recent paper out of the Torralba lab</a> and computes the average activity of units covering each location. The output of this feature averaging layer is then passed along to a fully connected layer. The fully connected layer “guesses” what the most likely digit is. My goal when I first created this network was to use this “guess” to guide where the model focused processing (i.e., attention), but I found guided models are irratic during training.</p>

<p>Instead, my current model directs attention to all locations that are predictive of all digits. I haven’t toyed too much with inbetween models - models that direct attention to locations that are predictive of the <em>N</em> most likely digits.</p>

<p>So what does it mean to “direct attention” in this model. Here, directing attention means that neurons covering “attended” locations are more active than neurons covering the unattended locations. I apply attention to the input of the second convolutional layer. The attentionally weighted signal passes through the second convolutional layer and passes onto the feature averaging layer. The feature averaging layer feeds to the fully connected layer, which then produces a final guess about what digit is present.</p>

<p>I first tested this model on the plain MNIST set. For testing, I wanted to compare my model to a model without attention. My comparison model is the same as the model with attention except that the attention directing signal is a matrix of ones - meaning that it doesn’t have any effect on the model’s activity. I use this comparison model because it has the same architecture as the model with attention.</p>

<p>I depict the results of my attentional and comparison models below. On the X-axis is the test phase (10k trials) following each training epoch (60k trials). On the Y-axis is percent accuracy during the test phase. I did 3 training runs with both sets of models. All models gave fairly similar results, which led to small error bars (these depict standard error). The results are … dissapointing. As you can see both the model with attention and the comparison model perform similarly. There might be an initial impact of attention, but this impact is slight.</p>

<p><img src="/images/BMM_CNN/model_performance_nonoise.png" /></p>

<p>This result was a little dissapointing (since I’m an attention researcher and consider attention an important part of cognition), but it might not be so surprising given the task. If I gave you the task of naming digits, this task would be virtually effortless; probably so effortless that you would not have to pay very much attention to the task. You could probably talk on the phone or text while doing this task. Basically, I might have failed to find an effect of attention because this task is so easy that it does not require attention.</p>

<p>I decided to try my network when the task was a little more difficult. To make the task more difficult, I added random noise to each image (thank you to Nancy Kanwisher for the suggestion). This trick of adding noise to images is one that’s frequently done in psychophysical attention expeirments, so it would be fitting if it worked here.</p>

<p><img src="/images/BMM_CNN/model_performance_noise.png" /></p>

<p>The figure above depicts model performance on noisy images. The models are the as before, but this time the model with attention is far superior to the comparison model. Good news for attention researchers! This work suggests that visual attentional mechanisms similar to those in the brain may be beneficial in convolutional neural networks, and this effect is particularly strong with the images are noisy.</p>

<p>This work bears superficial similarity to recent <a href="http://arxiv.org/pdf/1603.01417.pdf">language translation and question answering models</a>. Models like the cited one report using a biologically inspired version of attention, and I agree they do, but they do not use attention in the same way that I am here. I believe this difference demonstrates a problem with what we call “attention.” Attention is not a single cognitive process. Instead, its a family of cognitive processes that we’ve simply given the same name. Thats not to say these forms of attention are completely distinct, but they likely involve different information transformations and probably even different brain regions.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to Neural Networks: Part 2]]></title>
    <link href="http://www.danvatterott.com/blog/2016/05/02/an-introduction-to-neural-networks-part-2/"/>
    <updated>2016-05-02T21:56:27-04:00</updated>
    <id>http://www.danvatterott.com/blog/2016/05/02/an-introduction-to-neural-networks-part-2</id>
    <content type="html"><![CDATA[<p>In a previous <a href="http://www.danvatterott.com/blog/2016/04/29/an-introduction-to-neural-networks-part-1/">post</a>, I described how to do <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropogation</a> with a 1-layer <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network</a>. I’ve written this post assuming some familiarity with the previous post.</p>

<p>When first created, 1-layer neural networks <a href="https://en.wikipedia.org/wiki/Perceptron">brought about quite a bit of excitement</a>, but this excitement quickly dissipated when researchers realized that 1-layer <a href="https://en.wikipedia.org/wiki/Perceptrons_%28book%29">neural networks could only solve a limited set of problems</a>.</p>

<p>Researchers knew that adding an extra layer to the neural networks enabled neural networks to solve much more complex problems, but they didn’t know how to train these more complex networks.</p>

<p>In the previous post, I described “backpropogation,” but this wasn’t the portion of backpropogation that really changed the history of neural networks. What really changed neural networks is backpropogation with an extra layer. This extra layer enabled researchers to train more complex networks. The extra layer(s) is(are) called the <em>hidden layer(s)</em>. In this post, I will describe backpropogation with a hidden layer.</p>

<p>To describe backpropogation with a hidden layer, I will demonstrate how neural networks can solve the <a href="https://en.wikipedia.org/wiki/Exclusive_or">XOR problem</a>.</p>

<p>In this example of the XOR problem there are four items. Each item is defined by two values. If these two values are the same, then the item belongs to one group (blue here). If the two values are different, then the item belongs to another group (red here).</p>

<p>Below, I have depicted the XOR problem. The goal is to find a model that can distinguish between the blue and red groups based on an item’s values.</p>

<p>This code is also available as a jupyter notebook on <a href="https://github.com/dvatterott/jupyter_notebooks">my github</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="c">#import important libraries.</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="err">’</span><span class="n">bo</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="err">’</span><span class="n">ro</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Value</span> <span class="mi">2</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Value</span> <span class="mi">1</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/exampleXOR.png" /></p>

<p>Again, each item has two values. An item’s first value is represented on the x-axis. An items second value is represented on the y-axis. The red items belong to one category and the blue items belong to another.</p>

<p>This is a non-linear problem because no linear function can segregate the groups. For instance, a horizontal line could segregate the upper and lower items and a vertical line could segregate the left and right items, but no single linear function can segregate the red and blue items.</p>

<p>We need a non-linear function to seperate the groups, and neural networks can emulate a non-linear function that segregates them.</p>

<p>While this problem may seem relatively simple, it gave the initial neural networks quite a hard time. In fact, this is the problem that depleted much of the original enthusiasm for neural networks.</p>

<p>Neural networks can easily solve this problem, but they require an extra layer. Below I depict a network with an extra layer (a 2-layer network). To depict the network, I use a repository available on my <a href="https://github.com/dvatterott/visualise_neural_network">github</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">visualise_neural_network</span> <span class="kn">import</span> <span class="n">NeuralNetwork</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span> <span class="c">#create neural network object</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">])</span> <span class="c">#input layer with names</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="err">‘</span><span class="n">Hidden</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Hidden</span> <span class="mi">2</span><span class="err">’</span><span class="p">])</span> <span class="c">#hidden layer with names</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">,[</span><span class="err">‘</span><span class="n">Output</span><span class="err">’</span><span class="p">])</span> <span class="c">#output layer with name</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/3_layer_net.png" /></p>

<p>Notice that this network now has 5 total neurons. The two units at the bottom are the <em>input layer</em>. The activity of input units is the value of the inputs (same as the inputs in my previous post). The two units in the middle are the <em>hidden layer</em>. The activity of hidden units are calculated in the same manner as the output units from my previous post. The unit at the top is the <em>output layer</em>. The activity of this unit is found in the same manner as in my previous post, but the activity of the hidden units replaces the input units.</p>

<p>Thus, when the neural network makes its guess, the only difference is we have to compute an extra layer’s activity.</p>

<p>The goal of this network is for the output unit to have an activity of 0 when presented with an item from the blue group (inputs are same) and to have an activity of 1 when presented with an item from the red group (inputs are different).</p>

<p>One additional aspect of neural networks that I haven’t discussed is each non-input unit can have a <em>bias</em>. You can think about bias as a propensity for the unit to become active or not to become active. For instance, a unit with a postitive bias is more likely to be active than a unit with no bias.</p>

<p>I will implement bias as an extra line feeding into each unit. The weight of this line is the bias, and the bias line is always active, meaning this bias is always present.</p>

<p>Below, I seed this 3-layer neural network with a random set of weights.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c">#seed random number generator for reproducibility&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Weights_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">2</span> <span class="c">#connections between hidden and output</span>
</span><span class='line'><span class="n">Weights_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">2</span> <span class="c">#connections between input and hidden&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Weight_Dict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_1</span><span class="p">,</span><span class="err">’</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_2</span><span class="p">}</span> <span class="c">#place weights in a dictionary&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Train_Set</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]</span> <span class="c">#train set&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">],</span>
</span><span class='line'>                  <span class="p">[[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">]],</span>
</span><span class='line'>                   <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">]]])</span>
</span><span class='line'><span class="c">#add input layer with names and weights leaving the input neurons</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)],</span>
</span><span class='line'>                  <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">]])</span>
</span><span class='line'><span class="c">#add hidden layer with names (each units’ bias) and weights leaving the hidden units</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)])</span>
</span><span class='line'><span class="c">#add output layer with name (the output unit’s bias)</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/3_layer_weights.png" /></p>

<p>Above we have out network. The depiction of <script type="math/tex">Weight_{Input_{1}\to.Hidden_{2}}</script> and <script type="math/tex">Weight_{Input_{2}\to.Hidden_{1}}</script> are confusing. -0.8 belongs to <script type="math/tex">Weight_{Input_{1}\to.Hidden_{2}}</script>. -0.5 belongs to <script type="math/tex">Weight_{Input_{2}\to.Hidden_{1}}</script>.</p>

<p>Lets go through one example of our network receiving an input and making a guess. Lets say the input is [0 1].
This means <script type="math/tex">Input_{1} = 0</script> and <script type="math/tex">Input_{2} = 1</script>. The correct answer in this case is 1.</p>

<p>First, we have to calculate <script type="math/tex">Hidden _{1}</script>’s input. Remember we can write input as</p>

<script type="math/tex; mode=display">net = \displaystyle\sum_{i=1}^{Inputs}Input_i * Weight_i</script>

<p>with the a bias we can rewrite it as</p>

<script type="math/tex; mode=display">net = Bias + \displaystyle\sum_{i=1}^{Inputs}Input_i * Weight_i</script>

<p>Specifically for <script type="math/tex">Hidden_{1}</script></p>

<script type="math/tex; mode=display">net_{Hidden_{1}} = -0.78 + -0.25*0 + -0.5*1 = -1.28</script>

<p>Remember the first term in the equation above is the bias term. Lets see what this looks like in code.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span><span class='line'><span class="n">net_Hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="mf">1.0</span><span class="p">),</span><span class="n">Weights_1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c">#append the bias input</span>
</span><span class='line'><span class="k">print</span> <span class="n">net_Hidden</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>[-1.27669634 -1.07035845]
</code></pre>

<p>Note that by using np.dot, I can calculate both hidden unit’s input in a single line of code.</p>

<p>Next, we have to find the activity of units in the hidden layer.</p>

<p>I will translate input into activity with a logistic function, as I did in the previous post.</p>

<script type="math/tex; mode=display">Logistic = \frac{1}{1+e^{-x}}</script>

<p>Lets see what this looks like in code.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c">#each neuron has a logistic activation function</span>
</span><span class='line'>    <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Hidden_Units</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">net_Hidden</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="n">Hidden_Units</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>[ 0.2181131   0.25533492]
</code></pre>

<p>So far so good, the logistic function has transformed the negative inputs into values near 0.</p>

<p>Now we have to compute the output unit’s acitivity.</p>

<script type="math/tex; mode=display">net_{Output} = Bias + Hidden_{1}*Weight_{Hidden_{1}\to.Output} + Hidden_{2}*Weight_{Hidden_{2}\to.Output}</script>

<p>plugging in the numbers</p>

<script type="math/tex; mode=display">net_{Output} = -0.37 + 0.22*-0.23 + 0.26*-0.98 = -0.67</script>

<p>Now the code for computing <script type="math/tex">net_{Output}</script> and the Output unit’s activity.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">net_Output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Hidden_Units</span><span class="p">,</span><span class="mf">1.0</span><span class="p">),</span><span class="n">Weights_2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="err">‘</span><span class="n">net_Output</span><span class="err">’</span>
</span><span class='line'><span class="k">print</span> <span class="n">net_Output</span>
</span><span class='line'><span class="n">Output</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">net_Output</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="err">‘</span><span class="n">Output</span><span class="err">’</span>
</span><span class='line'><span class="k">print</span> <span class="n">Output</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>net_Output
[-0.66626595]
Output
[ 0.33933346]
</code></pre>

<p>Okay, thats the network’s guess for one input…. no where near the correct answer (1). Let’s look at what the network predicts for the other input patterns. Below I create a feedfoward, 1-layer neural network and plot the neural nets’ guesses to the four input patterns.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">layer_InputOutput</span><span class="p">(</span><span class="n">Inputs</span><span class="p">,</span><span class="n">Weights</span><span class="p">):</span> <span class="c">#find a layers input and activity</span>
</span><span class='line'>    <span class="n">Inputs_with_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Inputs</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span> <span class="c">#input 1 for each unit’s bias</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Inputs_with_bias</span><span class="p">,</span><span class="n">Weights</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">neural_net</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights_1</span><span class="p">,</span><span class="n">Weights_2</span><span class="p">,</span><span class="n">Training</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span> <span class="c">#this function creates and runs the neural net&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">target</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#set target value</span>
</span><span class='line'><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">Input</span><span class="p">[</span><span class="mi">1</span><span class="p">]]):</span> <span class="n">target</span> <span class="o">=</span> <span class="mi">0</span> <span class="c">#change target value if needed</span>
</span><span class='line'>
</span><span class='line'><span class="c">#forward pass</span>
</span><span class='line'><span class="n">Hidden_Units</span> <span class="o">=</span> <span class="n">layer_InputOutput</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights_1</span><span class="p">)</span> <span class="c">#find hidden unit activity</span>
</span><span class='line'><span class="n">Output</span> <span class="o">=</span> <span class="n">layer_InputOutput</span><span class="p">(</span><span class="n">Hidden_Units</span><span class="p">,</span><span class="n">Weights_2</span><span class="p">)</span> <span class="c">#find Output layer actiity</span>
</span><span class='line'>
</span><span class='line'><span class="k">return</span> <span class="p">{</span><span class="s">&#39;output&#39;</span><span class="p">:</span><span class="n">Output</span><span class="p">,</span><span class="s">&#39;target&#39;</span><span class="p">:</span><span class="n">target</span><span class="p">,</span><span class="s">&#39;input&#39;</span><span class="p">:</span><span class="n">Input</span><span class="p">}</span> <span class="c">#record trial output</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Train_Set</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]</span> <span class="c">#the four input patterns</span>
</span><span class='line'><span class="n">tempdict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">output</span><span class="err">’</span><span class="p">:[],</span><span class="err">’</span><span class="n">target</span><span class="err">’</span><span class="p">:[],</span><span class="err">’</span><span class="nb">input</span><span class="err">’</span><span class="p">:[]}</span> <span class="c">#data dictionary</span>
</span><span class='line'><span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="n">neural_net</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights_1</span><span class="p">,</span><span class="n">Weights_2</span><span class="p">)</span> <span class="k">for</span> <span class="n">Input</span> <span class="ow">in</span> <span class="n">Train_Set</span><span class="p">]</span> <span class="c">#get the data</span>
</span><span class='line'><span class="p">[</span><span class="n">tempdict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">temp</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tempdict</span><span class="p">]</span> <span class="c">#combine all the output dictionaries&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plotter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tempdict</span><span class="p">[</span><span class="err">‘</span><span class="n">output</span><span class="err">’</span><span class="p">]),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">plotter</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.75</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="mi">0</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">1</span><span class="err">’</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="mi">0</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">1</span><span class="err">’</span><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/net_guess1_1.png" /></p>

<p>In the plot above, I have Input 1 on the x-axis and Input 2 on the y-axis. So if the Input is [0,0], the network produces the activity depicted in the lower left square. If the Input is [1,0], the network produces the activity depicted in the lower right square. If the network produces an output of 0, then the square will be blue. If the network produces an output of 1, then the square will be red. As you can see, the network produces all output between 0.25 and 0.5… no where near the correct answers.</p>

<p>So how do we update the weights in order to reduce the error between our guess and the correct answer?</p>

<p>First, we will do backpropogation between the output and hidden layers. This is exactly the same as backpropogation in the previous post.</p>

<p>In the previous post I described how our goal was to decrease error by changing the weights between units. This is the equation we used to describe changes in error with changes in the weights. The equation below expresses changes in error with changes to weights between the <script type="math/tex">Hidden_{1}</script> and the Output unit.</p>

<script type="math/tex; mode=display">\frac{\partial Error}{\partial Weight_{Hidden_{1}\to.Output}} = \frac{\partial Error}{\partial Output} * \frac{\partial Output}{\partial net_{Output}} * \frac{\partial net_{Output}}{\partial Weight_{Hidden_{1}\to.Output}}</script>

<script type="math/tex; mode=display">\begin{multline}
\frac{\partial Error}{\partial Weight_{Hidden_{1}\to.Output}} = -(target-Output) * Output(1-Output) * Hidden_{1} \\= -(1-0.34) * 0.34(1-0.34) * 0.22 = -0.03
\end{multline}</script>

<p>Now multiply this weight adjustment by the learning rate.</p>

<script type="math/tex; mode=display">\Delta Weight_{Input_{1}\to.Output} = \alpha * \frac{\partial Error}{\partial Weight_{Input_{1}\to.Output}}</script>

<p>Finally, we apply the weight adjustment to <script type="math/tex">Weight_{Hidden_{1}\to.Output}</script>.</p>

<script type="math/tex; mode=display">Weight_{Hidden_{1}\to.Output}^{\prime} = Weight_{Hidden_{1}\to.Output} - 0.5 * -0.03 = -0.23 - 0.5 * -0.03 = -0.21</script>

<p>Now lets do the same thing, but for both the weights and in the code.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c">#learning rate</span>
</span><span class='line'><span class="n">target</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#target outpu&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">error</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">Output</span> <span class="c">#amount of error</span>
</span><span class='line'><span class="n">delta_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">error</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">(</span><span class="n">Output</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Output</span><span class="p">)))</span> <span class="c">#first two terms of error by weight derivative&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Hidden_Units</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Hidden_Units</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span> <span class="c">#add an input of 1 for the bias</span>
</span><span class='line'><span class="k">print</span> <span class="n">Weights_2</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_out</span><span class="p">,</span><span class="n">Hidden_Units</span><span class="p">)</span> <span class="c">#apply weight change</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>[[-0.21252673 -0.96033892 -0.29229558]]
</code></pre>

<p>The hidden layer changes things when we do backpropogation. Above, we computed the new weights using the output unit’s error. Now, we want to find how adjusting a weight changes the error, but this weight connects an input to the hidden layer rather than connecting to the output layer. This means we have to propogate the error backwards to the hidden layer.</p>

<p>We will describe backpropogation for the line connecting <script type="math/tex">Input_{1}</script> and <script type="math/tex">Hidden_{1}</script> as</p>

<script type="math/tex; mode=display">\frac{\partial Error}{\partial Weight_{Input_{1}\to.Hidden_{1}}} = \frac{\partial Error}{\partial Hidden_{1}} * \frac{\partial Hidden_{1}}{\partial net_{Hidden_{1}}} * \frac{\partial net_{Hidden_{1}}}{\partial Weight_{Input_{1}\to.Hidden_{1}}}</script>

<p>Pretty similar. We just replaced Output with <script type="math/tex">Hidden_{1}</script>. The interpretation (starting with the final term and moving left) is that changing the <script type="math/tex">Weight_{Input_{1}\to.Hidden_{1}}</script> changes <script type="math/tex">Hidden_{1}</script>’s input. Changing <script type="math/tex">Hidden_{1}</script>’s input changes <script type="math/tex">Hidden_{1}</script>’s activity. Changing <script type="math/tex">Hidden_{1}</script>’s activity changes the error. This last assertion (the first term) is where things get complicated. Lets take a closer look at this first term</p>

<script type="math/tex; mode=display">\frac{\partial Error}{\partial Hidden_{1}} = \frac{\partial Error}{\partial net_{Output}} * \frac{\partial net_{Output}}{\partial Hidden_{1}}</script>

<p>Changing <script type="math/tex">Hidden_{1}</script>’s activity changes changes the input to the Output unit. Changing the output unit’s input changes the error. hmmmm still not quite there yet. Lets look at how changes to the output unit’s input changes the error.</p>

<script type="math/tex; mode=display">\frac{\partial Error}{\partial net_{Output}} = \frac{\partial Error}{\partial Output} * \frac{\partial Output}{\partial net_{Output}}</script>

<p>You can probably see where this is going. Changing the output unit’s input changes the output unit’s activity. Changing the output unit’s activity changes error. There we go.</p>

<p>Okay, this got a bit heavy, but here comes some good news. Compare the two terms of the equation above to the first two terms of our original backpropogation equation. They’re the same! Now lets look at <script type="math/tex">\frac{\partial net_{Output}}{\partial Hidden_{1}}</script> (the second term from the first equation after our new backpropogation equation).</p>

<script type="math/tex; mode=display">\frac{\partial net_{Output}}{\partial Hidden_{1}} = Weight_{Hidden_{1}\to Output}</script>

<p>Again, I am glossing over how to derive these partial derivatives. For a more complete explantion, I recommend <a href="http://www-psych.stanford.edu/~jlm/papers/PDP/Volume%201/Chap8_PDP86.pdf">Chapter 8 of Rumelhart and McClelland’s PDP book</a>. Nonetheless, this means we can take the output of our function <em>delta_output</em> multiplied by <script type="math/tex">Weight_{Hidden_{1}\to Output}</script> and we have the first term of our backpropogation equation! We want <script type="math/tex">Weight_{Hidden_{1}\to Output}</script> to be the weight used in the forward pass. Not the updated weight.</p>

<p>The second two terms from our backpropogation equation are the same as in our original backpropogation equation.</p>

<p><script type="math/tex">\frac{\partial Hidden_{1}}{\partial net_{Hidden_{1}}} = Hidden_{1}(1-Hidden_{1})</script> - this is specific to logistic activation functions.</p>

<p>and</p>

<script type="math/tex; mode=display">\frac{\partial net_{Hidden_{1}}}{\partial Weight_{1}} = Input_{1}</script>

<p>Lets try and write this out.</p>

<script type="math/tex; mode=display">\begin{multline}
\frac{\partial Error}{\partial Weight_{Input_{1}\to.Hidden_{1}}} = -(target-Output) * Output(1-Output) * Weight_{Hidden_{1}\to Output}\\* Hidden_{1}(1-Hidden_{1}) * Input_{1}
\end{multline}</script>

<p>It’s not short, but its doable. Let’s plug in the numbers.</p>

<script type="math/tex; mode=display">\frac{\partial Error}{\partial Weight_{Input_{1}\to.Hidden_{1}}} = -(1-0.34)*0.34(1-0.34)*-0.23*0.22(1-0.22)*0 = 0</script>

<p>Not too bad. Now lets see the code.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">delta_hidden</span> <span class="o">=</span> <span class="n">delta_out</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Weights_2</span><span class="p">)</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">(</span><span class="n">Hidden_Units</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Hidden_Units</span><span class="p">))</span> <span class="c">#find delta portion of weight update&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">delta_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">delta_hidden</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c">#remove the bias input</span>
</span><span class='line'><span class="k">print</span> <span class="n">Weights_1</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_hidden</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="mf">1.0</span><span class="p">))</span> <span class="c">#append bias input and multiply input by delta portion</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>[[-0.25119612 -0.50149299 -0.77809147]
 [-0.80193714 -0.23946929 -0.84467792]]
</code></pre>

<p>Alright! Lets implement all of this into a single model and train the model on the XOR problem. Below I create a neural network that includes both a forward pass and an optional backpropogation pass.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">neural_net</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights_1</span><span class="p">,</span><span class="n">Weights_2</span><span class="p">,</span><span class="n">Training</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span> <span class="c">#this function creates and runs the neural net&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">target</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#set target value</span>
</span><span class='line'><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">Input</span><span class="p">[</span><span class="mi">1</span><span class="p">]]):</span> <span class="n">target</span> <span class="o">=</span> <span class="mi">0</span> <span class="c">#change target value if needed</span>
</span><span class='line'>
</span><span class='line'><span class="c">#forward pass</span>
</span><span class='line'><span class="n">Hidden_Units</span> <span class="o">=</span> <span class="n">layer_InputOutput</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights_1</span><span class="p">)</span> <span class="c">#find hidden unit activity</span>
</span><span class='line'><span class="n">Output</span> <span class="o">=</span> <span class="n">layer_InputOutput</span><span class="p">(</span><span class="n">Hidden_Units</span><span class="p">,</span><span class="n">Weights_2</span><span class="p">)</span> <span class="c">#find Output layer actiity</span>
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="n">Training</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
</span><span class='line'>    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c">#learning rate</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Weights_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">Weights_2</span><span class="p">)</span> <span class="c">#make sure this weight vector is 2d.</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">error</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">Output</span> <span class="c">#error</span>
</span><span class='line'>    <span class="n">delta_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">error</span><span class="o">*</span><span class="p">(</span><span class="n">Output</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Output</span><span class="p">)))</span> <span class="c">#delta between output and hidden</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Hidden_Units</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Hidden_Units</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span> <span class="c">#append an input for the bias</span>
</span><span class='line'>    <span class="n">delta_hidden</span> <span class="o">=</span> <span class="n">delta_out</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">Weights_2</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">Hidden_Units</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Hidden_Units</span><span class="p">))</span> <span class="c">#delta between hidden and input</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Weights_2</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_out</span><span class="p">,</span><span class="n">Hidden_Units</span><span class="p">)</span> <span class="c">#update weights</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">delta_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">delta_hidden</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c">#remove bias activity</span>
</span><span class='line'>    <span class="n">Weights_1</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_hidden</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="mf">1.0</span><span class="p">))</span>  <span class="c">#update weights</span>
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="n">Training</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
</span><span class='line'>    <span class="k">return</span> <span class="p">{</span><span class="s">&#39;output&#39;</span><span class="p">:</span><span class="n">Output</span><span class="p">,</span><span class="s">&#39;target&#39;</span><span class="p">:</span><span class="n">target</span><span class="p">,</span><span class="s">&#39;input&#39;</span><span class="p">:</span><span class="n">Input</span><span class="p">}</span> <span class="c">#record trial output</span>
</span><span class='line'><span class="k">elif</span> <span class="n">Training</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
</span><span class='line'>    <span class="k">return</span> <span class="p">{</span><span class="s">&#39;Weights_1&#39;</span><span class="p">:</span><span class="n">Weights_1</span><span class="p">,</span><span class="s">&#39;Weights_2&#39;</span><span class="p">:</span><span class="n">Weights_2</span><span class="p">,</span><span class="s">&#39;target&#39;</span><span class="p">:</span><span class="n">target</span><span class="p">,</span><span class="s">&#39;output&#39;</span><span class="p">:</span><span class="n">Output</span><span class="p">,</span><span class="s">&#39;error&#39;</span><span class="p">:</span><span class="n">error</span><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p>Okay, thats the network. Below, I train the network until its answers are very close to the correct answer.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">choice</span>
</span><span class='line'><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c">#seed random number generator for reproducibility&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Weights_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">2</span> <span class="c">#connections between hidden and output</span>
</span><span class='line'><span class="n">Weights_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">2</span> <span class="c">#connections between input and hidden&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Weight_Dict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_1</span><span class="p">,</span><span class="err">’</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_2</span><span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Train_Set</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]</span> <span class="c">#train set&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Error</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">while</span> <span class="bp">True</span><span class="p">:</span> <span class="c">#train the neural net</span>
</span><span class='line'>    <span class="n">Train_Dict</span> <span class="o">=</span> <span class="n">neural_net</span><span class="p">(</span><span class="n">choice</span><span class="p">(</span><span class="n">Train_Set</span><span class="p">),</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">],</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">],</span><span class="n">Training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">Train_Dict</span><span class="p">[</span><span class="s">&#39;error&#39;</span><span class="p">]))</span>
</span><span class='line'><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Error</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="mi">6</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Error</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mf">0.025</span><span class="p">:</span> <span class="k">break</span> <span class="c">#tell the code to stop iterating when recent mean error is small </span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p>Lets see how error changed across training</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Error_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Error</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Error_vec</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Error</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Iteration</span> <span class="c">#’);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/net_learn1_1.png" /></p>

<p>Really cool. The network start with volatile error - sometimes being nearly correct ans sometimes being completely incorrect. Then After about 5000 iterations, the network starts down the slow path of perfecting an answer scheme. Below, I create a plot depicting the networks’ activity for the different input patterns.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Weights_1</span> <span class="o">=</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="n">Weights_2</span> <span class="o">=</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Train_Set</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]</span> <span class="c">#train set&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">tempdict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">output</span><span class="err">’</span><span class="p">:[],</span><span class="err">’</span><span class="n">target</span><span class="err">’</span><span class="p">:[],</span><span class="err">’</span><span class="nb">input</span><span class="err">’</span><span class="p">:[]}</span> <span class="c">#data dictionary</span>
</span><span class='line'><span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="n">neural_net</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights_1</span><span class="p">,</span><span class="n">Weights_2</span><span class="p">)</span> <span class="k">for</span> <span class="n">Input</span> <span class="ow">in</span> <span class="n">Train_Set</span><span class="p">]</span> <span class="c">#get the data</span>
</span><span class='line'><span class="p">[</span><span class="n">tempdict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">temp</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tempdict</span><span class="p">]</span> <span class="c">#combine all the output dictionaries&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plotter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tempdict</span><span class="p">[</span><span class="err">‘</span><span class="n">output</span><span class="err">’</span><span class="p">]),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">plotter</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.75</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="mi">0</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">1</span><span class="err">’</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="mi">0</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">1</span><span class="err">’</span><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/net_guess2_1.png" /></p>

<p>Again, the Input 1 value is on the x-axis and the Input 2 value is on the y-axis. As you can see, the network guesses 1 when the inputs are different and it guesses 0 when the inputs are the same. Perfect! Below I depict the network with these correct weights.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Weight_Dict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_1</span><span class="p">,</span><span class="err">’</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_2</span><span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">],</span>
</span><span class='line'>                  <span class="p">[[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">]],</span>
</span><span class='line'>                   <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">]]])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)],</span>
</span><span class='line'>                  <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][:</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/3_layer_weights1.png" /></p>

<p>The network finds a pretty cool solution. Both hidden units are relatively active, but one hidden unit sends a strong postitive signal and the other sends a strong negative signal. The output unit has a negative bias, so if neither input is on, it will have an activity around 0. If both Input units are on, then the hidden unit that sends a postitive signal will be inhibited, and the output unit will have activity near 0. Otherwise, the hidden unit with a positive signal gives the output unit an acitivty near 1.</p>

<p>This is all well and good, but if you try to train this network with random weights you might find that it produces an incorrect set of weights sometimes. This is because the network runs into a <a href="https://en.wikipedia.org/wiki/Maxima_and_minima">local minima</a>. A local minima is an instance when any change in the weights would increase the error, so the network is left with a sub-optimal set of weights.</p>

<p>Below I hand-pick of set of weights that produce a local optima.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Weights_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span><span class="mf">5.3</span><span class="p">,</span><span class="o">-</span><span class="mf">0.8</span><span class="p">])</span> <span class="c">#connections between hidden and output</span>
</span><span class='line'><span class="n">Weights_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">9.2</span><span class="p">,</span><span class="mf">2.0</span><span class="p">],</span>
</span><span class='line'>                     <span class="p">[</span><span class="mf">4.3</span><span class="p">,</span><span class="mf">8.8</span><span class="p">,</span><span class="o">-</span><span class="mf">0.1</span><span class="p">]])</span><span class="c">#connections between input and hidden&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Weight_Dict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_1</span><span class="p">,</span><span class="err">’</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_2</span><span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">],</span>
</span><span class='line'>                  <span class="p">[[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">]],</span>
</span><span class='line'>                   <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">]]])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)],</span>
</span><span class='line'>                  <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][:</span><span class="mi">2</span><span class="p">]])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/local_minimaWeights.png" /></p>

<p>Using these weights as the start of the training set, lets see what the network will do with training.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Train_Set</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]</span> <span class="c">#train set&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Error</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class='line'>    <span class="n">Train_Dict</span> <span class="o">=</span> <span class="n">neural_net</span><span class="p">(</span><span class="n">choice</span><span class="p">(</span><span class="n">Train_Set</span><span class="p">),</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">],</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">],</span><span class="n">Training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">Train_Dict</span><span class="p">[</span><span class="s">&#39;error&#39;</span><span class="p">]))</span>
</span><span class='line'><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Error</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="mi">6</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Error</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mf">0.025</span><span class="p">:</span> <span class="k">break</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Error_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Error</span><span class="p">)[:]</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Error_vec</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Error</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Iteration</span> <span class="c">#’);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/net_learn2_1.png" /></p>

<p>As you can see the network never reduces error. Let’s see how the network answers to the different input patterns.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Weights_1</span> <span class="o">=</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="n">Weights_2</span> <span class="o">=</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Train_Set</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]</span> <span class="c">#train set&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">tempdict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">output</span><span class="err">’</span><span class="p">:[],</span><span class="err">’</span><span class="n">target</span><span class="err">’</span><span class="p">:[],</span><span class="err">’</span><span class="nb">input</span><span class="err">’</span><span class="p">:[]}</span> <span class="c">#data dictionary</span>
</span><span class='line'><span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="n">neural_net</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights_1</span><span class="p">,</span><span class="n">Weights_2</span><span class="p">)</span> <span class="k">for</span> <span class="n">Input</span> <span class="ow">in</span> <span class="n">Train_Set</span><span class="p">]</span> <span class="c">#get the data</span>
</span><span class='line'><span class="p">[</span><span class="n">tempdict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">temp</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tempdict</span><span class="p">]</span> <span class="c">#combine all the output dictionaries&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plotter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tempdict</span><span class="p">[</span><span class="err">‘</span><span class="n">output</span><span class="err">’</span><span class="p">]),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">plotter</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.75</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="mi">0</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">1</span><span class="err">’</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="mi">0</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">1</span><span class="err">’</span><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/net_guess3_1.png" /></p>

<p>Looks like the network produces the correct answer in some cases but not others. The network is particularly confused when Inputs 2 is 0. Below I depict the weights after “training.” As you can see, they have not changed too much from where the weights started before training.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Weights_1</span> <span class="o">=</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="n">Weights_2</span> <span class="o">=</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Weight_Dict</span> <span class="o">=</span> <span class="p">{</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_1</span><span class="p">,</span><span class="err">’</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">:</span><span class="n">Weights_2</span><span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="err">‘</span><span class="n">Input</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Input</span> <span class="mi">2</span><span class="err">’</span><span class="p">],</span>
</span><span class='line'>                  <span class="p">[[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">]],</span>
</span><span class='line'>                   <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">]]])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_1</span><span class="err">’</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)],</span>
</span><span class='line'>                  <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][:</span><span class="mi">2</span><span class="p">]])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">,[</span><span class="nb">round</span><span class="p">(</span><span class="n">Weight_Dict</span><span class="p">[</span><span class="err">‘</span><span class="n">Weights_2</span><span class="err">’</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="mi">2</span><span class="p">)])</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/local_minimaWeights1.png" /></p>

<p>This network was unable to push itself out of the local optima. While local optima are a problem, they’re are a couple things we can do to avoid them. First, we should always train a network multiple times with different random weights in order to test for local optima. If the network continually finds local optima, then we can increase the learning rate. By increasing the learning rate, the network can escape local optima in some cases. This should be done with care though as too big of a learning rate can also prevent finding the global minima.</p>

<p>Alright, that’s it. Obviously the neural network behind <a href="https://en.wikipedia.org/wiki/AlphaGo">alpha go</a> is much more complex than this one, but I would guess that while alpha go is much larger the basic computations underlying it are similar.</p>

<p>Hopefully these posts have given you an idea for how neural networks function and why they’re so cool!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to Neural Networks: Part 1]]></title>
    <link href="http://www.danvatterott.com/blog/2016/04/29/an-introduction-to-neural-networks-part-1/"/>
    <updated>2016-04-29T19:17:07-04:00</updated>
    <id>http://www.danvatterott.com/blog/2016/04/29/an-introduction-to-neural-networks-part-1</id>
    <content type="html"><![CDATA[<p>We use our most advanced technologies as metaphors for the brain: The industrial revolution inspired descriptions of the brain as mechanical. The telephone inspired descriptions of the brain as a telephone switchboard. The computer inspired descriptions of the brain as a computer. Recently, we have reached a point where our most advanced technologies - such as AI (e.g., <a href="https://en.wikipedia.org/wiki/AlphaGo">Alpha Go</a>), and our current understanding of the brain inform each other in an awesome synergy. Neural networks exemplify this synergy. Neural networks offer a relatively advanced description of the brain and are the software underlying some of our most advanced technology. As our understanding of the brain increases, neural networks become more sophisticated. As our understanding of neural networks increases, our understanding of the brain becomes more sophisticated.</p>

<p>With the recent success of neural networks, I thought it would be useful to write a few posts describing the basics of neural networks.</p>

<p>First, what are <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural networks</a> - neural networks are a family of machine learning algorithms that can learn data’s underlying structure. Neural networks are composed of many <em>neurons</em> that perform simple computations. By performing many simple computations, neural networks can answer even the most complicated problems.</p>

<p>Lets get started.</p>

<p>As usual, I will post this code as a jupyter notebook on <a href="https://github.com/dvatterott/jupyter_notebooks">my github</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="c">#import important libraries. </span>
</span><span class='line'><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>When talking about neural networks, it’s nice to visualize the network with a figure. For drawing the neural networks, I forked a <a href="https://github.com/miloharper/visualise-neural-network">repository from miloharper</a> and made some changes so that this repository could be imported into python and so that I could label the network. <a href="https://github.com/dvatterott/visualise_neural_network">Here</a> is my forked repository.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">visualise_neural_network</span> <span class="kn">import</span> <span class="n">NeuralNetwork</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span> <span class="c">#create neural network object</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">2</span><span class="p">,[</span><span class="err">‘</span><span class="n">Input</span> <span class="n">A</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Input</span> <span class="n">B</span><span class="err">’</span><span class="p">],[</span><span class="err">‘</span><span class="n">Weight</span> <span class="n">A</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Weight</span> <span class="n">B</span><span class="err">’</span><span class="p">])</span> <span class="c">#create the input layer which has two neurons.</span>
</span><span class='line'><span class="c">#Each input neuron has a single line extending to the next layer up</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">add_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">,[</span><span class="err">‘</span><span class="n">Output</span><span class="err">’</span><span class="p">])</span> <span class="c">#create output layer - a single output neuron</span>
</span><span class='line'><span class="n">network</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span> <span class="c">#draw the network</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/example1.png" /></p>

<p>Above is our neural network. It has two input neurons and a single output neuron. In this example, I’ll give the network an input of [0 1]. This means Input A will receive an input value of 0 and Input B will have an input value of 1.</p>

<p>The input is the input unit’s <em>activity.</em> This activity is sent to the Output unit, but the activity changes when traveling to the Output unit. The <em>weights</em> between the input and output units change the activity. A large positive weight between the input and output units causes the input unit to send a large positive (excitatory) signal. A large negative weight between the input and output units causes the input unit to send a large negative (inhibitory) signal. A weight near zero means the input unit does not influence the output unit.</p>

<p>In order to know the Output unit’s activity, we need to know its input. I will refer to the output unit’s input as <script type="math/tex">net_{Output}</script>. Here is how we can calculate <script type="math/tex">net_{Output}</script></p>

<script type="math/tex; mode=display">net_{Output} = Input_A * Weight_A + Input_B * Weight_B</script>

<p>a more general way of writing this is</p>

<script type="math/tex; mode=display">net = \displaystyle\sum_{i=1}^{Inputs}Input_i * Weight_i</script>

<p>Let’s pretend the inputs are [0 1] and the Weights are [0.25 0.5]. Here is the input to the output neuron -</p>

<script type="math/tex; mode=display">net_{Output} = 0 * 0.25 + 1 * 0.5</script>

<p>Thus, the input to the output neuron is 0.5. A quick way of programming this is through the function numpy.dot which finds the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> of two vectors (or matrices). This might sound a little scary, but in this case its just multiplying the items by each other and then summing everything up - like we did above.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span><span class='line'><span class="n">Weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">net_Output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Inputs</span><span class="p">,</span><span class="n">Weights</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="n">net_Output</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>0.5
</code></pre>

<p>All this is good, but we haven’t actually calculated the output unit’s activity we have only calculated its input. What makes neural networks able to solve complex problems is they include a non-linearity when translating the input into activity. In this case we will translate the input into activity by putting the input through a <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a>.</p>

<script type="math/tex; mode=display">Logistic = \frac{1}{1+e^{-x}}</script>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c">#each neuron has a logistic activation function</span>
</span><span class='line'>    <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Lets take a look at a logistic function.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span> <span class="c">#create vector of numbers between -5 and 5</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Activation</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span><span class="err">’</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/logistic1.png" /></p>

<p>As you can see above, the logistic used here transforms negative values into values near 0 and positive values into values near 1. Thus, when a unit receives a negative input it has activity near zero and when a unit receives a postitive input it has activity near 1. The most important aspect of this activation function is that its non-linear - it’s not a straight line.</p>

<p>Now lets see the activity of our output neuron. Remember, the net input is 0.5</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Output_neuron</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">net_Output</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="n">Output_neuron</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">));</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Activation</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">net_Output</span><span class="p">,</span><span class="n">Output_neuron</span><span class="p">,</span><span class="err">’</span><span class="n">ro</span><span class="err">’</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>0.622459331202
</code></pre>

<p><img src="/images/neural_net/logistic2.png" /></p>

<p>The activity of our output neuron is depicted as the red dot.</p>

<p>So far I’ve described how to find a unit’s activity, but I haven’t described how to find the weights of connections between units. In the example above, I chose the weights to be 0.25 and 0.5, but I can’t arbitrarily decide weights unless I already know the solution to the problem. If I want the network to find a solution for me, I need the network to find the weights itself.</p>

<p>In order to find the weights of connections between neurons, I will use an algorithm called <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropogation</a>. In backpropogation, we have the neural network guess the answer to a problem and adjust the weights so that this guess gets closer and closer to the correct answer. Backpropogation is the method by which we reduce the distance between guesses and the correct answer. After many iterations of guesses by the neural network and weight adjustments through backpropogation, the network can learn an answer to a problem.</p>

<p>Lets say we want our neural network to give an answer of 0 when the left input unit is active and an answer of 1 when the right unit is active. In this case the inputs I will use are [1,0] and [0,1]. The corresponding correct answers will be [0] and [1].</p>

<p>Lets see how close our network is to the correct answer. I am using the weights from above ([0.25, 0.5]).</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Inputs</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
</span><span class='line'><span class="n">Answers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Guesses</span> <span class="o">=</span> <span class="p">[</span><span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Weights</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Inputs</span><span class="p">]</span> <span class="c">#loop through inputs and find logistic(sum(input*weights))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Guesses</span><span class="p">,</span><span class="err">’</span><span class="n">bo</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Answers</span><span class="p">,</span><span class="err">’</span><span class="n">ro</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Activation</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="c">#’)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="err">‘</span><span class="n">Guesses</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Answers</span><span class="err">’</span><span class="p">]);</span>
</span><span class='line'><span class="k">print</span> <span class="n">Guesses</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>[0.56217650088579807, 0.62245933120185459]
</code></pre>

<p><img src="/images/neural_net/net_guess1.png" /></p>

<p>The guesses are in blue and the answers are in red. As you can tell, the guesses and the answers look almost nothing alike. Our network likes to guess around 0.6 while the correct answer is 0 in the first example and 1 in the second.</p>

<p>Lets look at how backpropogation reduces the distance between our guesses and the correct answers.</p>

<p>First, we want to know how the amount of error changes with an adjustment to a given weight. We can write this as</p>

<script type="math/tex; mode=display">\partial Error \over \partial Weight_{Input_{1}\to.Output}</script>

<p>This change in error with changes in the weights has a number of different sub components.</p>

<ul>
  <li>Changes in error with changes in the output unit’s activity: <script type="math/tex">\partial Error \over \partial Output</script></li>
  <li>Changes in the output unit’s activity with changes in this unit’s input: <script type="math/tex">\partial Output \over \partial net_{Output}</script></li>
  <li>Changes in the output unit’s input with changes in the weight: <script type="math/tex">\partial net_{Output} \over \partial Weight_{Input_{1}\to.Output}</script></li>
</ul>

<p>Through the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> we know</p>

<script type="math/tex; mode=display">\frac{\partial Error}{\partial Weight_{Input_{1}\to.Output}} = \frac{\partial Error}{\partial Output} * \frac{\partial Output}{\partial net_{Output}} * \frac{\partial net_{Output}}{\partial Weight_{Input_{1}\to.Output}}</script>

<p>This might look scary, but with a little thought it should make sense: (starting with the final term and moving left) When we change the weight of a connection to a unit, we change the input to that unit. When we change the input to a unit, we change its activity (written Output above). When we change a units activity, we change the amount of error.</p>

<p>Let’s break this down using our example. During this portion, I am going to gloss over some details about how exactly to derive the partial derivatives. <a href="https://en.wikipedia.org/wiki/Delta_rule">Wikipedia has a more complete derivation</a>.</p>

<p>In the first example, the input is [1,0] and the correct answer is [0]. Our network’s guess in this example was about 0.56.</p>

<script type="math/tex; mode=display">\frac{\partial Error}{\partial Output} = -(target-Output) = -(0-0.56)</script>

<script type="math/tex; mode=display">\frac{\partial Output}{\partial net_{Output}} = Output(1-Output) = 0.56*(1-0.56)</script>

<p>Please note that this is specific to our example with a logistic activation function</p>

<script type="math/tex; mode=display">\frac{\partial net_{Output}}{\partial Weight_{Input_{1}\to.Output}} = Input_{1} = 1</script>

<p>To summarize:</p>

<script type="math/tex; mode=display">\begin{multline}
\frac{\partial Error}{\partial Weight_{Input_{1}\to.Output}} = -(target-Output) * Output(1-Output) * Input_{1} \\
= -(0-0.56) * 0.56(1-0.56) * 1 = 0.14
\end{multline}</script>

<p>This is the direction we want to move in, but taking large steps in this direction can prevent us from finding the optimal weights. For this reason, we reduce our step size. We will reduce our step size with a parameter called the <em>learning rate</em> (<script type="math/tex">\alpha</script>). <script type="math/tex">\alpha</script> is bound between 0 and 1.</p>

<p>Here is how we can write our change in weights</p>

<script type="math/tex; mode=display">\Delta Weight_{Input_{1}\to.Output} = \alpha * \frac{\partial Error}{\partial Weight_{Input_{1}\to.Output}}</script>

<p>This is known as the <a href="https://en.wikipedia.org/wiki/Delta_rule">delta rule</a>.</p>

<p>We will set <script type="math/tex">\alpha</script> to be 0.5. Here is how we will calculate the new <script type="math/tex">Weight_{Input_{1}\to.Output}</script>.</p>

<script type="math/tex; mode=display">Weight_{Input_{1}\to.Output}^{\prime} = Weight_{Input_{1}\to.Output} - 0.5 * 0.14 = 0.25 - 0.5 * 0.14 = 0.18</script>

<p>Thus, <script type="math/tex">Weight_{Input_{1}\to.Output}</script> is shrinking which will move the output towards 0. Below I write the code to implement our backpropogation.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">delta_Output</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="n">Output</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">target</span><span class="o">-</span><span class="n">Output</span><span class="p">)</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="n">Output</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Output</span><span class="p">)</span> <span class="c">#find the amount of error and derivative of activation function&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">delta</span><span class="p">,</span><span class="n">unit_input</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span><span class="n">unit_input</span><span class="p">)</span> <span class="c">#multiply delta output by all the inputs and then multiply these by the learning rate </span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Above I use the <a href="https://en.wikipedia.org/wiki/Outer_product">outer product</a> of our delta function and the input in order to spread the weight changes to all lines connecting to the output unit.</p>

<p>Okay, hopefully you made it through that. I promise thats as bad as it gets. Now that we’ve gotten through the nasty stuff, lets use backpropogation to find an answer to our problem.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">network_guess</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="c">#input by weights then through a logistic&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Output</span><span class="p">,</span><span class="n">target</span><span class="p">,</span><span class="n">Weights</span><span class="p">):</span>
</span><span class='line'>    <span class="n">delta</span> <span class="o">=</span> <span class="n">delta_Output</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="n">Output</span><span class="p">)</span> <span class="c">#find delta portion</span>
</span><span class='line'>    <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">delta</span><span class="p">,</span><span class="n">Input</span><span class="p">)</span> <span class="c">#find amount to update weights</span>
</span><span class='line'>    <span class="n">Weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">Weights</span><span class="p">)</span> <span class="c">#convert weights to array</span>
</span><span class='line'>    <span class="n">Weights</span> <span class="o">+=</span> <span class="o">-</span><span class="n">delta_weight</span> <span class="c">#update weights</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">Weights</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">choice</span><span class="p">,</span> <span class="n">seed</span>
</span><span class='line'><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c">#seed random number generator so that these results can be replicated&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Error</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">while</span> <span class="bp">True</span><span class="p">:</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Trial_Type</span> <span class="o">=</span> <span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c">#generate random number to choose between the two inputs</span>
</span><span class='line'>
</span><span class='line'><span class="n">Input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">Inputs</span><span class="p">[</span><span class="n">Trial_Type</span><span class="p">])</span> <span class="c">#choose input and convert to array</span>
</span><span class='line'><span class="n">Answer</span> <span class="o">=</span> <span class="n">Answers</span><span class="p">[</span><span class="n">Trial_Type</span><span class="p">]</span> <span class="c">#get the correct answer</span>
</span><span class='line'>
</span><span class='line'><span class="n">Output</span> <span class="o">=</span> <span class="n">network_guess</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Weights</span><span class="p">)</span> <span class="c">#compute the networks guess</span>
</span><span class='line'><span class="n">Weights</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">Output</span><span class="p">,</span><span class="n">Answer</span><span class="p">,</span><span class="n">Weights</span><span class="p">)</span> <span class="c">#change the weights based on the error</span>
</span><span class='line'>
</span><span class='line'><span class="n">Error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">Output</span><span class="o">-</span><span class="n">Answer</span><span class="p">))</span> <span class="c">#record error</span>
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Error</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="mi">6</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Error</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:])</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mf">0.05</span><span class="p">:</span> <span class="k">break</span> <span class="c">#tell the code to stop iterating when mean error is &amp;lt; 0.05 in the last 5 guesses </span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p>It seems our code has found an answer, so lets see how the amount of error changed as the code progressed.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Error_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Error</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Error_vec</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Error</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Iteration</span> <span class="c">#’);&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/neural_net/net_learn1.png" /></p>

<p>It looks like the while loop excecuted about 1000 iterations before converging. As you can see the error decreases. Quickly at first then slowly as the weights zone in on the correct answer. lets see how our guesses compare to the correct answers.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Inputs</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
</span><span class='line'><span class="n">Answers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Guesses</span> <span class="o">=</span> <span class="p">[</span><span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Weights</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Inputs</span><span class="p">]</span> <span class="c">#loop through inputs and find logistic(sum(input*weights))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Guesses</span><span class="p">,</span><span class="err">’</span><span class="n">bo</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Answers</span><span class="p">,</span><span class="err">’</span><span class="n">ro</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Activation</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Input</span> <span class="c">#’)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="err">‘</span><span class="n">Guesses</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Answers</span><span class="err">’</span><span class="p">]);</span>
</span><span class='line'><span class="k">print</span> <span class="n">Guesses</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>[array([ 0.05420561]), array([ 0.95020512])]
</code></pre>

<p><img src="/images/neural_net/net_guess2.png" /></p>

<p>Not bad! Our guesses are much closer to the correct answers than before we started running the backpropogation procedure! Now, you might say, “HEY! But you haven’t reached the <em>correct</em> answers.” That’s true, but note that acheiving the values of 0 and 1 with a logistic function are only possible at -<script type="math/tex">\infty</script> and <script type="math/tex">\infty</script>, respectively. Because of this, we treat 0.05 as 0 and 0.95 as 1.</p>

<p>Okay, all this is great, but that was a really simple problem, and I said that neural networks could solve interesting problems!</p>

<p>Well… this post is already longer than I anticipated. I will follow-up this post with another post explaining how we can expand neural networks to solve more interesting problems.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Revisiting NBA Career Predictions From Rookie Performance]]></title>
    <link href="http://www.danvatterott.com/blog/2016/04/08/revisiting-nba-career-predictions-from-rookie-performance/"/>
    <updated>2016-04-08T21:19:25-04:00</updated>
    <id>http://www.danvatterott.com/blog/2016/04/08/revisiting-nba-career-predictions-from-rookie-performance</id>
    <content type="html"><![CDATA[<p>In this post I wanted to do a quick follow up to a previous post about <a href="http://www.danvatterott.com/blog/2016/03/20/predicting-career-performance-from-rookie-performance/">predicting career nba performance from rookie year data</a>.</p>

<p>After my previous post, I started to get a little worried about my career prediction model. Specifically, I started to wonder about whether my model was <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">underfitting or overfitting the data</a>. Underfitting occurs when the model has too much “bias” and cannot accomodate the data’s shape. <a href="https://en.wikipedia.org/wiki/Overfitting">Overfitting</a> occurs when the model is too flexible and can account for all variance in a data set - even variance due to noise. In this post, I will quickly re-create my player prediction model, and investigate whether underfitting and overfitting are a problem.</p>

<p>Because this post largely repeats a previous one, I haven’t written quite as much about the code. If you would like to read more about the code, see my previous posts.</p>

<p>As usual, I will post all code as a jupyter notebook on my <a href="https://github.com/dvatterott/jupyter_notebooks/blob/master/nba_rookie_regression2.ipynb">github</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#import some libraries and tell ipython we want inline figures rather than interactive figures. </span>
</span><span class='line'><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span><span class="o">,</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span><span class="o">,</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="o">,</span> <span class="nn">matplotlib</span> <span class="kn">as</span> <span class="nn">mpl</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="kn">from</span> <span class="o">&lt;</span><span class="n">strong</span><span class="o">&gt;</span><span class="n">future</span><span class="o">&lt;/</span><span class="n">strong</span><span class="o">&gt;</span> <span class="kn">import</span> <span class="nn">print_function</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span><span class='line'><span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">mpl_style</span> <span class="o">=</span> <span class="err">‘</span><span class="n">default</span><span class="err">’</span> <span class="c">#load matplotlib for plotting</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="err">‘</span><span class="n">ggplot</span><span class="err">’</span><span class="p">)</span> <span class="c">#im addicted to ggplot. so pretty.</span>
</span><span class='line'><span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="err">‘</span><span class="n">font</span><span class="o">.</span><span class="n">family</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="err">‘</span><span class="n">Bitstream</span> <span class="n">Vera</span> <span class="n">Sans</span><span class="err">’</span><span class="p">]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Load the data. Reminder - this data is still available on my <a href="https://github.com/dvatterott/nba_project">github</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">rookie_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="err">‘</span><span class="n">nba_bballref_rookie_stats_2016_Mar_15</span><span class="o">.</span><span class="n">pkl</span><span class="err">’</span><span class="p">)</span> <span class="c">#here’s the rookie year data&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">rook_games</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="p">[</span><span class="err">‘</span><span class="n">Career</span> <span class="n">Games</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">50</span>
</span><span class='line'><span class="n">rook_year</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="p">[</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span> <span class="nb">id</span><span class="o">=</span><span class="s">&quot;remove-rookies-from-before-1980-and-who-have-played-less-than-50-games-i-also-remove-some-features-that-seem-irrelevant-or-unfair&quot;</span><span class="o">&gt;</span><span class="n">remove</span> <span class="n">rookies</span> <span class="kn">from</span> <span class="nn">before</span> <span class="mi">1980</span> <span class="ow">and</span> <span class="n">who</span> <span class="n">have</span> <span class="n">played</span> <span class="n">less</span> <span class="n">than</span> <span class="mi">50</span> <span class="n">games</span><span class="o">.</span> <span class="n">I</span> <span class="n">also</span> <span class="n">remove</span> <span class="n">some</span> <span class="n">features</span> <span class="n">that</span> <span class="n">seem</span> <span class="n">irrelevant</span> <span class="ow">or</span> <span class="n">unfair</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">rookie_df_games</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="p">[</span><span class="n">rook_games</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">rook_year</span><span class="p">]</span> <span class="c">#only players with more than 50 games. </span>
</span><span class='line'><span class="n">rookie_df_drop</span> <span class="o">=</span> <span class="n">rookie_df_games</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Career</span> <span class="n">Games</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Name</span><span class="err">’</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Load more data, and normalize it data for the <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA transformation</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="err">‘</span><span class="n">nba_bballref_career_stats_2016_Mar_15</span><span class="o">.</span><span class="n">pkl</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">G</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">50</span><span class="p">]</span>
</span><span class='line'><span class="n">df_drop</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Name</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">G</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">GS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">MP</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FG</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FGA</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FG</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">3</span><span class="n">P</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">2</span><span class="n">P</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FT</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TRB</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">PTS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">ORtg</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DRtg</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">PER</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TS</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">3</span><span class="n">PAr</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FTr</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">ORB</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DRB</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TRB</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">AST</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">STL</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">BLK</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TOV</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">USG</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">OWS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DWS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">WS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">OBPM</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DBPM</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">BPM</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">VORP</span><span class="err">’</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">df_drop</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe</span>
</span><span class='line'><span class="n">ScaleModel</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">ScaleModel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Use <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means</a> to group players according to their performance. See my post on <a href="http://www.danvatterott.com/blog/2016/02/21/grouping-nba-players/">grouping players</a> for more info.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">reduced_model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">reduced_data</span> <span class="o">=</span> <span class="n">reduced_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c">#transform data into the 5 PCA components space</span>
</span><span class='line'><span class="n">final_fit</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">reduced_data</span><span class="p">)</span> <span class="c">#fit 6 clusters</span>
</span><span class='line'><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span> <span class="c">#label each data point with its clusters</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Run a separate regression on each group of players. I calculate mean absolute error (a variant of <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a>) for each model. I used mean absolute error because it’s on the same scale as the data, and easier to interpret. I will use this later to evaluate just how accurate these models are. Quick reminder - I am trying to predict career WS/48 with MANY predictor variables from rookie year performance such rebounding and scoring statistics.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="kn">as</span> <span class="nn">sm</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span> <span class="c">#import function for calculating mean squared error.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">X</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span><span class="p">][</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="n">rookie_df_drop</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_labels</span> <span class="c">#label each data point with its clusters&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">estHold</span> <span class="o">=</span> <span class="p">[[],[],[],[],[],[]]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">score</span> <span class="o">=</span> <span class="p">[]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span><span class="p">)):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Grouper</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span> <span class="c">#do one regression at a time</span>
</span><span class='line'><span class="n">Yearer</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Year&#39;</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">Grouper</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">Yearer</span><span class="p">]</span>
</span><span class='line'><span class="n">Y</span> <span class="o">=</span> <span class="n">Group1</span><span class="p">[</span><span class="s">&#39;WS/48&#39;</span><span class="p">]</span> <span class="c">#get predictor data</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">rookie_df_drop</span><span class="p">[</span><span class="n">rookie_df_drop</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span><span class="p">]</span>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span> <span class="c">#get predicted data</span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe    </span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># Adds a constant term to the predictor</span>
</span><span class='line'><span class="n">est</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c">#fit with linear regression model</span>
</span><span class='line'><span class="n">est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</span><span class='line'><span class="n">estHold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">est</span>
</span><span class='line'><span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span> <span class="c">#calculate the mean squared error</span>
</span><span class='line'><span class="c">#print est.summary()</span>
</span><span class='line'>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c">#plot each regression&#39;s prediction against actual data</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">Y</span><span class="p">,</span><span class="s">&#39;o&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">&#39;axes.color_cycle&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.01</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.01</span><span class="p">),</span><span class="s">&#39;-&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Group </span><span class="si">%d</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span><span class="s">&#39;$r^2$=</span><span class="si">%.2f</span><span class="s">&#39;</span><span class="o">%</span><span class="n">est</span><span class="o">.</span><span class="n">rsquared</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.12</span><span class="p">,</span><span class="mf">0.25</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.12</span><span class="p">,</span><span class="mf">0.25</span><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p><img src="/images/regression2NBA/original_model.png" /></p>

<p>More quick reminders - predicted performances are on the Y-axis, actual performances are on the X-axis, and the red line is the <a href="https://en.wikipedia.org/wiki/Identity_line">identity line</a>. Thus far, everything has been exactly the same as my previous post (although my group labels are different).</p>

<p>I want to investigate whether the model is overfitting the data. If the data is overfitting the data, then the error should go up when training and testing with different datasets (because the model was fitting itself to noise and noise changes when the datasets change). To investigate whether the model overfits the data, I will evaluate whether the model “generalizes” via <a href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">cross-validation</a>.</p>

<p>The reason I’m worried about overfitting is I used a LOT of predictors in these models and the number of predictors might have allowed the model the model to fit noise in the predictors.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span> <span class="c">#I am using sklearns linear regression because it plays well with their cross validation function</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span> <span class="c">#import the cross validation function&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">X</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span><span class="p">][</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="n">rookie_df_drop</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_labels</span> <span class="c">#label each data point with its clusters&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span><span class="p">)):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Grouper</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span> <span class="c">#do one regression at a time</span>
</span><span class='line'><span class="n">Yearer</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Year&#39;</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">Grouper</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">Yearer</span><span class="p">]</span>
</span><span class='line'><span class="n">Y</span> <span class="o">=</span> <span class="n">Group1</span><span class="p">[</span><span class="s">&#39;WS/48&#39;</span><span class="p">]</span> <span class="c">#get predictor data</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">rookie_df_drop</span><span class="p">[</span><span class="n">rookie_df_drop</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span><span class="p">]</span>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span> <span class="c">#get predicted data</span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe    </span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># Adds a constant term to the predictor</span>
</span><span class='line'>
</span><span class='line'><span class="n">est</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span> <span class="c">#fit with linear regression model</span>
</span><span class='line'>
</span><span class='line'><span class="n">this_scores</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;mean_absolute_error&#39;</span><span class="p">)</span> <span class="c">#find mean square error across different datasets via cross validations</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="s">&#39;Group &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="s">&#39;Initial Mean Absolute Error: &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">])</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="s">&#39;Cross Validation MAE: &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">this_scores</span><span class="p">)))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">])</span> <span class="c">#find the mean MSE across validations </span>
</span></code></pre></td></tr></table></div></figure>

Group 0
Initial Mean Absolute Error: 0.0161
Cross Validation MAE: 0.0520
Group 1
Initial Mean Absolute Error: 0.0251
Cross Validation MAE: 0.0767
Group 2
Initial Mean Absolute Error: 0.0202
Cross Validation MAE: 0.0369
Group 3
Initial Mean Absolute Error: 0.0200
Cross Validation MAE: 0.0263
Group 4
Initial Mean Absolute Error: 0.0206
Cross Validation MAE: 0.0254
Group 5
Initial Mean Absolute Error: 0.0244
Cross Validation MAE: 0.0665
</code></pre>

<p>Above I print out the model’s initial mean absolute error and median absolute error when fitting cross-validated data.</p>

<p>The models definitely have more error when cross validated. The change in error is worse in some groups than others. For instance, error dramatically increases in Group 1. Keep in mind that the scoring measure here is mean absolute error, so error is in the same scale as WS/48. An average error of 0.04 in WS/48 is sizable, leaving me worried that the models overfit the data.</p>

<p>Unfortunately, Group 1 is the “scorers” group, so the group with most the interesting players is where the model fails most…</p>

<p>Next, I will look into whether my models underfit the data. I am worried that my models underfit the data because I used linear regression, which has very little flexibility. To investigate this, I will plot the <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">residuals</a> of each model. Residuals are the error between my model’s prediction and the actual performance.</p>

<p>Linear regression assumes that residuals are uncorrelated and evenly distributed around 0. If this is not the case, then the linear regression is underfitting the data.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#plot the residuals. there’s obviously a problem with under/over prediction&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span><span class="p">)):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Grouper</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span> <span class="c">#do one regression at a time</span>
</span><span class='line'><span class="n">Yearer</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Year&#39;</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">Grouper</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">Yearer</span><span class="p">]</span>
</span><span class='line'><span class="n">Y</span> <span class="o">=</span> <span class="n">Group1</span><span class="p">[</span><span class="s">&#39;WS/48&#39;</span><span class="p">]</span> <span class="c">#get predictor data</span>
</span><span class='line'><span class="n">resid</span> <span class="o">=</span> <span class="n">estHold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">resid</span> <span class="c">#extract residuals</span>
</span><span class='line'>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c">#plot each regression&#39;s prediction against actual data</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">resid</span><span class="p">,</span><span class="s">&#39;o&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">&#39;axes.color_cycle&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Group </span><span class="si">%d</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.12</span><span class="p">,</span><span class="mf">0.25</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p><img src="/images/regression2NBA/residuals.png" /></p>

<p>Residuals are on the Y-axis and career performances are on the X-axis. Negative residuals are over predictions (the player is worse than my model predicts) and postive residuals are under predictions (the player is better than my model predicts). I don’t test this, but the residuals appear VERY correlated. That is, the model tends to over estimate bad players (players with WS/48 less than 0.0) and under estimate good players. Just to clarify, non-correlated residuals would have no apparent slope.</p>

<p>This means the model is making systematic errors and not fitting the actual shape of the data. I’m not going to say the model is damned, but this is an obvious sign that the model needs more flexibility.</p>

<p>No model is perfect, but this model definitely needs more work. I’ve been playing with more flexible models and will post these models here if they do a better job predicting player performance.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Predicting Career Performance From Rookie Performance]]></title>
    <link href="http://www.danvatterott.com/blog/2016/03/20/predicting-career-performance-from-rookie-performance/"/>
    <updated>2016-03-20T15:56:18-04:00</updated>
    <id>http://www.danvatterott.com/blog/2016/03/20/predicting-career-performance-from-rookie-performance</id>
    <content type="html"><![CDATA[<p>As a huge t-wolves fan, I’ve been curious all year by what we can infer from Karl-Anthony Towns’ great rookie season. To answer this question, I’ve create a simple linear regression model that uses rookie year performance to predict career performance.</p>

<p>Many have attempted to predict NBA players’ success via regression style approaches. Notable models I know of include <a href="http://laynevashro.com/basketball/predsFAQ.html">Layne Vashro’s model</a> which uses combine and college performance to predict career performance. Layne Vashro’s model is a quasi-poisson GLM. I tried a similar approach, but had the most success when using ws/48 and OLS. I will discuss this a little more at the end of the post.</p>

<p>A jupyter notebook of this post can be found on my <a href="https://github.com/dvatterott/jupyter_notebooks/blob/master/nba_rookie_regression.ipynb">github</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#import some libraries and tell ipython we want inline figures rather than interactive figures. </span>
</span><span class='line'><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span><span class="o">,</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span><span class="o">,</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="o">,</span> <span class="nn">matplotlib</span> <span class="kn">as</span> <span class="nn">mpl</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="kn">from</span> <span class="o">&lt;</span><span class="n">strong</span><span class="o">&gt;</span><span class="n">future</span><span class="o">&lt;/</span><span class="n">strong</span><span class="o">&gt;</span> <span class="kn">import</span> <span class="nn">print_function</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span><span class='line'><span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">mpl_style</span> <span class="o">=</span> <span class="err">‘</span><span class="n">default</span><span class="err">’</span> <span class="c">#load matplotlib for plotting</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="err">‘</span><span class="n">ggplot</span><span class="err">’</span><span class="p">)</span> <span class="c">#im addicted to ggplot. so pretty.</span>
</span><span class='line'><span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="err">‘</span><span class="n">font</span><span class="o">.</span><span class="n">family</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="err">‘</span><span class="n">Bitstream</span> <span class="n">Vera</span> <span class="n">Sans</span><span class="err">’</span><span class="p">]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>I collected all the data for this project from basketball-reference.com. I posted the functions for collecting the data on my <a href="https://github.com/dvatterott/nba_project">github</a>. The data is also posted there. Beware, the data collection scripts take awhile to run.</p>

<p>This data includes per 36 stats and advanced statistics such as usage percentage. I simply took all the per 36 and advanced statistics from a player’s page on basketball-reference.com.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="err">‘</span><span class="n">nba_bballref_career_stats_2016_Mar_15</span><span class="o">.</span><span class="n">pkl</span><span class="err">’</span><span class="p">)</span> <span class="c">#here’s the career data. </span>
</span><span class='line'><span class="n">rookie_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="err">‘</span><span class="n">nba_bballref_rookie_stats_2016_Mar_15</span><span class="o">.</span><span class="n">pkl</span><span class="err">’</span><span class="p">)</span> <span class="c">#here’s the rookie year data</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>The variable I am trying to predict is average <a href="http://www.basketball-reference.com/about/ws.html">WS/48</a> over a player’s career. There’s no perfect box-score statistic when it comes to quantifying a player’s peformance, but ws/48 seems relatively solid.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Games</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">G</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">50</span> <span class="c">#only using players who played in more than 50 games.</span>
</span><span class='line'><span class="n">Year</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span> <span class="c">#only using players after 1980 when they started keeping many important records such as games started&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">Games</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">Year</span><span class="p">][</span><span class="err">‘</span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">]</span> <span class="c">#predicted variable&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Bin</span> <span class="n">Count</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/regressionNBA/predictor_hist.png" /></p>

<p>The predicted variable looks pretty gaussian, so I can use ordinary least squares. This will be nice because while ols is not flexible, it’s highly interpretable. At the end of the post I’ll mention some more complex models that I will try.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">rook_games</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="p">[</span><span class="err">‘</span><span class="n">Career</span> <span class="n">Games</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">50</span>
</span><span class='line'><span class="n">rook_year</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="p">[</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span> <span class="nb">id</span><span class="o">=</span><span class="s">&quot;remove-rookies-from-before-1980-and-who-have-played-less-than-50-games-i-also-remove-some-features-that-seem-irrelevant-or-unfair&quot;</span><span class="o">&gt;</span><span class="n">remove</span> <span class="n">rookies</span> <span class="kn">from</span> <span class="nn">before</span> <span class="mi">1980</span> <span class="ow">and</span> <span class="n">who</span> <span class="n">have</span> <span class="n">played</span> <span class="n">less</span> <span class="n">than</span> <span class="mi">50</span> <span class="n">games</span><span class="o">.</span> <span class="n">I</span> <span class="n">also</span> <span class="n">remove</span> <span class="n">some</span> <span class="n">features</span> <span class="n">that</span> <span class="n">seem</span> <span class="n">irrelevant</span> <span class="ow">or</span> <span class="n">unfair</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">rookie_df_games</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="p">[</span><span class="n">rook_games</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">rook_year</span><span class="p">]</span> <span class="c">#only players with more than 50 games. </span>
</span><span class='line'><span class="n">rookie_df_drop</span> <span class="o">=</span> <span class="n">rookie_df_games</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Career</span> <span class="n">Games</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Name</span><span class="err">’</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Above, I remove some predictors from the rookie data. Lets run the regression!</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="kn">as</span> <span class="nn">sm</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">X_rookie</span> <span class="o">=</span> <span class="n">rookie_df_drop</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe</span>
</span><span class='line'><span class="n">X_rookie</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_rookie</span><span class="p">)</span>  <span class="c"># Adds a constant term to the predictor&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">estAll</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">X_rookie</span><span class="p">)</span> <span class="c">#create ordinary least squares model</span>
</span><span class='line'><span class="n">estAll</span> <span class="o">=</span> <span class="n">estAll</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span> <span class="c">#fit the model</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">estAll</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  WS/48   R-squared:                       0.476
Model:                            OLS   Adj. R-squared:                  0.461
Method:                 Least Squares   F-statistic:                     31.72
Date:                Sun, 20 Mar 2016   Prob (F-statistic):          2.56e-194
Time:                        15:29:43   Log-Likelihood:                 3303.9
No. Observations:                1690   AIC:                            -6512.
Df Residuals:                    1642   BIC:                            -6251.
Df Model:                          47                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const          0.2509      0.078      3.223      0.001         0.098     0.404
x1            -0.0031      0.001     -6.114      0.000        -0.004    -0.002
x2            -0.0004   9.06e-05     -4.449      0.000        -0.001    -0.000
x3            -0.0003   8.12e-05     -3.525      0.000        -0.000    -0.000
x4          1.522e-05   4.73e-06      3.218      0.001      5.94e-06  2.45e-05
x5             0.0030      0.031      0.096      0.923        -0.057     0.063
x6             0.0109      0.019      0.585      0.559        -0.026     0.047
x7            -0.0312      0.094     -0.331      0.741        -0.216     0.154
x8             0.0161      0.027      0.594      0.553        -0.037     0.069
x9            -0.0054      0.018     -0.292      0.770        -0.041     0.031
x10            0.0012      0.007      0.169      0.866        -0.013     0.015
x11            0.0136      0.023      0.592      0.554        -0.031     0.059
x12           -0.0099      0.018     -0.538      0.591        -0.046     0.026
x13            0.0076      0.054      0.141      0.888        -0.098     0.113
x14            0.0094      0.012      0.783      0.433        -0.014     0.033
x15            0.0029      0.002      1.361      0.174        -0.001     0.007
x16            0.0078      0.009      0.861      0.390        -0.010     0.026
x17           -0.0107      0.019     -0.573      0.567        -0.047     0.026
x18           -0.0062      0.018     -0.342      0.732        -0.042     0.029
x19            0.0095      0.017      0.552      0.581        -0.024     0.043
x20            0.0111      0.004      2.853      0.004         0.003     0.019
x21            0.0109      0.018      0.617      0.537        -0.024     0.046
x22           -0.0139      0.006     -2.165      0.030        -0.026    -0.001
x23            0.0024      0.005      0.475      0.635        -0.008     0.012
x24            0.0022      0.001      1.644      0.100        -0.000     0.005
x25           -0.0125      0.012     -1.027      0.305        -0.036     0.011
x26           -0.0006      0.000     -1.782      0.075        -0.001  5.74e-05
x27           -0.0011      0.001     -1.749      0.080        -0.002     0.000
x28            0.0012      0.003      0.487      0.626        -0.004     0.006
x29            0.1824      0.089      2.059      0.040         0.009     0.356
x30           -0.0288      0.025     -1.153      0.249        -0.078     0.020
x31           -0.0128      0.011     -1.206      0.228        -0.034     0.008
x32           -0.0046      0.008     -0.603      0.547        -0.020     0.010
x33           -0.0071      0.005     -1.460      0.145        -0.017     0.002
x34            0.0131      0.012      1.124      0.261        -0.010     0.036
x35           -0.0023      0.001     -2.580      0.010        -0.004    -0.001
x36           -0.0077      0.013     -0.605      0.545        -0.033     0.017
x37            0.0069      0.004      1.916      0.055        -0.000     0.014
x38           -0.0015      0.001     -2.568      0.010        -0.003    -0.000
x39           -0.0002      0.002     -0.110      0.912        -0.005     0.004
x40           -0.0109      0.017     -0.632      0.528        -0.045     0.023
x41           -0.0142      0.017     -0.821      0.412        -0.048     0.020
x42            0.0217      0.017      1.257      0.209        -0.012     0.056
x43            0.0123      0.102      0.121      0.904        -0.188     0.213
x44            0.0441      0.018      2.503      0.012         0.010     0.079
x45            0.0406      0.018      2.308      0.021         0.006     0.075
x46           -0.0410      0.018     -2.338      0.020        -0.075    -0.007
x47            0.0035      0.003      1.304      0.192        -0.002     0.009
==============================================================================
Omnibus:                       42.820   Durbin-Watson:                   1.966
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               54.973
Skew:                           0.300   Prob(JB):                     1.16e-12
Kurtosis:                       3.649   Cond. No.                     1.88e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.88e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
</code></pre>

<p>There’s a lot to look at in the regression output (especially with this many features). For an explanation of all the different parts of the regression take a look at this <a href="http://connor-johnson.com/2014/02/18/linear-regression-with-python/">post</a>. Below is a quick plot of predicted ws/48 against actual ws/48.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">estAll</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_rookie</span><span class="p">),</span><span class="n">Y</span><span class="p">,</span><span class="err">’</span><span class="n">o</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.01</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.01</span><span class="p">),</span><span class="err">’</span><span class="n">b</span><span class="o">-</span><span class="err">‘</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Career</span> <span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Predicted</span> <span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/regressionNBA/regression1_predict.png" /></p>

<p>The blue line above is NOT the best-fit line. It’s the identity line. I plot it to help visualize where the model fails. The model seems to primarily fail in the extremes - it tends to overestimate the worst players.</p>

<p>All in all, This model does a remarkably good job given its simplicity (linear regression), but it also leaves a lot of variance unexplained.</p>

<p>One reason this model might miss some variance is there’s more than one way to be a productive basketball player. For instance, Dwight Howard and Steph Curry find very different ways to contribute. One linear regression model is unlikely to succesfully predict both players.</p>

<p>In a <a href="http://www.danvatterott.com/blog/2016/02/21/grouping-nba-players/">previous post</a>, I grouped players according to their on-court performance. These player groupings might help predict career performance.</p>

<p>Below, I will use the same player grouping I developed in my previous post, and examine how these groupings impact my ability to predict career performance.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="err">‘</span><span class="n">nba_bballref_career_stats_2016_Mar_15</span><span class="o">.</span><span class="n">pkl</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">G</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">50</span><span class="p">]</span>
</span><span class='line'><span class="n">df_drop</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Name</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">G</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">GS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">MP</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FG</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FGA</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FG</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">3</span><span class="n">P</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">2</span><span class="n">P</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FT</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TRB</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">PTS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">ORtg</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DRtg</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">PER</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TS</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="mi">3</span><span class="n">PAr</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">FTr</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">ORB</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DRB</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TRB</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">AST</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">STL</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">BLK</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TOV</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">USG</span><span class="o">%</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">OWS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DWS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">WS</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">OBPM</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">DBPM</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">BPM</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">VORP</span><span class="err">’</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">df_drop</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe</span>
</span><span class='line'><span class="n">ScaleModel</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">ScaleModel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">reduced_model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">reduced_data</span> <span class="o">=</span> <span class="n">reduced_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c">#transform data into the 5 PCA components space</span>
</span><span class='line'><span class="n">final_fit</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">reduced_data</span><span class="p">)</span> <span class="c">#fit 6 clusters</span>
</span><span class='line'><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span> <span class="c">#label each data point with its clusters</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>See my other post for more details about this clustering procedure.</p>

<p>Let’s see how WS/48 varies across the groups.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">WS_48</span> <span class="o">=</span> <span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span> <span class="c">#create a vector of ws/48. One for each cluster</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">WS_48</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/regressionNBA/boxwhisk_ws48.png" /></p>

<p>Some groups perform better than others, but there’s lots of overlap between the groups. Importantly, each group has a fair amount of variability. Each group spans at least 0.15 WS/48. This gives the regression enough room to successfully predict performance in each group.</p>

<p>Now, lets get a bit of a refresher on what the groups are. Again, my previous post has a good description of these groups.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">TS</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">TS</span><span class="o">%</span><span class="err">’</span><span class="p">])</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">100</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span> <span class="c">#create vectors of each stat for each cluster</span>
</span><span class='line'><span class="n">ThreeAr</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="mi">3</span><span class="n">PAr</span><span class="err">’</span><span class="p">])</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">100</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span>
</span><span class='line'><span class="n">FTr</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">FTr</span><span class="err">’</span><span class="p">])</span><span class="o">*</span><span class="mi">100</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span>
</span><span class='line'><span class="n">RBD</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">TRB</span><span class="o">%</span><span class="err">’</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span>
</span><span class='line'><span class="n">AST</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">AST</span><span class="o">%</span><span class="err">’</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span>
</span><span class='line'><span class="n">STL</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">STL</span><span class="o">%</span><span class="err">’</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span>
</span><span class='line'><span class="n">TOV</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">TOV</span><span class="o">%</span><span class="err">’</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span>
</span><span class='line'><span class="n">USG</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span><span class="o">==</span><span class="n">x</span><span class="p">][</span><span class="err">‘</span><span class="n">USG</span><span class="o">%</span><span class="err">’</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">])]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">TS</span><span class="p">,</span><span class="n">ThreeAr</span><span class="p">,</span><span class="n">FTr</span><span class="p">,</span><span class="n">RBD</span><span class="p">,</span><span class="n">AST</span><span class="p">,</span><span class="n">STL</span><span class="p">,</span><span class="n">TOV</span><span class="p">,</span><span class="n">USG</span><span class="p">])</span>
</span><span class='line'><span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span><span class="n">Data</span><span class="p">,</span><span class="err">’</span><span class="n">o</span><span class="o">-</span><span class="err">‘</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ind</span><span class="p">,(</span><span class="err">‘</span><span class="bp">True</span> <span class="n">Shooting</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="mi">3</span> <span class="n">point</span> <span class="n">Attempt</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">Free</span> <span class="n">Throw</span> <span class="n">Rate</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">Rebound</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">Assist</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Steal</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">TOV</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Usage</span><span class="err">’</span><span class="p">),</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="err">‘</span><span class="n">Group</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">2</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">3</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">4</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">5</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">6</span><span class="err">’</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Percent</span><span class="err">’</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/regressionNBA/grouping_performance.png" /></p>

<p>I’ve plotted the groups across a number of useful categories. For information about these categories see <a href="http://www.basketball-reference.com/about/glossary.html">basketball reference’s glossary</a>.</p>

<p>Here’s a quick rehash of the groupings. See my <a href="http://www.danvatterott.com/blog/2016/02/21/grouping-nba-players/">previous post</a> for more detail.</p>

<ul>
<li>**Group 1:** These are the distributors who shoot a fair number of threes, don't rebound at all, dish out assists, gather steals, and ...turn the ball over.</li> 
<li>**Group 2:** These are the scorers who get to the free throw line, dish out assists, and carry a high usage.</li> 
<li>**Group 3:** These are the bench players who don't score...or do much in general.</li>
<li>**Group 4:** These are the 3 point shooters who shoot tons of 3 pointers, almost no free throws, and don't rebound well.</li>
<li>**Group 5:** These are the mid-range shooters who shoot well, but don't shoot threes or draw free throws</li>
<li>**Group 6:** These are the defensive big men who shoot no threes, rebound lots, and carry a low usage.</li>
</ul>

<p>On to the regression.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">rookie_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="err">‘</span><span class="n">nba_bballref_rookie_stats_2016_Mar_15</span><span class="o">.</span><span class="n">pkl</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">rookie_df</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Career</span> <span class="n">Games</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Name</span><span class="err">’</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">X</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe</span>
</span><span class='line'><span class="n">ScaleRookie</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c">#scale data</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">ScaleRookie</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c">#transform data to scale&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">reduced_model_rookie</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c">#create pca model of first 10 components. </span>
</span></code></pre></td></tr></table></div></figure></p>

<p>You might have noticed the giant condition number in the regression above. This indicates significant <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a> of the features, which isn’t surprising since I have many features that reflect the same abilities.</p>

<p>The multicollinearity doesn’t prevent the regression model from making accurate predictions, but does it make the beta weight estimates irratic. With irratic beta weights, it’s hard to tell whether the different clusters use different models when predicting career ws/48.</p>

<p>In the following regression, I put the predicting features through a PCA and keep only the first 10 PCA components. Using only the first 10 PCA components keeps the component score below 20, indicating that multicollinearity is not a problem. I then examine whether the different groups exhibit a different patterns of beta weights (whether different models predict success of the different groups).</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span><span class="p">][</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span> <span class="c">#limit labels to players after 1980</span>
</span><span class='line'><span class="n">rookie_df_drop</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_labels</span> <span class="c">#label each data point with its clusters&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">estHold</span> <span class="o">=</span> <span class="p">[[],[],[],[],[],[]]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span><span class="p">)):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Grouper</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span> <span class="c">#do regression one group at a time</span>
</span><span class='line'><span class="n">Yearer</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Year&#39;</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">Grouper</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">Yearer</span><span class="p">]</span>
</span><span class='line'><span class="n">Y</span> <span class="o">=</span> <span class="n">Group1</span><span class="p">[</span><span class="s">&#39;WS/48&#39;</span><span class="p">]</span> <span class="c">#get predicted data</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">rookie_df_drop</span><span class="p">[</span><span class="n">rookie_df_drop</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span><span class="p">]</span> <span class="c">#get predictor data of group</span>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">ScaleRookie</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c">#scale data</span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">reduced_model_rookie</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c">#transform data into the 10 PCA components space</span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># Adds a constant term to the predictor</span>
</span><span class='line'><span class="n">est</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c">#create regression model</span>
</span><span class='line'><span class="n">est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</span><span class='line'><span class="c">#print(est.summary())</span>
</span><span class='line'><span class="n">estHold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">est</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span> <span class="c">#plot the beta weights</span>
</span><span class='line'><span class="n">width</span><span class="o">=</span><span class="mf">0.12</span>
</span><span class='line'><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">est</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">estHold</span><span class="p">):</span>
</span><span class='line'>    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="o">+</span><span class="n">width</span><span class="o">*</span><span class="n">i</span><span class="p">,</span><span class="n">est</span><span class="o">.</span><span class="n">params</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="err">‘</span><span class="n">axes</span><span class="o">.</span><span class="n">color_cycle</span><span class="err">’</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span><span class="n">yerr</span><span class="o">=</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">conf_int</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">est</span><span class="o">.</span><span class="n">conf_int</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">right</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Principle</span> <span class="n">Components</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="err">‘</span><span class="n">Group</span> <span class="mi">1</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">2</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">3</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">4</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">5</span><span class="err">’</span><span class="p">,</span><span class="err">’</span><span class="n">Group</span> <span class="mi">6</span><span class="err">’</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Beta</span> <span class="n">Weights</span><span class="err">’</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/regressionNBA/beta_weights.png" /></p>

<p>Above I plot the beta weights for each principle component across the groupings. This plot is a lot to look at, but I wanted to depict how the beta values changed across the groups. They are not drastically different, but they’re also not identical. Error bars depict 95% confidence intervals.</p>

<p>Below I fit a regression to each group, but with all the features. Again, multicollinearity will be a problem, but this will not decrease the regression’s accuracy, which is all I really care about.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">rookie_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="err">‘</span><span class="n">Year</span><span class="err">’</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span><span class="p">][</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="n">rookie_df_drop</span><span class="p">[</span><span class="err">‘</span><span class="n">kmeans_label</span><span class="err">’</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_labels</span> <span class="c">#label each data point with its clusters&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">estHold</span> <span class="o">=</span> <span class="p">[[],[],[],[],[],[]]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span><span class="p">)):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Grouper</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span> <span class="c">#do one regression at a time</span>
</span><span class='line'><span class="n">Yearer</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Year&#39;</span><span class="p">]</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="mi">1980</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">Grouper</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">Yearer</span><span class="p">]</span>
</span><span class='line'><span class="n">Y</span> <span class="o">=</span> <span class="n">Group1</span><span class="p">[</span><span class="s">&#39;WS/48&#39;</span><span class="p">]</span> <span class="c">#get predictor data</span>
</span><span class='line'>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">rookie_df_drop</span><span class="p">[</span><span class="n">rookie_df_drop</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">group</span><span class="p">]</span>
</span><span class='line'><span class="n">Group1_rookie</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span> <span class="c">#get predicted data</span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">Group1_rookie</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe    </span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># Adds a constant term to the predictor</span>
</span><span class='line'><span class="n">est</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c">#fit with linear regression model</span>
</span><span class='line'><span class="n">est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</span><span class='line'><span class="n">estHold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">est</span>
</span><span class='line'><span class="c">#print est.summary()</span>
</span><span class='line'>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c">#plot each regression&#39;s prediction against actual data</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">Y</span><span class="p">,</span><span class="s">&#39;o&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">&#39;axes.color_cycle&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.01</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.01</span><span class="p">),</span><span class="s">&#39;-&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Group </span><span class="si">%d</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span><span class="s">&#39;$r^2$=</span><span class="si">%.2f</span><span class="s">&#39;</span><span class="o">%</span><span class="n">est</span><span class="o">.</span><span class="n">rsquared</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.12</span><span class="p">,</span><span class="mf">0.25</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.12</span><span class="p">,</span><span class="mf">0.25</span><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p><img src="/images/regressionNBA/model2_predictions.png" /></p>

<p>The plots above depict each regression’s predictions against actual ws/48. I provide each model’s r^2 in the plot too.</p>

<p>Some regressions are better than others. For instance, the regression model does a pretty awesome job predicting the bench warmers…I wonder if this is because they have shorter careers… The regression model does not do a good job predicting the 3-point shooters.</p>

<p>Now onto the fun stuff though.</p>

<p>Below, create a function for predicting a players career WS/48. First, I write a function that finds what cluster a player would belong to, and what the regression model predicts for this players career (with 95% confidence intervals).</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">player_prediction__regressionModel</span><span class="p">(</span><span class="n">PlayerName</span><span class="p">):</span>
</span><span class='line'>    <span class="kn">from</span> <span class="nn">statsmodels.sandbox.regression.predstd</span> <span class="kn">import</span> <span class="n">wls_prediction_std</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">clust_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s">&#39;nba_bballref_career_stats_2016_Mar_05.pkl&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">clust_df</span> <span class="o">=</span> <span class="n">clust_df</span><span class="p">[</span><span class="n">clust_df</span><span class="p">[</span><span class="s">&#39;Name&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">PlayerName</span><span class="p">]</span>
</span><span class='line'><span class="n">clust_df</span> <span class="o">=</span> <span class="n">clust_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;Name&#39;</span><span class="p">,</span><span class="s">&#39;G&#39;</span><span class="p">,</span><span class="s">&#39;GS&#39;</span><span class="p">,</span><span class="s">&#39;MP&#39;</span><span class="p">,</span><span class="s">&#39;FG&#39;</span><span class="p">,</span><span class="s">&#39;FGA&#39;</span><span class="p">,</span><span class="s">&#39;FG%&#39;</span><span class="p">,</span><span class="s">&#39;3P&#39;</span><span class="p">,</span><span class="s">&#39;2P&#39;</span><span class="p">,</span><span class="s">&#39;FT&#39;</span><span class="p">,</span><span class="s">&#39;TRB&#39;</span><span class="p">,</span><span class="s">&#39;PTS&#39;</span><span class="p">,</span><span class="s">&#39;ORtg&#39;</span><span class="p">,</span><span class="s">&#39;DRtg&#39;</span><span class="p">,</span><span class="s">&#39;PER&#39;</span><span class="p">,</span><span class="s">&#39;TS%&#39;</span><span class="p">,</span><span class="s">&#39;3PAr&#39;</span><span class="p">,</span><span class="s">&#39;FTr&#39;</span><span class="p">,</span><span class="s">&#39;ORB%&#39;</span><span class="p">,</span><span class="s">&#39;DRB%&#39;</span><span class="p">,</span><span class="s">&#39;TRB%&#39;</span><span class="p">,</span><span class="s">&#39;AST%&#39;</span><span class="p">,</span><span class="s">&#39;STL%&#39;</span><span class="p">,</span><span class="s">&#39;BLK%&#39;</span><span class="p">,</span><span class="s">&#39;TOV%&#39;</span><span class="p">,</span><span class="s">&#39;USG%&#39;</span><span class="p">,</span><span class="s">&#39;OWS&#39;</span><span class="p">,</span><span class="s">&#39;DWS&#39;</span><span class="p">,</span><span class="s">&#39;WS&#39;</span><span class="p">,</span><span class="s">&#39;WS/48&#39;</span><span class="p">,</span><span class="s">&#39;OBPM&#39;</span><span class="p">,</span><span class="s">&#39;DBPM&#39;</span><span class="p">,</span><span class="s">&#39;BPM&#39;</span><span class="p">,</span><span class="s">&#39;VORP&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="n">new_vect</span> <span class="o">=</span> <span class="n">ScaleModel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">clust_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</span><span class='line'><span class="n">reduced_data</span> <span class="o">=</span> <span class="n">reduced_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_vect</span><span class="p">)</span>
</span><span class='line'><span class="n">Group</span> <span class="o">=</span> <span class="n">final_fit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">reduced_data</span><span class="p">)</span>
</span><span class='line'><span class="n">clust_df</span><span class="p">[</span><span class="s">&#39;kmeans_label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="n">Predrookie_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s">&#39;nba_bballref_rookie_stats_2016_Mar_15.pkl&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">Predrookie_df</span> <span class="o">=</span> <span class="n">Predrookie_df</span><span class="p">[</span><span class="n">Predrookie_df</span><span class="p">[</span><span class="s">&#39;Name&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">PlayerName</span><span class="p">]</span>
</span><span class='line'><span class="n">Predrookie_df</span> <span class="o">=</span> <span class="n">Predrookie_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;Year&#39;</span><span class="p">,</span><span class="s">&#39;Career Games&#39;</span><span class="p">,</span><span class="s">&#39;Name&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="n">predX</span> <span class="o">=</span> <span class="n">Predrookie_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span> <span class="c">#take data out of dataframe</span>
</span><span class='line'><span class="n">predX</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">predX</span><span class="p">,</span><span class="n">has_constant</span><span class="o">=</span><span class="s">&#39;add&#39;</span><span class="p">)</span>  <span class="c"># Adds a constant term to the predictor</span>
</span><span class='line'><span class="n">prstd_ols</span><span class="p">,</span> <span class="n">iv_l_ols</span><span class="p">,</span> <span class="n">iv_u_ols</span> <span class="o">=</span> <span class="n">wls_prediction_std</span><span class="p">(</span><span class="n">estHold</span><span class="p">[</span><span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">predX</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</span><span class='line'><span class="k">return</span> <span class="p">{</span><span class="s">&#39;Name&#39;</span><span class="p">:</span><span class="n">PlayerName</span><span class="p">,</span><span class="s">&#39;Group&#39;</span><span class="p">:</span><span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="s">&#39;Prediction&#39;</span><span class="p">:</span><span class="n">estHold</span><span class="p">[</span><span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predX</span><span class="p">),</span><span class="s">&#39;Upper_CI&#39;</span><span class="p">:</span><span class="n">iv_u_ols</span><span class="p">,</span><span class="s">&#39;Lower_CI&#39;</span><span class="p">:</span><span class="n">iv_l_ols</span><span class="p">}</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Here I create a function that creates a list of all the first round draft picks from a given year.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">gather_draftData</span><span class="p">(</span><span class="n">Year</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="kn">import</span> <span class="nn">urllib2</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'>
</span><span class='line'><span class="n">draft_len</span> <span class="o">=</span> <span class="mi">30</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">convert_float</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
</span><span class='line'>    <span class="k">try</span><span class="p">:</span>
</span><span class='line'>        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
</span><span class='line'>    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
</span><span class='line'>
</span><span class='line'><span class="n">url</span> <span class="o">=</span> <span class="s">&#39;http://www.basketball-reference.com/draft/NBA_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">Year</span><span class="p">)</span><span class="o">+</span><span class="s">&#39;.html&#39;</span>
</span><span class='line'><span class="n">html</span> <span class="o">=</span> <span class="n">urllib2</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</span><span class='line'><span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">html</span><span class="p">,</span><span class="s">&quot;lxml&quot;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">draft_num</span> <span class="o">=</span> <span class="p">[</span><span class="n">soup</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">&#39;tbody&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">&#39;tr&#39;</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">&#39;td&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">draft_len</span><span class="p">)]</span>
</span><span class='line'><span class="n">draft_nam</span> <span class="o">=</span> <span class="p">[</span><span class="n">soup</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">&#39;tbody&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">&#39;tr&#39;</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">&#39;td&#39;</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">draft_len</span><span class="p">)]</span>
</span><span class='line'>
</span><span class='line'><span class="n">draft_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">draft_num</span><span class="p">,</span><span class="n">draft_nam</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</span><span class='line'><span class="n">draft_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Number&#39;</span><span class="p">,</span><span class="s">&#39;Name&#39;</span><span class="p">]</span>
</span><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</span><span class='line'><span class="k">return</span> <span class="n">draft_df</span>
</span></code></pre></td></tr></table></div></figure>
</code></pre>

<p>Below I create predictions for each first-round draft pick from 2015. The spurs’ first round pick, Nikola Milutinov, has yet to play so I do not create a prediction for him.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="kn">as</span> <span class="nn">mpatches</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">draft_df</span> <span class="o">=</span> <span class="n">gather_draftData</span><span class="p">(</span><span class="mi">2015</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">draft_df</span><span class="p">[</span><span class="err">‘</span><span class="n">Name</span><span class="err">’</span><span class="p">][</span><span class="mi">14</span><span class="p">]</span> <span class="o">=</span>  <span class="err">‘</span><span class="n">Kelly</span> <span class="n">Oubre</span> <span class="n">Jr</span><span class="o">.</span><span class="err">’</span> <span class="c">#annoying name inconsistencies&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">31</span><span class="p">),</span><span class="n">draft_df</span><span class="p">[</span><span class="err">‘</span><span class="n">Name</span><span class="err">’</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">draft_df</span> <span class="o">=</span> <span class="n">draft_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c">#Sam Dekker has received little playing time making his prediction highly erratic</span>
</span><span class='line'><span class="n">draft_df</span> <span class="o">=</span> <span class="n">draft_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c">#spurs’ 1st round pick has not played yet&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">draft_df</span><span class="p">[</span><span class="err">‘</span><span class="n">Name</span><span class="err">’</span><span class="p">]:</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">draft_num</span> <span class="o">=</span> <span class="n">draft_df</span><span class="p">[</span><span class="n">draft_df</span><span class="p">[</span><span class="s">&#39;Name&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">name</span><span class="p">][</span><span class="s">&#39;Number&#39;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="n">predict_dict</span> <span class="o">=</span> <span class="n">player_prediction__regressionModel</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</span><span class='line'><span class="n">yerr</span> <span class="o">=</span> <span class="p">(</span><span class="n">predict_dict</span><span class="p">[</span><span class="s">&#39;Upper_CI&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">predict_dict</span><span class="p">[</span><span class="s">&#39;Lower_CI&#39;</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
</span><span class='line'>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">draft_num</span><span class="p">,</span><span class="n">predict_dict</span><span class="p">[</span><span class="s">&#39;Prediction&#39;</span><span class="p">],</span><span class="n">fmt</span><span class="o">=</span><span class="s">&#39;o&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
</span><span class='line'>            <span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">&#39;axes.color_cycle&#39;</span><span class="p">][</span><span class="n">predict_dict</span><span class="p">[</span><span class="s">&#39;Group&#39;</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">yerr</span><span class="o">=</span><span class="n">yerr</span><span class="p">);</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">right</span><span class="o">=</span><span class="mi">31</span><span class="p">)</span>
</span><span class='line'><span class="n">patch</span> <span class="o">=</span> <span class="p">[</span><span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="err">‘</span><span class="n">axes</span><span class="o">.</span><span class="n">color_cycle</span><span class="err">’</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="err">’</span><span class="n">Group</span> <span class="o">%</span><span class="n">d</span><span class="err">’</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">patch</span><span class="p">,</span><span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Predicted</span> <span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="err">’</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="err">‘</span><span class="n">Draft</span> <span class="n">Position</span><span class="err">’</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><img src="/images/regressionNBA/draft_2015_predict.png" /></p>

<p>The plot above is ordered by draft pick. The error bars depict 95% confidence interbals…which are a little wider than I would like. It’s interesting to look at what clusters these players fit into. Lots of 3-pt shooters! It could be that rookies play a limited role in the offense - just shooting 3s.</p>

<p>As a t-wolves fan, I am relatively happy about the high prediction for Karl-Anthony Towns. His predicted ws/48 is between Marc Gasol and Elton Brand. Again, the CIs are quite wide, so the model says there’s a 95% chance he is somewhere between Lebron James ever and a player that averages less than 0.1 ws/48.</p>

<p>Karl-Anthony Towns would have the highest predicted ws/48 if it were not for Kevin Looney who the model loves. Kevin Looney has not seen much playing time though, which likely makes his prediction more erratic. Keep in mind I did not use draft position as a predictor in the model.</p>

<p>Sam Dekker has a pretty huge error bar, likely because of his limited playing time this year.</p>

<p>While I fed a ton of features into this model, it’s still just a linear regression. The simplicity of the model might prevent me from making more accurate predictions.</p>

<p>I’ve already started playing with some more complex models. If those work out well, I will post them here. I ended up sticking with a plain linear regression because my vast number of features is a little unwieldy in a more complex models. If you’re interested (and the models produce better results) check back in the future.</p>

<p>For now, these models explain between 40 and 70% of the variance in career ws/48 from only a player’s rookie year. Even predicting 30% of variance is pretty remarkable, so I don’t want to trash on this part of the model. Explaining 65% of the variance is pretty awesome. The model gives us a pretty accurate idea of how these “bench players” will perform. For instance, the future does not look bright for players like Emmanuel Mudiay and Tyus Jones. Not to say these players are doomed. The model assumes that players will retain their grouping for the entire career. Emmanuel Mudiay and Tyus Jones might start performing more like distributors as their career progresses. This could result in a better career.</p>

<p>One nice part about this model is it tells us where the predictions are less confident. For instance, it is nice to know that we’re relatively confident when predicting bench players, but not when we’re predicting 3-point shooters.</p>

<p>For those curious, I output each groups regression summary below.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="p">[</span><span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">estHold</span><span class="p">];</span>
</span></code></pre></td></tr></table></div></figure></p>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  WS/48   R-squared:                       0.648
Model:                            OLS   Adj. R-squared:                  0.575
Method:                 Least Squares   F-statistic:                     8.939
Date:                Sun, 20 Mar 2016   Prob (F-statistic):           2.33e-24
Time:                        10:40:28   Log-Likelihood:                 493.16
No. Observations:                 212   AIC:                            -912.3
Df Residuals:                     175   BIC:                            -788.1
Df Model:                          36                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const         -0.1072      0.064     -1.682      0.094        -0.233     0.019
x1             0.0012      0.001      0.925      0.356        -0.001     0.004
x2            -0.0005      0.000     -2.355      0.020        -0.001 -7.53e-05
x3            -0.0005      0.000     -1.899      0.059        -0.001  2.03e-05
x4          3.753e-05   1.27e-05      2.959      0.004      1.25e-05  6.26e-05
x5            -0.1152      0.088     -1.315      0.190        -0.288     0.058
x6             0.0240      0.053      0.456      0.649        -0.080     0.128
x7            -0.4318      0.372     -1.159      0.248        -1.167     0.303
x8             0.0089      0.085      0.105      0.917        -0.159     0.177
x9            -0.0479      0.054     -0.893      0.373        -0.154     0.058
x10           -0.0055      0.021     -0.265      0.792        -0.046     0.035
x11           -0.0011      0.076     -0.015      0.988        -0.152     0.149
x12           -0.0301      0.053     -0.569      0.570        -0.134     0.074
x13            0.7814      0.270      2.895      0.004         0.249     1.314
x14           -0.0323      0.028     -1.159      0.248        -0.087     0.023
x15           -0.0108      0.007     -1.451      0.149        -0.025     0.004
x16           -0.0202      0.030     -0.676      0.500        -0.079     0.039
x17           -0.0461      0.039     -1.172      0.243        -0.124     0.032
x18           -0.0178      0.040     -0.443      0.659        -0.097     0.062
x19            0.0450      0.038      1.178      0.240        -0.030     0.121
x20            0.0354      0.014      2.527      0.012         0.008     0.063
x21           -0.0418      0.044     -0.947      0.345        -0.129     0.045
x22           -0.0224      0.015     -1.448      0.150        -0.053     0.008
x23           -0.0158      0.008     -2.039      0.043        -0.031    -0.001
x24            0.0058      0.001      4.261      0.000         0.003     0.009
x25            0.0577      0.027      2.112      0.036         0.004     0.112
x26           -0.1913      0.267     -0.718      0.474        -0.717     0.335
x27           -0.0050      0.093     -0.054      0.957        -0.189     0.179
x28           -0.0133      0.039     -0.344      0.731        -0.090     0.063
x29           -0.0071      0.015     -0.480      0.632        -0.036     0.022
x30           -0.0190      0.010     -1.973      0.050        -0.038  5.68e-06
x31            0.0221      0.023      0.951      0.343        -0.024     0.068
x32           -0.0083      0.003     -2.490      0.014        -0.015    -0.002
x33            0.0386      0.031      1.259      0.210        -0.022     0.099
x34            0.0153      0.008      1.819      0.071        -0.001     0.032
x35        -1.734e-05      0.001     -0.014      0.989        -0.002     0.002
x36            0.0033      0.004      0.895      0.372        -0.004     0.011
==============================================================================
Omnibus:                        2.457   Durbin-Watson:                   2.144
Prob(Omnibus):                  0.293   Jarque-Bera (JB):                2.475
Skew:                           0.007   Prob(JB):                        0.290
Kurtosis:                       3.529   Cond. No.                     1.78e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.78e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  WS/48   R-squared:                       0.443
Model:                            OLS   Adj. R-squared:                  0.340
Method:                 Least Squares   F-statistic:                     4.307
Date:                Sun, 20 Mar 2016   Prob (F-statistic):           1.67e-11
Time:                        10:40:28   Log-Likelihood:                 447.99
No. Observations:                 232   AIC:                            -822.0
Df Residuals:                     195   BIC:                            -694.4
Df Model:                          36                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const         -0.0532      0.090     -0.594      0.553        -0.230     0.124
x1            -0.0020      0.002     -1.186      0.237        -0.005     0.001
x2            -0.0006      0.000     -1.957      0.052        -0.001  4.47e-06
x3            -0.0007      0.000     -2.559      0.011        -0.001    -0.000
x4          5.589e-05   1.39e-05      4.012      0.000      2.84e-05  8.34e-05
x5             0.0386      0.093      0.414      0.679        -0.145     0.222
x6            -0.0721      0.051     -1.407      0.161        -0.173     0.029
x7            -0.6259      0.571     -1.097      0.274        -1.751     0.499
x8            -0.0653      0.079     -0.822      0.412        -0.222     0.091
x9             0.0756      0.051      1.485      0.139        -0.025     0.176
x10           -0.0046      0.031     -0.149      0.881        -0.066     0.057
x11           -0.0365      0.066     -0.554      0.580        -0.166     0.093
x12            0.0679      0.051      1.332      0.185        -0.033     0.169
x13            0.0319      0.183      0.174      0.862        -0.329     0.393
x14            0.0106      0.040      0.262      0.793        -0.069     0.090
x15           -0.0232      0.017     -1.357      0.176        -0.057     0.011
x16           -0.1121      0.039     -2.869      0.005        -0.189    -0.035
x17           -0.0675      0.060     -1.134      0.258        -0.185     0.050
x18           -0.0314      0.059     -0.536      0.593        -0.147     0.084
x19            0.0266      0.055      0.487      0.627        -0.081     0.134
x20            0.0259      0.009      2.827      0.005         0.008     0.044
x21           -0.0155      0.050     -0.307      0.759        -0.115     0.084
x22            0.1170      0.051      2.281      0.024         0.016     0.218
x23           -0.0157      0.014     -1.102      0.272        -0.044     0.012
x24            0.0021      0.003      0.732      0.465        -0.003     0.008
x25           -0.0012      0.038     -0.032      0.974        -0.077     0.075
x26            0.8379      0.524      1.599      0.111        -0.196     1.871
x27           -0.0511      0.113     -0.454      0.651        -0.273     0.171
x28            0.0944      0.111      0.852      0.395        -0.124     0.313
x29           -0.0018      0.029     -0.061      0.951        -0.059     0.055
x30           -0.0167      0.017     -0.969      0.334        -0.051     0.017
x31            0.0377      0.044      0.854      0.394        -0.049     0.125
x32           -0.0052      0.002     -2.281      0.024        -0.010    -0.001
x33            0.0132      0.037      0.360      0.719        -0.059     0.086
x34           -0.0650      0.028     -2.356      0.019        -0.119    -0.011
x35           -0.0012      0.002     -0.668      0.505        -0.005     0.002
x36            0.0087      0.008      1.107      0.270        -0.007     0.024
==============================================================================
Omnibus:                        2.161   Durbin-Watson:                   2.000
Prob(Omnibus):                  0.339   Jarque-Bera (JB):                1.942
Skew:                           0.222   Prob(JB):                        0.379
Kurtosis:                       3.067   Cond. No.                     3.94e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.94e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  WS/48   R-squared:                       0.358
Model:                            OLS   Adj. R-squared:                  0.270
Method:                 Least Squares   F-statistic:                     4.050
Date:                Sun, 20 Mar 2016   Prob (F-statistic):           1.93e-11
Time:                        10:40:28   Log-Likelihood:                 645.12
No. Observations:                 298   AIC:                            -1216.
Df Residuals:                     261   BIC:                            -1079.
Df Model:                          36                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const          0.0306      0.040      0.763      0.446        -0.048     0.110
x1            -0.0013      0.001     -1.278      0.202        -0.003     0.001
x2            -0.0003      0.000     -1.889      0.060        -0.001  1.39e-05
x3            -0.0002      0.000     -1.196      0.233        -0.001     0.000
x4          2.388e-05   8.83e-06      2.705      0.007       6.5e-06  4.13e-05
x5            -0.0643      0.089     -0.724      0.470        -0.239     0.111
x6             0.0131      0.046      0.286      0.775        -0.077     0.103
x7            -0.4703      0.455     -1.034      0.302        -1.366     0.426
x8             0.0194      0.089      0.219      0.827        -0.155     0.194
x9            -0.0330      0.052     -0.638      0.524        -0.135     0.069
x10           -0.0221      0.013     -1.754      0.081        -0.047     0.003
x11            0.0161      0.074      0.216      0.829        -0.130     0.162
x12           -0.0228      0.047     -0.489      0.625        -0.115     0.069
x13            0.2619      0.423      0.620      0.536        -0.570     1.094
x14           -0.0303      0.027     -1.136      0.257        -0.083     0.022
x15           -0.0023      0.003     -0.895      0.372        -0.007     0.003
x16            0.0005      0.023      0.021      0.983        -0.045     0.046
x17            0.0206      0.040      0.513      0.608        -0.059     0.100
x18            0.0507      0.040      1.271      0.205        -0.028     0.129
x19           -0.0349      0.037     -0.942      0.347        -0.108     0.038
x20            0.0210      0.017      1.252      0.212        -0.012     0.054
x21            0.0400      0.041      0.964      0.336        -0.042     0.122
x22           -0.0239      0.009     -2.530      0.012        -0.042    -0.005
x23           -0.0140      0.008     -1.683      0.094        -0.030     0.002
x24            0.0045      0.001      4.594      0.000         0.003     0.006
x25            0.0264      0.026      1.004      0.316        -0.025     0.078
x26            0.2730      0.169      1.615      0.107        -0.060     0.606
x27           -0.0208      0.187     -0.111      0.912        -0.389     0.348
x28           -0.0007      0.015     -0.051      0.959        -0.029     0.028
x29            0.0168      0.018      0.917      0.360        -0.019     0.053
x30            0.0059      0.011      0.524      0.601        -0.016     0.028
x31           -0.0196      0.028     -0.711      0.478        -0.074     0.035
x32           -0.0035      0.004     -0.899      0.370        -0.011     0.004
x33           -0.0246      0.029     -0.858      0.392        -0.081     0.032
x34            0.0145      0.005      2.903      0.004         0.005     0.024
x35           -0.0017      0.001     -1.442      0.150        -0.004     0.001
x36            0.0069      0.005      1.514      0.131        -0.002     0.016
==============================================================================
Omnibus:                        5.509   Durbin-Watson:                   1.845
Prob(Omnibus):                  0.064   Jarque-Bera (JB):                5.309
Skew:                           0.272   Prob(JB):                       0.0703
Kurtosis:                       3.362   Cond. No.                     3.70e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.7e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  WS/48   R-squared:                       0.304
Model:                            OLS   Adj. R-squared:                  0.248
Method:                 Least Squares   F-statistic:                     5.452
Date:                Sun, 20 Mar 2016   Prob (F-statistic):           4.41e-19
Time:                        10:40:28   Log-Likelihood:                 1030.4
No. Observations:                 486   AIC:                            -1987.
Df Residuals:                     449   BIC:                            -1832.
Df Model:                          36                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const          0.1082      0.033      3.280      0.001         0.043     0.173
x1            -0.0018      0.001     -2.317      0.021        -0.003    -0.000
x2            -0.0005      0.000     -3.541      0.000        -0.001    -0.000
x3          4.431e-05      0.000      0.359      0.720        -0.000     0.000
x4           1.71e-05   6.08e-06      2.813      0.005      5.15e-06   2.9e-05
x5             0.0257      0.044      0.580      0.562        -0.061     0.113
x6             0.0133      0.029      0.464      0.643        -0.043     0.070
x7            -0.5271      0.357     -1.476      0.141        -1.229     0.175
x8             0.0415      0.038      1.090      0.277        -0.033     0.116
x9            -0.0117      0.029     -0.409      0.682        -0.068     0.044
x10            0.0031      0.018      0.171      0.865        -0.032     0.038
x11            0.0253      0.031      0.819      0.413        -0.035     0.086
x12           -0.0196      0.028     -0.687      0.492        -0.076     0.036
x13            0.0360      0.067      0.535      0.593        -0.096     0.168
x14            0.0096      0.021      0.461      0.645        -0.031     0.050
x15            0.0101      0.009      1.165      0.245        -0.007     0.027
x16            0.0227      0.015      1.556      0.120        -0.006     0.051
x17            0.0413      0.034      1.198      0.232        -0.026     0.109
x18            0.0195      0.031      0.623      0.533        -0.042     0.081
x19           -0.0267      0.029     -0.906      0.366        -0.085     0.031
x20            0.0199      0.008      2.652      0.008         0.005     0.035
x21           -0.0442      0.033     -1.325      0.186        -0.110     0.021
x22            0.0232      0.025      0.946      0.345        -0.025     0.072
x23            0.0085      0.009      0.976      0.330        -0.009     0.026
x24            0.0025      0.001      1.782      0.075        -0.000     0.005
x25           -0.0200      0.019     -1.042      0.298        -0.058     0.018
x26            0.4937      0.331      1.491      0.137        -0.157     1.144
x27           -0.1406      0.074     -1.907      0.057        -0.286     0.004
x28           -0.0638      0.049     -1.304      0.193        -0.160     0.032
x29           -0.0252      0.015     -1.690      0.092        -0.055     0.004
x30           -0.0217      0.008     -2.668      0.008        -0.038    -0.006
x31            0.0483      0.020      2.387      0.017         0.009     0.088
x32           -0.0036      0.002     -2.159      0.031        -0.007    -0.000
x33            0.0388      0.023      1.681      0.094        -0.007     0.084
x34           -0.0105      0.011     -0.923      0.357        -0.033     0.012
x35           -0.0028      0.001     -1.966      0.050        -0.006 -1.59e-06
x36           -0.0017      0.003     -0.513      0.608        -0.008     0.005
==============================================================================
Omnibus:                        5.317   Durbin-Watson:                   2.030
Prob(Omnibus):                  0.070   Jarque-Bera (JB):                5.115
Skew:                           0.226   Prob(JB):                       0.0775
Kurtosis:                       3.221   Cond. No.                     4.51e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 4.51e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  WS/48   R-squared:                       0.455
Model:                            OLS   Adj. R-squared:                  0.378
Method:                 Least Squares   F-statistic:                     5.852
Date:                Sun, 20 Mar 2016   Prob (F-statistic):           4.77e-18
Time:                        10:40:28   Log-Likelihood:                 631.81
No. Observations:                 289   AIC:                            -1190.
Df Residuals:                     252   BIC:                            -1054.
Df Model:                          36                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const          0.1755      0.096      1.827      0.069        -0.014     0.365
x1            -0.0031      0.001     -2.357      0.019        -0.006    -0.001
x2            -0.0005      0.000     -2.424      0.016        -0.001 -8.68e-05
x3            -0.0003      0.000     -2.154      0.032        -0.001  -2.9e-05
x4          2.374e-05   8.35e-06      2.842      0.005      7.29e-06  4.02e-05
x5             0.0391      0.070      0.556      0.579        -0.099     0.177
x6             0.0672      0.040      1.662      0.098        -0.012     0.147
x7             0.9503      0.458      2.075      0.039         0.048     1.852
x8            -0.0013      0.061     -0.021      0.983        -0.122     0.119
x9            -0.0270      0.041     -0.659      0.510        -0.108     0.054
x10           -0.0072      0.017     -0.426      0.671        -0.041     0.026
x11            0.0604      0.056      1.083      0.280        -0.049     0.170
x12           -0.0723      0.041     -1.782      0.076        -0.152     0.008
x13           -1.2499      0.392     -3.186      0.002        -2.022    -0.477
x14            0.0502      0.028      1.776      0.077        -0.005     0.106
x15            0.0048      0.011      0.456      0.649        -0.016     0.026
x16           -0.0637      0.042     -1.530      0.127        -0.146     0.018
x17            0.0042      0.038      0.112      0.911        -0.070     0.078
x18            0.0318      0.038      0.830      0.408        -0.044     0.107
x19           -0.0220      0.037     -0.602      0.548        -0.094     0.050
x20        -4.535e-05      0.009     -0.005      0.996        -0.018     0.018
x21           -0.0176      0.040     -0.440      0.660        -0.097     0.061
x22           -0.0244      0.021     -1.182      0.238        -0.065     0.016
x23            0.0135      0.012      1.128      0.260        -0.010     0.037
x24            0.0024      0.002      1.355      0.177        -0.001     0.006
x25           -0.0418      0.026     -1.583      0.115        -0.094     0.010
x26            0.3619      0.328      1.105      0.270        -0.283     1.007
x27            0.0090      0.186      0.049      0.961        -0.358     0.376
x28           -0.0613      0.057     -1.068      0.286        -0.174     0.052
x29            0.0124      0.016      0.779      0.436        -0.019     0.044
x30            0.0042      0.011      0.379      0.705        -0.018     0.026
x31           -0.0108      0.026     -0.412      0.681        -0.062     0.041
x32            0.0014      0.002      0.588      0.557        -0.003     0.006
x33            0.0195      0.029      0.672      0.502        -0.038     0.077
x34            0.0168      0.011      1.554      0.121        -0.004     0.038
x35           -0.0026      0.002     -1.227      0.221        -0.007     0.002
x36           -0.0072      0.004     -1.958      0.051        -0.014  4.02e-05
==============================================================================
Omnibus:                        4.277   Durbin-Watson:                   1.995
Prob(Omnibus):                  0.118   Jarque-Bera (JB):                4.056
Skew:                           0.226   Prob(JB):                        0.132
Kurtosis:                       3.364   Cond. No.                     4.24e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 4.24e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  WS/48   R-squared:                       0.476
Model:                            OLS   Adj. R-squared:                  0.337
Method:                 Least Squares   F-statistic:                     3.431
Date:                Sun, 20 Mar 2016   Prob (F-statistic):           1.19e-07
Time:                        10:40:28   Log-Likelihood:                 330.36
No. Observations:                 173   AIC:                            -586.7
Df Residuals:                     136   BIC:                            -470.1
Df Model:                          36                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const          0.1822      0.262      0.696      0.488        -0.335     0.700
x1            -0.0011      0.002     -0.491      0.624        -0.005     0.003
x2             0.0001      0.000      0.310      0.757        -0.001     0.001
x3          6.743e-05      0.000      0.220      0.827        -0.001     0.001
x4          5.819e-06   1.63e-05      0.357      0.722     -2.65e-05  3.81e-05
x5             0.0618      0.122      0.507      0.613        -0.179     0.303
x6             0.0937      0.074      1.272      0.206        -0.052     0.240
x7             0.8422      0.919      0.917      0.361        -0.975     2.659
x8            -0.1109      0.111     -1.001      0.319        -0.330     0.108
x9            -0.1334      0.075     -1.767      0.079        -0.283     0.016
x10           -0.0357      0.024     -1.500      0.136        -0.083     0.011
x11           -0.1373      0.103     -1.335      0.184        -0.341     0.066
x12           -0.1002      0.075     -1.329      0.186        -0.249     0.049
x13           -0.2963      0.616     -0.481      0.631        -1.515     0.922
x14           -0.0278      0.047     -0.588      0.557        -0.121     0.066
x15           -0.0099      0.015     -0.661      0.510        -0.040     0.020
x16            0.1532      0.106      1.444      0.151        -0.057     0.363
x17           -0.1569      0.072     -2.168      0.032        -0.300    -0.014
x18           -0.1633      0.068     -2.385      0.018        -0.299    -0.028
x19            0.1550      0.066      2.356      0.020         0.025     0.285
x20           -0.0114      0.017     -0.688      0.492        -0.044     0.021
x21           -0.0130      0.076     -0.170      0.865        -0.164     0.138
x22           -0.0202      0.024     -0.857      0.393        -0.067     0.026
x23           -0.0203      0.028     -0.737      0.462        -0.075     0.034
x24           -0.0023      0.004     -0.608      0.544        -0.010     0.005
x25            0.0546      0.048      1.141      0.256        -0.040     0.149
x26           -1.0180      0.714     -1.426      0.156        -2.430     0.394
x27            0.3371      0.203      1.664      0.098        -0.064     0.738
x28            0.1286      0.140      0.916      0.361        -0.149     0.406
x29           -0.0561      0.035     -1.607      0.110        -0.125     0.013
x30           -0.0535      0.020     -2.645      0.009        -0.093    -0.013
x31            0.1169      0.051      2.305      0.023         0.017     0.217
x32            0.0039      0.004      1.030      0.305        -0.004     0.011
x33            0.0179      0.055      0.324      0.746        -0.091     0.127
x34            0.0081      0.013      0.632      0.529        -0.017     0.033
x35            0.0013      0.006      0.229      0.819        -0.010     0.013
x36           -0.0068      0.007     -1.045      0.298        -0.020     0.006
==============================================================================
Omnibus:                        2.969   Durbin-Watson:                   2.098
Prob(Omnibus):                  0.227   Jarque-Bera (JB):                2.526
Skew:                           0.236   Prob(JB):                        0.283
Kurtosis:                       3.357   Cond. No.                     6.96e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 6.96e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
</code></pre>

]]></content>
  </entry>
  
</feed>
