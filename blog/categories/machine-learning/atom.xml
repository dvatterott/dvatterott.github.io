<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine-learning | Dan Vatterott]]></title>
  <link href="https://danvatterott.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="https://danvatterott.com/"/>
  <updated>2020-01-17T21:13:21-06:00</updated>
  <id>https://danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introducing Predeval]]></title>
    <link href="https://danvatterott.com/blog/2019/01/29/introducing-predeval/"/>
    <updated>2019-01-29T20:27:57-06:00</updated>
    <id>https://danvatterott.com/blog/2019/01/29/introducing-predeval</id>
    <content type="html"><![CDATA[<p><a href="https://predeval.readthedocs.io/en/latest/">Predeval</a> is software designed to help you identify changes in a model’s output.</p>

<p>For instance, you might be tasked with building a model to predict churn. When you deploy this model in production, you have to wait to learn which users churned in order to know how your model performed. While Predeval will not free you from this wait, it can provide initial signals as to whether the model is producing reasonable (i.e., expected) predictions. Unexpected predictions <em>might</em> reflect a poor performing model. They also <em>might</em> reflect a change in your input data. Either way, something has changed and you will want to investigate further.</p>

<p>Using predeval, you can detect changes in model output ASAP. You can then use python’s libraries to build a surrounding alerting system that will signal a need to investigate. This system should give you additional confidence that your model is performing reasonably. Here’s a <a href="https://danvatterott.com/blog/2018/06/02/random-weekly-reminders/">post</a> where I configure an alerting system using python, mailutils, and postfix (although the alerting system is not built around predeval).</p>

<p>Predeval operates by forming expectations about what your model’s outputs will look like. For example, you might give predeval the model’s output from a validation dataset. Predeval will then compare new outputs to the outputs produced by the validation dataset, and will report whether it detects a difference.</p>

<p>Predeval works with models producing both categorical and continuous outputs.</p>

<p>Here’s an <a href="https://predeval.readthedocs.io/en/latest/usage.html#categoricalevaluator">example</a> of predeval with a model producing categorical outputs. Predeval will (by default) check whether all expected output categories are present, and whether the output categories occur at their expected frequencies (using a <a href="https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.chi2_contingency.html">Chi-square test of independence of variables in a contingency table</a>).</p>

<p>Here’s an <a href="https://predeval.readthedocs.io/en/latest/usage.html#continuousevaluator">example</a> of predeval with a model producing continuous outputs. Predeval will (by default) check whether the new output have a minimum lower than expected, a maximum greater than expected, a different mean, a different standard deviation, and whether the new output are distributed as expected (using a <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ks_2samp.html#scipy.stats.ks_2samp">Kolmogorov-Smirnov test</a>)</p>

<p>I’ve tried to come up with reasonable defaults for determining whether data are different, but you can also <a href="https://predeval.readthedocs.io/en/latest/usage.html#updating-test-parameters">set these thresholds yourself</a>. You can also <a href="https://predeval.readthedocs.io/en/latest/usage.html#changing-evaluation-tests">choose what comparison tests to run</a> (e.g., checking the minimum, maximum etc.).</p>

<p>You will likely need to save your predeval objects so that you can apply them to future data. Here’s an <a href="https://predeval.readthedocs.io/en/latest/usage.html#saving-and-loading-your-evaluator">example</a> of saving the objects.</p>

<p>Documentation about how to install predeval can be found <a href="https://predeval.readthedocs.io/en/latest/installation.html#installation">here</a>.</p>

<p>If you have comments about improvements or would like to <a href="https://predeval.readthedocs.io/en/latest/contributing.html">contribute</a>, please reach out!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Looking Towards the Future of Automated Machine-learning]]></title>
    <link href="https://danvatterott.com/blog/2018/11/03/looking-towards-the-future-of-automated-machine-learning/"/>
    <updated>2018-11-03T08:27:07-05:00</updated>
    <id>https://danvatterott.com/blog/2018/11/03/looking-towards-the-future-of-automated-machine-learning</id>
    <content type="html"><![CDATA[<p>I recently gave a <a href="https://vencafstl.org/event/where-automated-machine-learning-fits-in-your-data-science-toolbox-prepare-ai">presentation</a> at <a href="https://vencafstl.org/">Venture Cafe</a> describing how I see automation changing python, machine-learning workflows in the near future.</p>

<p>In this post, I highlight the presentation’s main points. You can find the slides <a href="https://danvatterott.com/presentations/automation_presentation/">here</a>.</p>

<p>From <a href="https://en.wikipedia.org/wiki/Ray_Kurzweil">Ray Kurzweil’s</a> excitement about a <a href="https://en.wikipedia.org/wiki/Technological_singularity">technological singularity</a> to Elon Musk’s warnings about an <a href="https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x">A.I. Apocalypse</a>, automated machine-learning evokes strong feelings. Neither of these futures will be true in the near-term, but where will automation fit in your machine-learning workflows?</p>

<p>Our existing machine-learning workflows might look a little like the following (please forgive the drastic oversimplification of a purely sequential progression across stages!).</p>

<p><img src="/presentations/automation_presentation/slides/data_science_pipeline/ds_pipeline.png" style="background-color:white;" /></p>

<p>Where does automation exist in this workflow? Where can automation improve this workflow?</p>

<p>Not all these stages are within the scope of machine-learning. For instance, while you should automate gathering data, I view this as a data engineering problem. In the image below, I depict the stages that I consider ripe for automation, and the stages I consider wrong for automation. For example, data cleaning is too idiosyncratic to each dataset for true automation. I “X” out model evaluation as wrong for automation. In retrospect, I believe this is a great place for automation, but I don’t know of any existing python packages handling it.</p>

<p><img src="/presentations/automation_presentation/slides/libraries_pipeline/tpot_pipeline.png" style="background-color:white;" /></p>

<p>I depict feature engineering and model selection as the most promising areas for automation. I consider feature engineering as the stage where advances in automation can have the largest impact on your model performance. In the presentation, I include a <a href="https://www.quora.com/What-generally-improves-a-models-score-more-on-average-feature-engineering-or-hyperparameter-tuning">strong quote</a> from a Quora user saying that <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyper-parameter tuning</a> (a part of model selection) “hardly matters at all.” I agree with the sentiment of this quote, but it’s not true. Choosing roughly the correct hyper-parameter values is <em>VERY</em> important, and choosing the very best hyper-parameter values can be equally important depending on how your model is used. I highlight feature engineering over model selection because automated model selection is largely solved. For example <a href="http://scikit-learn.org/stable/modules/grid_search.html">grid-search</a> automates model selection. It’s not a fast solution, but given infinite time, it will find the best hyper-parameter values!</p>

<p>There are many python libraries automating these parts of the workflow. I highlight three libraries that automate feature engineering.</p>

<p><img src="/presentations/automation_presentation/slides/feature_automation/tpot-logo.jpg" width="200" style="background-color:white;" /></p>

<p>The first is <a href="https://github.com/EpistasisLab/tpot">teapot</a>. Teapot (more or less) takes all the different operations and models available in <a href="http://scikit-learn.org/stable/">scikit-learn</a>, and allows you to assemble these operations into a pipeline. Many of these operations (e.g., <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>) are forms of feature engineering. Teapot measures which operations lead to the best model performance. Because Teapot enables users to assemble <em>SO MANY</em> different operations, it utilizes a <a href="https://en.wikipedia.org/wiki/Genetic_algorithm">genetic search algorithm</a> to search through the different possibilities more efficiently than grid-search would.</p>

<p>The second is <a href="https://github.com/ClimbsRocks/auto_ml">auto_ml</a>. In auto_ml users simply pass a dataset to the software and it will do model selection and hyper-parameter tuning for you. Users can also <a href="https://auto-ml.readthedocs.io/en/latest/deep_learning.html#feature-learning">ask the software to train a deep learning model that will learn new features</a> from your dataset. The authors claim this approach can improve model accuracy by 5%.</p>

<p><img src="/presentations/automation_presentation/slides/feature_automation/featuretools.png" width="400" style="background-color:white;" /></p>

<p>The third is <a href="https://github.com/Featuretools/featuretools">feature tools</a>. Feature Tools is the piece of automation software whose future I am most excited about. I find this software exciting because users can feed it pre-aggregated data. Most machine-learning models expect that for each value of the <a href="https://www.quora.com/What-is-a-response-variable-in-statistics">response variable</a>, you supply a vector of explanatory variables. This is an example of aggregated data. Teapot and auto_ml both expect users to supply aggregated data. Lots of important information is lost in the aggregation process, and allowing automation to thoroughly explore different aggregations will lead to predictive features that we would not have created otherwise (any many believe this is why deep learning is so effective). Feature tools explores different aggregations all while creating easily interpreted variables (in contrast to deep learning). While I am excited about the future of feature tools, it is a new piece of software and has a ways to go before I use it in my workflows. Like most automation machine-learning software it’s very slow/resource intensive. Also, the software is not very intuitive. That said, I created a <a href="https://mybinder.org/v2/gh/dvatterott/explore_feature_automation/master">binder notebook</a> demoing feature tools, so check it out yourself!</p>

<p>We should always keep in mind the possible dangers of automation and machine-learning. Removing humans from decisions accentuates biases baked into data and algorithms. These accentuated biases can have dangerous effects. We should carefully choose which decisions we’re comfortable automating and what safeguards to build around these decisions. Check out <a href="https://en.wikipedia.org/wiki/Cathy_O%27Neil">Cathy O’Neil’s</a> amazing <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">Weapons for Math Destruction</a> for an excellent treatment of the topic.</p>

<p>This post makes no attempt to give an exhaustive view of automated machine-learning. This is my single view point on where I think automated machine-learning can have an impact on your python workflows in the near-term. For a more thorough view of automated machine-learning, check out this <a href="https://twitter.com/randal_olson/status/992105498894831616">presentation</a> by <a href="http://www.randalolson.com/">Randy Olson</a> (the creator of teapot).</p>
]]></content>
  </entry>
  
</feed>
