<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Data Engineering | Dan Vatterott]]></title>
  <link href="https://danvatterott.com/blog/categories/data-engineering/atom.xml" rel="self"/>
  <link href="https://danvatterott.com/"/>
  <updated>2022-02-25T14:33:11-06:00</updated>
  <id>https://danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Posting Collections as Hive Tables]]></title>
    <link href="https://danvatterott.com/blog/2020/08/10/posting-collections-as-hive-tables/"/>
    <updated>2020-08-10T20:03:43-05:00</updated>
    <id>https://danvatterott.com/blog/2020/08/10/posting-collections-as-hive-tables</id>
    <content type="html"><![CDATA[<p>I was recently asked to post a series of parquet collection as tables so analysts could query them in SQL. This should be straight forward, but it took me awhile to figure out. Hopefully, you find this post before spending too much time on such an easy task.</p>

<p>You should use the <a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-table.html"><code>CREATE TABLE</code></a>. This is pretty straight forward. By creating a permanent table (rather than a temp table), you can use a database name. Also, by using a table (rather than  a view), you can load the data from an s3 location.</p>

<p>Next, you can specify the table’s schema. Again, this is pretty straight forward. Columns used to partition the data should be declared here.</p>

<p>Next, you can specify how the data is stored (below, I use Parquet) and how the data is partitioned (below, there are two partitioning columns).</p>

<p>Finally, you specify the data’s location.</p>

<p>The part that really threw me for a loop here is that I wasn’t done yet! You need one more command so that Spark can go examine the partitions - <a href="https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-repair-table.html"><code>MSCK REPAIR TABLE</code></a>. Also please note that this command needs to be re-run whenever a partition is added.</p>

<p>{% codeblock lang:python %}
spark.sql(“””
CREATE TABLE my_db.my_table (
(example_key INT, example_col STRING, example_string STRING, example_date STRING)
)
USING PARQUET
PARTITIONED BY (example_string, example_date)
LOCATION ‘s3://my.example.bucket/my_db/my_table/’
“””
spark.sql(“MSCK REPAIR TABLE my_db.my_table”)
{% endcodeblock %}</p>

<p>Hope this post saves you some time!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Limiting Cardinality With a PySpark Custom Transformer]]></title>
    <link href="https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer/"/>
    <updated>2019-07-12T06:30:28-05:00</updated>
    <id>https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer</id>
    <content type="html"><![CDATA[<p>When onehot-encoding columns in pyspark, <a href="https://livebook.datascienceheroes.com/data-preparation.html#high_cardinality_descriptive_stats">column cardinality</a> can become a problem. The size of the data often leads to an enourmous number of unique values. If a minority of the values are common and the majority of the values are rare, you might want to represent the rare values as a single group. Note that this might not be appropriate for your problem. <a href="https://livebook.datascienceheroes.com/data-preparation.html#analysis-for-predictive-modeling">Here’s</a> some nice text describing the costs and benefits of this approach. In the following blog post I describe how to implement this solution.</p>

<p>I begin by importing the necessary libraries and creating a spark session.</p>

<p>{% codeblock lang:python %}
import string
import random
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark import keyword_only
from pyspark.ml.pipeline import Transformer
from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param</p>

<p>random.seed(1)</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Next create the custom transformer. This class inherits from the <code>Transformer</code>, <code>HasInputCol</code>, and <code>HasOutputCol</code> classes. I also call an additional parameter <code>n</code> which controls the maximum cardinality allowed in the tranformed column. Because I have the additional parameter, I need some methods for calling and setting this paramter (<code>setN</code> and <code>getN</code>). Finally, there’s <code>_tranform</code> which limits the cardinality of the desired column (set by <code>inputCol</code> parameter). This tranformation method simply takes the desired column and changes all values greater than <code>n</code> to <code>n</code>. It outputs a column named by the <code>outputCol</code> parameter.</p>

<p>{% codeblock lang:python %}
class LimitCardinality(Transformer, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):  
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")  
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):  
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):  
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):  
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col)))) {% endcodeblock %}
</code></pre>

<p>Now that we have the tranformer, I will create some data and apply the transformer to it. I want categorical data, so I will randomly draw letters of the alphabet. The only trick is I’ve made some letters of the alphabet much more common than other ones.</p>

<p>{% codeblock lang:python %}</p>

<p>letter_pool = string.ascii_letters[:26]
letter_pool += ‘‘.join([x*y for x, y in zip(letter_pool[:5], range(100,50,-10))])</p>

<p>a = sc.parallelize([[x, random.choice(letter_pool)] for x in range(1000)]).toDF([“id”, “category”])
a.limit(5).show()
# +—+——–+                                                                <br />
# | id|category|
# +—+——–+
# |  0|       a|
# |  1|       c|
# |  2|       e|
# |  3|       e|
# |  4|       a|
# +—+——–+
{% endcodeblock %}</p>

<p>Take a look at the data.</p>

<p>{% codeblock lang:python %}
(a
 .groupBy(“category”)
 .agg(F.count(“*”).alias(“category_count”))
 .orderBy(F.col(“category_count”).desc())
 .limit(20)
 .show())
# +——–+————–+                                                     <br />
# |category|category_count|
# +——–+————–+
# |       b|           221|
# |       a|           217|
# |       c|           197|
# |       d|           162|
# |       e|           149|
# |       k|             5|
# |       p|             5|
# |       u|             5|
# |       f|             4|
# |       l|             3|
# |       g|             3|
# |       m|             3|
# |       o|             3|
# |       y|             3|
# |       j|             3|
# |       x|             2|
# |       n|             2|
# |       h|             2|
# |       i|             2|
# |       q|             2|
# +——–+————–+
{% endcodeblock %}</p>

<p>Now to apply the new class <code>LimitCardinality</code> after <code>StringIndexer</code> which maps each category (starting with the most common category) to numbers. This means the most common letter will be 1. <code>LimitCardinality</code> then sets the max value of <code>StringIndexer</code>’s output to <code>n</code>. <code>OneHotEncoderEstimator</code> one-hot encodes <code>LimitCardinality</code>’s output. I wrap <code>StringIndexer</code>, <code>LimitCardinality</code>, and <code>OneHotEncoderEstimator</code> into a single pipeline so that I can fit/transform the dataset at one time.</p>

<p>Note that <code>LimitCardinality</code> needs additional code in order to be saved to disk.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer
from pyspark.ml import Pipeline</p>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, stringOrderType=”frequencyDesc”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, n=10)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|          (10,[2],[1.0])|
# |  2|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  3|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+</p>

<p>fit_pipeline.transform(a).limit(5).filter(F.col(“category”) == “n”).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# | 35|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# |458|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>A quick improvement to <code>LimitCardinality</code> would be to set a column’s cardinality so that X% of rows retain their category values and 100-X% receive the default value (rather than arbitrarily selecting a cardinality limit). I implement this below. Note that <code>LimitCardinalityModel</code> is identical to the original <code>LimitCardinality</code>. The new <code>LimitCardinality</code> has a <code>_fit</code> method rather than <code>_transform</code> and this method determines a column’s cardinality.</p>

<p>In the <code>_fit</code> method I find the proportion of columns that are required to describe the requested amount of data.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.pipeline import Estimator, Model</p>

<p>class LimitCardinality(Estimator, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, proportion=None):
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.proportion = Param(self, "proportion", "Cardinality upper limit as a proportion of data.")
    self._setDefault(proportion=0.75)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, proportion=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setProportion(self, value):
    """Set cardinality limit as proportion of data."""
    return self._set(proportion=value)

def getProportion(self):
    """Get cardinality limit as proportion of data."""
    return self.getOrDefault(self.proportion)

def _fit(self, dataframe):
    """Fit transformer."""
    pandas_df = dataframe.groupBy(self.getInputCol()).agg(F.count("*").alias("my_count")).toPandas()
    n = sum((pandas_df
             .sort_values("my_count", ascending=False)
             .cumsum()["my_count"] / sum(pandas_df["my_count"])
            ) &lt; self.getProportion())
    return LimitCardinalityModel(inputCol=self.getInputCol(), outputCol=self.getOutputCol(), n=n)
</code></pre>

<p>class LimitCardinalityModel(Model, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):
    """Initialize."""
    super(LimitCardinalityModel, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col))))
</code></pre>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, handleInvalid=”skip”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, proportion=0.75)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|           (3,[2],[1.0])|
# |  2|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  3|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>There are <a href="https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159">other options</a> for dealing with high cardinality columns such as using a clustering or a <a href="https://tech.instacart.com/predicting-real-time-availability-of-200-million-grocery-items-in-us-canada-stores-61f43a16eafe">mean encoding</a> scheme.</p>

<p>Hope you find this useful and reach out if you have any questions.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Complex Aggregations in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/02/05/complex-aggregations-in-pyspark/"/>
    <updated>2019-02-05T19:09:32-06:00</updated>
    <id>https://danvatterott.com/blog/2019/02/05/complex-aggregations-in-pyspark</id>
    <content type="html"><![CDATA[<p>I’ve touched on this in <a href="https://danvatterott.com/blog/2018/09/06/python-aggregate-udfs-in-pyspark/">past posts</a>, but wanted to write a post specifically describing the power of what I call complex aggregations in PySpark.</p>

<p>The idea is that you have have a data request which initially seems to require multiple different queries, but using ‘complex aggregations’ you can create the requested data using a single query (and a single shuffle).</p>

<p>Let’s say you have a dataset like the following. You have one column (id) which is a unique key for each user, another column (group) which expresses the group that each user belongs to, and finally (value) which expresses the value of each customer. I apologize for the contrived example.</p>

<p>{% codeblock lang:python %}
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql import SparkSession
from pyspark import SparkContext</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)</p>

<p>a = sc.parallelize([[1, ‘a’, 5.1],
                    [2, ‘b’, 2.6],
                    [3, ‘b’, 3.4],
                    [4, ‘c’, 1.7]]).toDF([‘id’, ‘group’, ‘value’])
a.show()          <br />
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>group</th>
   <th>value</th>
 </tr>
 <tr>
   <td>1</td>
   <td>'a'</td>
   <td>5.1</td>
 </tr>
 <tr>
   <td>2</td>
   <td>'b'</td>
   <td>2.6</td>
 </tr>
 <tr>
   <td>3</td>
   <td>'b'</td>
   <td>3.4</td>
 </tr>
 <tr>
   <td>4</td>
   <td>'c'</td>
   <td>1.7</td>
 </tr>
</table>

<p>Let’s say someone wants the average value of group a, b, and c, <em>AND</em> the average value of users in group a <em>OR</em> b, the average value of users in group b <em>OR</em> c AND the value of users in group a <em>OR</em> c. Adds a wrinkle, right? The ‘or’ clauses prevent us from using a simple groupby, and we don’t want to have to write 4 different queries.</p>

<p>Using complex aggregations, we can access all these different conditions in a single query.</p>

<p>{% codeblock lang:python %}</p>

<p>final_data = (a
              .agg(
                F.avg(F.when(F.col(‘group’) == ‘a’, F.col(‘value’)).otherwise(None)).alias(‘group_a_avg’),
                F.avg(F.when(F.col(‘group’) == ‘b’, F.col(‘value’)).otherwise(None)).alias(‘group_b_avg’),
                F.avg(F.when(F.col(‘group’) == ‘c’, F.col(‘value’)).otherwise(None)).alias(‘group_c_avg’),
                F.avg((F.when(F.col(‘group’) == ‘a’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘b’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_ab_avg’),
                F.avg((F.when(F.col(‘group’) == ‘b’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘c’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_bc_avg’),
                F.avg((F.when(F.col(‘group’) == ‘a’, F.col(‘value’))
                        .when(F.col(‘group’) == ‘c’, F.col(‘value’))
                        .otherwise(None)
                      )).alias(‘group_ac_avg’),
                )
              )</p>

<p>final_data.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>group_a_avg</th>
   <th>group_b_avg</th>
   <th>group_c_avg</th>
   <th>group_ab_avg</th>
   <th>group_ac_avg</th>
   <th>group_bc_avg</th>
 </tr>
 <tr>
   <td>5.1</td>
   <td>3.0</td>
   <td>1.7</td>
   <td>3.7</td>
   <td>3.4</td>
   <td>2.6</td>
 </tr>
</table>

<p>They key here is using  <code>when</code> to filter different data in and out of different aggregations.</p>

<p>This approach can be quite concise when used with python list comprehensions. I’ll rewrite the query above, but using a list comprehension.</p>

<p>{% codeblock lang:python %}
from itertools import combinations</p>

<p>groups  = [‘a’, ‘b’, ‘c’]
combos = [x for x in combinations(groups,  2)]
print(combos)
#[(‘a’, ‘b’), (‘a’, ‘c’), (‘b’, ‘c’)]</p>

<p>single_group = [F.avg(F.when(F.col(‘group’) == x, F.col(‘value’)).otherwise(None)).alias(‘group_%s_avg’ % x) for x in groups]
double_group = [F.avg(F.when(F.col(‘group’) == x, F.col(‘value’)).when(F.col(‘group’)==y, F.col(‘value’)).otherwise(None)).alias(‘group_%s%s_avg’ % (x, y)) for x, y in combos]
final_data = a.agg(*single_group + double_group)
final_data.show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>group_a_avg</th>
   <th>group_b_avg</th>
   <th>group_c_avg</th>
   <th>group_ab_avg</th>
   <th>group_ac_avg</th>
   <th>group_bc_avg</th>
 </tr>
 <tr>
   <td>5.1</td>
   <td>3.0</td>
   <td>1.7</td>
   <td>3.7</td>
   <td>3.4</td>
   <td>2.6</td>
 </tr>
</table>

<p>Voila! Hope you find this little trick helpful! Let me know if you have any questions or comments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python Aggregate UDFs in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2018/09/06/python-aggregate-udfs-in-pyspark/"/>
    <updated>2018-09-06T16:04:43-05:00</updated>
    <id>https://danvatterott.com/blog/2018/09/06/python-aggregate-udfs-in-pyspark</id>
    <content type="html"><![CDATA[<p>PySpark has a great set of <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.agg">aggregate</a> functions (e.g., <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData">count, countDistinct, min, max, avg, sum</a>), but these are not enough for all cases (particularly if you’re trying to avoid costly Shuffle operations).</p>

<p>PySpark currently has <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf">pandas_udfs</a>, which can create custom aggregators, but you can only “apply” one pandas_udf at a time. If you want to use more than one, you’ll have to preform multiple groupBys…and there goes avoiding those shuffles.</p>

<p>In this post I describe a little hack which enables you to create simple python UDFs which act on aggregated data (this functionality is only supposed to exist in Scala!).</p>

<p>{% codeblock lang:python %}
from pyspark.sql import functions as F
from pyspark.sql import types as T</p>

<p>a = sc.parallelize([[1, ‘a’],
                    [1, ‘b’],
                    [1, ‘b’],
                    [2, ‘c’]]).toDF([‘id’, ‘value’])
a.show()          <br />
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>value</th>
 </tr>
 <tr>
   <td>1</td>
   <td>'a'</td>
 </tr>
 <tr>
   <td>1</td>
   <td>'b'</td>
 </tr>
 <tr>
   <td>1</td>
   <td>'b'</td>
 </tr>
 <tr>
   <td>2</td>
   <td>'c'</td>
 </tr>
</table>

<p>I use <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.collect_list">collect_list</a> to bring all data from a given group into a single row. I print the output of this operation below.</p>

<p>{% codeblock lang:python %}</p>

<p>a.groupBy(‘id’).agg(F.collect_list(‘value’).alias(‘value_list’)).show()</p>

<p>{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>value_list</th>
 </tr>
 <tr>
   <td>1</td>
   <td>['a', 'b', 'b']</td>
 </tr>
 <tr>
   <td>2</td>
   <td>['c']</td>
 </tr>
</table>

<p>I then create a <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf">UDF</a> which will count all the occurences of the letter ‘a’ in these lists (this can be easily done without a UDF but you get the point). This UDF wraps around collect_list, so it acts on the output of collect_list.</p>

<p>{% codeblock lang:python %}
def find_a(x):
  “"”Count ‘a’s in list.”””
  output_count = 0
  for i in x:
    if i == ‘a’:
      output_count += 1
  return output_count</p>

<p>find_a_udf = F.udf(find_a, T.IntegerType())</p>

<p>a.groupBy(‘id’).agg(find_a_udf(F.collect_list(‘value’)).alias(‘a_count’)).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>a_count</th>
 </tr>
 <tr>
   <td>1</td>
   <td>1</td>
 </tr>
 <tr>
   <td>2</td>
   <td>0</td>
 </tr>
</table>

<p>There we go! A UDF that acts on aggregated data! Next, I show the power of this approach when combined with <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.when">when</a> which let’s us control which data enters F.collect_list.</p>

<p>First, let’s create a dataframe with an extra column.</p>

<p>{% codeblock lang:python %}
from pyspark.sql import functions as F
from pyspark.sql import types as T</p>

<p>a = sc.parallelize([[1, 1, ‘a’],
                    [1, 2, ‘a’],
                    [1, 1, ‘b’],
                    [1, 2, ‘b’],
                    [2, 1, ‘c’]]).toDF([‘id’, ‘value1’, ‘value2’])
a.show()          <br />
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>value1</th>
   <th>value2</th>
 </tr>
 <tr>
   <td>1</td>
   <td>1</td>
   <td>'a'</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2</td>
   <td>'a'</td>
 </tr>
 <tr>
   <td>1</td>
   <td>1</td>
   <td>'b'</td>
 </tr>
 <tr>
   <td>1</td>
   <td>2</td>
   <td>'b'</td>
 </tr>
 <tr>
   <td>2</td>
   <td>1</td>
   <td>'c'</td>
 </tr>
</table>

<p>Notice, how I included a when in the collect_list. Note that the UDF still wraps around collect_list.</p>

<p>{% codeblock lang:python %}</p>

<p>a.groupBy(‘id’).agg(find_a_udf( F.collect_list(F.when(F.col(‘value1’) == 1, F.col(‘value2’)))).alias(‘a_count’)).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>id</th>
   <th>a_count</th>
 </tr>
 <tr>
   <td>1</td>
   <td>1</td>
 </tr>
 <tr>
   <td>2</td>
   <td>0</td>
 </tr>
</table>

<p>There we go! Hope you find this info helpful!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom Email Alerts in Airflow]]></title>
    <link href="https://danvatterott.com/blog/2018/08/29/custom-email-alerts-in-airflow/"/>
    <updated>2018-08-29T18:19:42-05:00</updated>
    <id>https://danvatterott.com/blog/2018/08/29/custom-email-alerts-in-airflow</id>
    <content type="html"><![CDATA[<p><a href="https://airflow.apache.org/">Apache Airflow</a> is great for coordinating automated jobs, and it provides a simple interface for sending email alerts when these jobs fail. Typically, one can request these emails by setting <code>email_on_failure</code> to <code>True</code> in your operators.</p>

<p>These email alerts work great, but I wanted to include additional links in them (I wanted to include a link to my spark cluster which can be grabbed from the <a href="https://airflow.incubator.apache.org/_modules/airflow/contrib/operators/databricks_operator.html#DatabricksSubmitRunOperator">Databricks Operator</a>). Here’s how I created a custom email alert on job failure.</p>

<p>First, I set <code>email_on_failure</code> to <code>False</code> and use the operators’s <code>on_failure_callback</code>. I give <code>on_failure_callback</code> the function described below.</p>

<p>{% codeblock lang:python %}
from airflow.utils.email import send_email</p>

<p>def notify_email(contextDict, **kwargs):
    “"”Send custom email alerts.”””</p>

<pre><code># email title.
title = "Airflow alert: {task_name} Failed".format(**contextDict)

# email contents
body = """
Hi Everyone, &lt;br&gt;
&lt;br&gt;
There's been an error in the {task_name} job.&lt;br&gt;
&lt;br&gt;
Forever yours,&lt;br&gt;
Airflow bot &lt;br&gt;
""".format(**contextDict)

send_email('you_email@address.com', title, body) {% endcodeblock %}
</code></pre>

<p><code>send_email</code> is a function imported from Airflow. <code>contextDict</code> is a dictionary given to the callback function on error. Importantly, <code>contextDict</code> contains lots of relevant information. This includes the Task Instance (key=’ti’) and Operator Instance (key=’task’) associated with your error. I was able to use the Operator Instance, to grab the relevant cluster’s address and I included this address in my email (this exact code is not present here).</p>

<p>To use the <code>notify_email</code>, I set <code>on_failure_callback</code> equal to <code>notify_email</code>.</p>

<p>I write out a short example airflow dag below.</p>

<p>{% codeblock lang:python %}
from airflow.models import DAG
from airflow.operators import PythonOperator
from airflow.utils.dates import days_ago</p>

<p>args = {
  ‘owner’: ‘me’,
  ‘description’: ‘my_example’,
  ‘start_date’: days_ago(1)
}</p>

<h1 id="run-every-day-at-1205-utc">run every day at 12:05 UTC</h1>
<p>dag = DAG(dag_id=’example_dag’, default_args=args, schedule_interval=’0 5 * * *’)</p>

<p>def print_hello():
  return ‘hello!’</p>

<p>py_task = PythonOperator(task_id=’example’,
                         python_callable=print_hello,
                         on_failure_callback=notify_email,
                         dag=dag)</p>

<p>py_task
{% endcodeblock %}</p>

<p>Note where set <code>on_failure_callback</code> equal to <code>notify_email</code> in the <code>PythonOperator</code>.</p>

<p>Hope you find this helpful! Don’t hesitate to reach out if you have a question.</p>
]]></content>
  </entry>
  
</feed>
