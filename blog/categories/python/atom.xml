<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Dan Vatterott]]></title>
  <link href="https://danvatterott.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="https://danvatterott.com/"/>
  <updated>2022-02-25T14:33:11-06:00</updated>
  <id>https://danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Balancing Model Weights in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark/"/>
    <updated>2019-11-18T18:57:03-06:00</updated>
    <id>https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark</id>
    <content type="html"><![CDATA[<p><a href="https://www.jeremyjordan.me/imbalanced-data/">Imbalanced classes</a> is a common problem. Scikit-learn provides an easy fix - “balancing” class weights. This makes models more likely to predict the less common classes (e.g., <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">logistic regression</a>).</p>

<p>The PySpark ML API doesn’t have this same functionality, so in this blog post, I describe how to balance class weights yourself.</p>

<p>{% codeblock lang:python %}
import numpy as np
import pandas as pd
from itertools import chain
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Generate some random data and put the data in a Spark DataFrame. Note that the input variables are not predictive. The model will behave randomly. This is okay, since I am not interested in model accuracy.</p>

<p>{% codeblock lang:python %}
X = np.random.normal(0, 1, (10000, 10))</p>

<p>y = np.ones(X.shape[0]).astype(int)
y[:1000] = 0
np.random.shuffle(y)</p>

<p>print(np.mean(y)) # 0.9</p>

<p>X = np.append(X, y.reshape((10000, 1)), 1)</p>

<p>DF = spark.createDataFrame(pd.DataFrame(X))
DF = DF.withColumnRenamed(“10”, “y”)
{% endcodeblock %}</p>

<p>Here’s how Scikit-learn computes class weights when “balanced” weights are requested.</p>

<p>{% codeblock lang:python %}
# class weight
# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
# n_samples / (n_classes * np.bincount(y)).</p>

<p>class_weights = {i: ii for i, ii in zip(np.unique(y), len(y) / (len(np.unique(y)) * np.bincount(y)))}
print(class_weights) # {0: 5.0, 1: 0.5555555555555556}
{% endcodeblock %}</p>

<p>Here’s how we can compute “balanced” weights with data from a PySpark DataFrame.</p>

<p>{% codeblock lang:python %}
y_collect = DF.select(“y”).groupBy(“y”).count().collect()
unique_y = [x[“y”] for x in y_collect]
total_y = sum([x[“count”] for x in y_collect])
unique_y_count = len(y_collect)
bin_count = [x[“count”] for x in y_collect]</p>

<p>class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}
print(class_weights_spark) # {0.0: 5.0, 1.0: 0.5555555555555556}
{% endcodeblock %}</p>

<p>PySpark needs to have a weight assigned to each instance (i.e., row) in the training set. I create a mapping to apply a weight to each training instance.</p>

<p>{% codeblock lang:python %}
mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])</p>

<p>DF = DF.withColumn(“weight”, mapping_expr.getItem(F.col(“y”)))
{% endcodeblock %}</p>

<p>I assemble all the input features into a vector.</p>

<p>{% codeblock lang:python %}
assembler = VectorAssembler(inputCols=[str(x) for x in range(10)], outputCol=”features”)</p>

<p>DF = assembler.transform(DF).drop(*[str(x) for x in range(10)])
{% endcodeblock %}</p>

<p>And train a logistic regression. Without the instance weights, the model predicts all instances as the frequent class.</p>

<p>{% codeblock lang:python %}
lr = LogisticRegression(featuresCol=”features”, labelCol=”y”)
lrModel = lr.fit(DF)
lrModel.transform(DF).agg(F.mean(“prediction”)).show()
{% endcodeblock %}</p>

<pre><code>+---------------+
|avg(prediction)|
+---------------+
|            1.0|
+---------------+
</code></pre>

<p>With the weights, the model assigns half the instances to each class (even the less commmon one).</p>

<p>{% codeblock lang:python %}
lr = LogisticRegression(featuresCol=”features”, labelCol=”y”, weightCol=”weight”)
lrModel = lr.fit(DF)
lrModel.transform(DF).agg(F.mean(“prediction”)).show()
{% endcodeblock %}</p>

<pre><code>+---------------+
|avg(prediction)|
+---------------+
|         0.5089|
+---------------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a CDF in PySpark]]></title>
    <link href="https://danvatterott.com/blog/2019/08/26/creating-a-cdf-in-pyspark/"/>
    <updated>2019-08-26T19:36:15-05:00</updated>
    <id>https://danvatterott.com/blog/2019/08/26/creating-a-cdf-in-pyspark</id>
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">CDFs</a> are a useful tool for understanding your data. This tutorial will demonstrate how to create a CDF in PySpark.</p>

<p>I start by creating normally distributed, fake data.</p>

<p>{% codeblock lang:python %}
import numpy as np
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark.sql.window import Window</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)</p>

<p>a = (sc.parallelize([(float(x),) for x in np.random.normal(0, 1, 1000)]).toDF([‘X’]))
a.limit(5).show() 
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
 </tr>
 <tr>
   <td>1.3162087724709406</td>
 </tr>
 <tr>
   <td>-0.9226127327757598</td>
 </tr>
 <tr>
   <td>0.5388249247619141</td>
 </tr>
 <tr>
   <td>-0.38263792383896356</td>
 </tr>
 <tr>
   <td>0.20584675505779562</td>
 </tr>
</table>

<p>To create the CDF I need to use a <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window">window</a> function to order the data. I can then use <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.percent_rank">percent_rank</a> to retrieve the percentile associated with each value.</p>

<p>The only trick here is I round the column of interest to make sure I don’t retrieve too much data onto the master node (not a concern here, but always good to think about).</p>

<p>After rounding, I group by the variable of interest, again, to limit the amount of data returned.</p>

<p>{% codeblock lang:python %}
win = Window.orderBy(‘X’)</p>

<p>output = (a
          .withColumn(‘cumulative_probability’, F.percent_rank().over(win))
          .withColumn(“X”, F.round(F.col(“X”), 1))
          .groupBy(“X”)
          .agg(F.max(“cumulative_probability”).alias(“cumulative_probability”),F.count(‘*’).alias(“my_count”)))</p>

<p>output.limit(5).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
   <th>cumulative_probability</th>
   <th>my_count</th>
 </tr>
 <tr>
   <td>-3.5</td>
   <td>0.0</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-3.3</td>
   <td>0.001001001001001001</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.9</td>
   <td>0.002002002002002002</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.8</td>
   <td>0.003003003003003003</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.7</td>
   <td>0.004004004004004004</td>
   <td>1</td>
 </tr>
</table>

<p>A CDF should report the percent of data less than or <em>equal</em> to the specified value. The data returned above is the percent of data less than the specified value. We need to fix this by shifting the data up.</p>

<p>To shift the data, I will use the function, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lead">lead</a>.</p>

<p>{% codeblock lang:python %}
output = (a
          .withColumn(‘cumulative_probability’, F.percent_rank().over(win))
          .withColumn(“X”, F.round(F.col(“X”), 1))
          .groupBy(“X”)
          .agg(F.max(“cumulative_probability”).alias(“cumulative_probability”),F.count(‘*’).alias(“my_count”))
          .withColumn(“cumulative_probability”, F.lead(F.col(“cumulative_probability”)).over(win))
          .fillna(1, subset=[“cumulative_probability”]))</p>

<p>output.limit(5).show()
{% endcodeblock %}</p>

<table style="width:100%">
 <tr>
   <th>X</th>
   <th>cumulative_probability</th>
   <th>my_count</th>
 </tr>
 <tr>
   <td>-3.5</td>
   <td>0.001001001001001001</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-3.3</td>
   <td>0.002002002002002002</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.9</td>
   <td>0.003003003003003003</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.8</td>
   <td>0.004004004004004004</td>
   <td>1</td>
 </tr>
 <tr>
   <td>-2.7</td>
   <td>0.005005005005005005</td>
   <td>1</td>
 </tr>
</table>

<p>There we go! A CDF of the data! I hope you find this helpful!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Limiting Cardinality With a PySpark Custom Transformer]]></title>
    <link href="https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer/"/>
    <updated>2019-07-12T06:30:28-05:00</updated>
    <id>https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer</id>
    <content type="html"><![CDATA[<p>When onehot-encoding columns in pyspark, <a href="https://livebook.datascienceheroes.com/data-preparation.html#high_cardinality_descriptive_stats">column cardinality</a> can become a problem. The size of the data often leads to an enourmous number of unique values. If a minority of the values are common and the majority of the values are rare, you might want to represent the rare values as a single group. Note that this might not be appropriate for your problem. <a href="https://livebook.datascienceheroes.com/data-preparation.html#analysis-for-predictive-modeling">Here’s</a> some nice text describing the costs and benefits of this approach. In the following blog post I describe how to implement this solution.</p>

<p>I begin by importing the necessary libraries and creating a spark session.</p>

<p>{% codeblock lang:python %}
import string
import random
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark import keyword_only
from pyspark.ml.pipeline import Transformer
from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param</p>

<p>random.seed(1)</p>

<p>sc = SparkContext(“local”, “Example”)
spark = SparkSession(sc)
{% endcodeblock %}</p>

<p>Next create the custom transformer. This class inherits from the <code>Transformer</code>, <code>HasInputCol</code>, and <code>HasOutputCol</code> classes. I also call an additional parameter <code>n</code> which controls the maximum cardinality allowed in the tranformed column. Because I have the additional parameter, I need some methods for calling and setting this paramter (<code>setN</code> and <code>getN</code>). Finally, there’s <code>_tranform</code> which limits the cardinality of the desired column (set by <code>inputCol</code> parameter). This tranformation method simply takes the desired column and changes all values greater than <code>n</code> to <code>n</code>. It outputs a column named by the <code>outputCol</code> parameter.</p>

<p>{% codeblock lang:python %}
class LimitCardinality(Transformer, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):  
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")  
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):  
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):  
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):  
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col)))) {% endcodeblock %}
</code></pre>

<p>Now that we have the tranformer, I will create some data and apply the transformer to it. I want categorical data, so I will randomly draw letters of the alphabet. The only trick is I’ve made some letters of the alphabet much more common than other ones.</p>

<p>{% codeblock lang:python %}</p>

<p>letter_pool = string.ascii_letters[:26]
letter_pool += ‘‘.join([x*y for x, y in zip(letter_pool[:5], range(100,50,-10))])</p>

<p>a = sc.parallelize([[x, random.choice(letter_pool)] for x in range(1000)]).toDF([“id”, “category”])
a.limit(5).show()
# +—+——–+                                                                <br />
# | id|category|
# +—+——–+
# |  0|       a|
# |  1|       c|
# |  2|       e|
# |  3|       e|
# |  4|       a|
# +—+——–+
{% endcodeblock %}</p>

<p>Take a look at the data.</p>

<p>{% codeblock lang:python %}
(a
 .groupBy(“category”)
 .agg(F.count(“*”).alias(“category_count”))
 .orderBy(F.col(“category_count”).desc())
 .limit(20)
 .show())
# +——–+————–+                                                     <br />
# |category|category_count|
# +——–+————–+
# |       b|           221|
# |       a|           217|
# |       c|           197|
# |       d|           162|
# |       e|           149|
# |       k|             5|
# |       p|             5|
# |       u|             5|
# |       f|             4|
# |       l|             3|
# |       g|             3|
# |       m|             3|
# |       o|             3|
# |       y|             3|
# |       j|             3|
# |       x|             2|
# |       n|             2|
# |       h|             2|
# |       i|             2|
# |       q|             2|
# +——–+————–+
{% endcodeblock %}</p>

<p>Now to apply the new class <code>LimitCardinality</code> after <code>StringIndexer</code> which maps each category (starting with the most common category) to numbers. This means the most common letter will be 1. <code>LimitCardinality</code> then sets the max value of <code>StringIndexer</code>’s output to <code>n</code>. <code>OneHotEncoderEstimator</code> one-hot encodes <code>LimitCardinality</code>’s output. I wrap <code>StringIndexer</code>, <code>LimitCardinality</code>, and <code>OneHotEncoderEstimator</code> into a single pipeline so that I can fit/transform the dataset at one time.</p>

<p>Note that <code>LimitCardinality</code> needs additional code in order to be saved to disk.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer
from pyspark.ml import Pipeline</p>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, stringOrderType=”frequencyDesc”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, n=10)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|          (10,[2],[1.0])|
# |  2|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  3|       e|           4.0|                    4.0| (25,[4],[1.0])|          (10,[4],[1.0])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|          (10,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+</p>

<p>fit_pipeline.transform(a).limit(5).filter(F.col(“category”) == “n”).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# | 35|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# |458|       n|          16.0|                   10.0|(25,[16],[1.0])|              (10,[],[])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>A quick improvement to <code>LimitCardinality</code> would be to set a column’s cardinality so that X% of rows retain their category values and 100-X% receive the default value (rather than arbitrarily selecting a cardinality limit). I implement this below. Note that <code>LimitCardinalityModel</code> is identical to the original <code>LimitCardinality</code>. The new <code>LimitCardinality</code> has a <code>_fit</code> method rather than <code>_transform</code> and this method determines a column’s cardinality.</p>

<p>In the <code>_fit</code> method I find the proportion of columns that are required to describe the requested amount of data.</p>

<p>{% codeblock lang:python %}
from pyspark.ml.pipeline import Estimator, Model</p>

<p>class LimitCardinality(Estimator, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, proportion=None):
    """Initialize."""
    super(LimitCardinality, self).__init__()
    self.proportion = Param(self, "proportion", "Cardinality upper limit as a proportion of data.")
    self._setDefault(proportion=0.75)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, proportion=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setProportion(self, value):
    """Set cardinality limit as proportion of data."""
    return self._set(proportion=value)

def getProportion(self):
    """Get cardinality limit as proportion of data."""
    return self.getOrDefault(self.proportion)

def _fit(self, dataframe):
    """Fit transformer."""
    pandas_df = dataframe.groupBy(self.getInputCol()).agg(F.count("*").alias("my_count")).toPandas()
    n = sum((pandas_df
             .sort_values("my_count", ascending=False)
             .cumsum()["my_count"] / sum(pandas_df["my_count"])
            ) &lt; self.getProportion())
    return LimitCardinalityModel(inputCol=self.getInputCol(), outputCol=self.getOutputCol(), n=n)
</code></pre>

<p>class LimitCardinalityModel(Model, HasInputCol, HasOutputCol):
    “"”Limit Cardinality of a column.”””</p>

<pre><code>@keyword_only
def __init__(self, inputCol=None, outputCol=None, n=None):
    """Initialize."""
    super(LimitCardinalityModel, self).__init__()
    self.n = Param(self, "n", "Cardinality upper limit.")
    self._setDefault(n=25)
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

@keyword_only
def setParams(self, inputCol=None, outputCol=None, n=None):
    """Get params."""
    kwargs = self._input_kwargs
    return self._set(**kwargs)

def setN(self, value):
    """Set cardinality limit."""
    return self._set(n=value)

def getN(self):
    """Get cardinality limit."""
    return self.getOrDefault(self.n)

def _transform(self, dataframe):
    """Do transformation."""
    out_col = self.getOutputCol()
    in_col = dataframe[self.getInputCol()]
    return (dataframe
            .withColumn(out_col, (F.when(in_col &gt; self.getN(), self.getN())
                                  .otherwise(in_col))))
</code></pre>

<p>string_to_num = StringIndexer(inputCol=”category”, outputCol=”category_index”, handleInvalid=”skip”)
censor_category = LimitCardinality(inputCol=”category_index”, outputCol=”censored_category_index”, proportion=0.75)
onehot_category = OneHotEncoderEstimator(inputCols=[“category_index”, “censored_category_index”],
                                     outputCols=[“onehot_category”, “onehot_censored_category”])
onehot_pipeline = Pipeline(stages=[string_to_num, censor_category, onehot_category])
fit_pipeline = onehot_pipeline.fit(a)</p>

<p>fit_pipeline.transform(a).limit(5).show()
# +—+——–+————–+———————–+—————+————————+
# | id|category|category_index|censored_category_index|onehot_category|onehot_censored_category|
# +—+——–+————–+———————–+—————+————————+
# |  0|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# |  1|       c|           2.0|                    2.0| (25,[2],[1.0])|           (3,[2],[1.0])|
# |  2|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  3|       e|           4.0|                    3.0| (25,[4],[1.0])|               (3,[],[])|
# |  4|       a|           1.0|                    1.0| (25,[1],[1.0])|           (3,[1],[1.0])|
# +—+——–+————–+———————–+—————+————————+
{% endcodeblock %}</p>

<p>There are <a href="https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159">other options</a> for dealing with high cardinality columns such as using a clustering or a <a href="https://tech.instacart.com/predicting-real-time-availability-of-200-million-grocery-items-in-us-canada-stores-61f43a16eafe">mean encoding</a> scheme.</p>

<p>Hope you find this useful and reach out if you have any questions.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Are Some MLB Players More Likely to Hit Into Errors: Statistics]]></title>
    <link href="https://danvatterott.com/blog/2019/06/04/are-some-mlb-players-more-likely-to-hit-into-errors-statistics/"/>
    <updated>2019-06-04T20:04:31-05:00</updated>
    <id>https://danvatterott.com/blog/2019/06/04/are-some-mlb-players-more-likely-to-hit-into-errors-statistics</id>
    <content type="html"><![CDATA[<p>In a <a href="https://danvatterott.com/blog/2019/04/19/are-some-mlb-players-more-likely-to-hit-into-errors-munging/">previous post</a>, I described how to download and clean data for understanding how likely a baseball player is to hit into an error given that they hit the ball into play.</p>

<p>This analysis will statistically demonstrate that some players are more likely to hit into errors than others.</p>

<p>Errors are uncommon, so players hit into errors very infrequently. Estimating the likelihood of an infrequent event is hard and requires lots of data. To acquire as much data as possible, I wrote a bash script that will download data for all players between 1970 and 2018.</p>

<p>This data enables me to use data from multiple years for each player, giving me more data when estimating how likely a particular player is to hit into an error.</p>

<p>{% codeblock lang:bash %}
%%bash</p>

<p>for i in {1970..2018}; do
    echo “YEAR: $i”
    ../scripts/get_data.sh ${i};
done</p>

<p>find processed_data/* -type f -name ‘errors_bip.out’ | \
    xargs awk ‘{print $0”, “FILENAME}’ | \
    sed s1processed_data/11g1 | \
    sed s1/errors_bip.out11g1 &gt; \
        processed_data/all_errors_bip.out
{% endcodeblock %}</p>

<p>The data has 5 columns: playerid, playername, errors hit into, balls hit into play (BIP), and year. The file does not have a header.</p>

<p>{% codeblock lang:bash %}
%%bash
head ../processed_data/all_errors_bip.out
{% endcodeblock %}</p>

<pre><code>aaroh101, Hank Aaron, 8, 453, 1970
aarot101, Tommie Aaron, 0, 53, 1970
abert101, Ted Abernathy, 0, 10, 1970
adaij101, Jerry Adair, 0, 24, 1970
ageet101, Tommie Agee, 12, 480, 1970
akerj102, Jack Aker, 0, 10, 1970
alcal101, Luis Alcaraz, 1, 107, 1970
alleb105, Bernie Allen, 1, 240, 1970
alled101, Dick Allen, 4, 341, 1970
alleg101, Gene Alley, 6, 356, 1970
</code></pre>

<p>I can load the data into pandas using the following command.</p>

<p>{% codeblock lang:python %}
import pandas as pd</p>

<p>DF = pd.read_csv(‘../processed_data/all_errors_bip.out’,
                 header=None,
                 names=[‘playerid’, ‘player_name’, ‘errors’, ‘bip’, ‘year’])
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
DF.head()
{% endcodeblock %}</p>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>playerid</th>
      <th>player_name</th>
      <th>errors</th>
      <th>bip</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>aaroh101</td>
      <td>Hank Aaron</td>
      <td>8</td>
      <td>453</td>
      <td>1970</td>
    </tr>
    <tr>
      <th>1</th>
      <td>aarot101</td>
      <td>Tommie Aaron</td>
      <td>0</td>
      <td>53</td>
      <td>1970</td>
    </tr>
    <tr>
      <th>2</th>
      <td>abert101</td>
      <td>Ted Abernathy</td>
      <td>0</td>
      <td>10</td>
      <td>1970</td>
    </tr>
    <tr>
      <th>3</th>
      <td>adaij101</td>
      <td>Jerry Adair</td>
      <td>0</td>
      <td>24</td>
      <td>1970</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ageet101</td>
      <td>Tommie Agee</td>
      <td>12</td>
      <td>480</td>
      <td>1970</td>
    </tr>
  </tbody>
</table>
</div>

<p>{% codeblock lang:python %}
len(DF)
{% endcodeblock %}</p>

<pre><code>38870
</code></pre>

<p>I have almost 39,000 year, player combinations…. a good amount of data to play with.</p>

<p>While exploring the data, I noticed that players hit into errors less frequently now than they used to. Let’s see how the probability that a player hits into an error has changed across the years.</p>

<p>{% codeblock lang:python %}
%matplotlib inline</p>

<p>YEAR_DF = (DF
           .groupby(“year”)
           .agg({
               “errors”: “sum”,
               “bip”: “sum”
           })
           .assign(prop_error=lambda x: x[“errors”] / x[“bip”])
          )</p>

<p>YEAR_DF[“prop_error”].plot(style=”o-“);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/error_year.png" /></p>

<p>Interestingly, the proportion of errors per BIP <a href="https://www.pinstripealley.com/2013/8/16/4623050/mlb-errors-trends-statistics">has been dropping over time</a>. I am not sure if this is a conscious effort by MLB score keepers, a change in how hitters hit, or improved fielding (but I suspect it’s the score keepers). It looks like this drop in errors per BIP leveled off around 2015. Zooming in.</p>

<p>{% codeblock lang:python %}
YEAR_DF[YEAR_DF.index &gt; 2010][“prop_error”].plot(style=”o-“);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/zoom_error_year.png" /></p>

<p>I explore this statistically in <a href="https://github.com/dvatterott/mlb_errors/blob/master/notebook/PYMC%20-%20Hierarchical%20Beta%20Binomial%20YEAR.ipynb">a jupyter notebook on my github</a>.</p>

<p>Because I don’t want year to confound the analysis, I remove all data before 2015.</p>

<p>{% codeblock lang:python %}
DF = DF[DF[“year”] &gt;= 2015]
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
len(DF)
{% endcodeblock %}</p>

<pre><code>3591
</code></pre>

<p>This leaves me with 3500 year, player combinations.</p>

<p>Next I combine players’ data across years.</p>

<p>{% codeblock lang:python %}
GROUPED_DF = DF.groupby([“playerid”, “player_name”]).agg({“errors”: “sum”, “bip”: “sum”}).reset_index()
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
GROUPED_DF.describe()
{% endcodeblock %}</p>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>errors</th>
      <th>bip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1552.000000</td>
      <td>1552.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.835052</td>
      <td>324.950387</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.073256</td>
      <td>494.688755</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>69.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>5.000000</td>
      <td>437.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>37.000000</td>
      <td>2102.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>I want an idea for how likely players are to hit into errors.</p>

<p>{% codeblock lang:python %}
TOTALS = GROUPED_DF.agg({“errors”: “sum”, “bip”: “sum”})
ERROR_RATE = TOTALS[“errors”] / TOTALS[“bip”]
ERROR_RATE
{% endcodeblock %}</p>

<pre><code>0.011801960251664112
</code></pre>

<p>Again, errors are very rare, so I want know how many “trials” (BIP) I need for a reasonable estimate of how likely each player is to hit into an error.</p>

<p>I’d like the majority of players to have at least 5 errors. I can estimate how many BIP that would require.</p>

<p>{% codeblock lang:python %}
5. /ERROR_RATE
{% endcodeblock %}</p>

<pre><code>423.65843413978496
</code></pre>

<p>Looks like I should require at least 425 BIP for each player. I round this to 500.</p>

<p>{% codeblock lang:python %}
GROUPED_DF = GROUPED_DF[GROUPED_DF[“bip”] &gt; 500]
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
GROUPED_DF = GROUPED_DF.reset_index(drop=True)
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
GROUPED_DF.head()
{% endcodeblock %}</p>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>playerid</th>
      <th>player_name</th>
      <th>errors</th>
      <th>bip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>abrej003</td>
      <td>Jose Abreu</td>
      <td>20</td>
      <td>1864</td>
    </tr>
    <tr>
      <th>1</th>
      <td>adamm002</td>
      <td>Matt Adams</td>
      <td>6</td>
      <td>834</td>
    </tr>
    <tr>
      <th>2</th>
      <td>adrie001</td>
      <td>Ehire Adrianza</td>
      <td>2</td>
      <td>533</td>
    </tr>
    <tr>
      <th>3</th>
      <td>aguij001</td>
      <td>Jesus Aguilar</td>
      <td>2</td>
      <td>551</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ahmen001</td>
      <td>Nick Ahmed</td>
      <td>12</td>
      <td>1101</td>
    </tr>
  </tbody>
</table>
</div>

<p>{% codeblock lang:python %}
GROUPED_DF.describe()
{% endcodeblock %}</p>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>errors</th>
      <th>bip</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>354.000000</td>
      <td>354.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>12.991525</td>
      <td>1129.059322</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.447648</td>
      <td>428.485467</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>503.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>8.000000</td>
      <td>747.250000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>12.000000</td>
      <td>1112.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>17.000000</td>
      <td>1475.750000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>37.000000</td>
      <td>2102.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>I’ve identified 354 players who have enough BIP for me to estimate how frequently they hit into errors.</p>

<p>Below, I plot how the likelihood of hitting into errors is distributed.</p>

<p>{% codeblock lang:python %}
%matplotlib inline</p>

<p>GROUPED_DF[“prop_error”] = GROUPED_DF[“errors”] / GROUPED_DF[“bip”]
GROUPED_DF[“prop_error”].hist();
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/error_dist.png" /></p>

<p>The question is whether someone who has hit into errors in 2% of their BIP is more likely to hit into an error than someone who has hit into errors in 0.5% of their BIP (or is this all just random variation).</p>

<p>To try and estimate this, I treat each BIP as a Bernoulli trial. Hitting into an error is a “success”. I use a Binomial distribution to model the number of “successes”. I would like to know if different players are more or less likely to hit into errors. To do this, I model each player as having their own Binomial distribution and ask whether <em>p</em> (the probability of success) differs across players.</p>

<p>To answer this question, I could use a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html#scipy.stats.chi2_contingency">chi square contingency test</a> but this would only tell me whether players differ at all and not which players differ.</p>

<p>The traditional way to identify which players differ is to do pairwise comparisons, but this would result in TONS of comparisons making <a href="https://en.wikipedia.org/wiki/Multiple_comparisons_problem">false positives all but certain</a>.</p>

<p>Another option is to harness Bayesian statistics and build a <a href="http://sl8r000.github.io/ab_testing_statistics/use_a_hierarchical_model/">Hierarchical Beta-Binomial model</a>. The intuition is that each player’s probability of hitting into an error is drawn from a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>. I want to know whether these Beta distributions are different. I then assume I can best estimate a player’s Beta distribution by using that particular player’s data AND data from all players together.</p>

<p>The model is built so that as I accrue data about a particular player, I will trust that data more and more, relying less and less on data from all players. This is called partial pooling. <a href="https://dsaber.com/2016/08/27/analyze-your-experiment-with-a-multilevel-logistic-regression-using-pymc3%E2%80%8B/">Here’s</a> a useful explanation.</p>

<p>I largely based my analysis on <a href="https://docs.pymc.io/notebooks/hierarchical_partial_pooling.html">this</a> tutorial. Reference the tutorial for an explanation of how I choose my priors. I ended up using a greater lambda value (because the model sampled better) in the Exponential prior, and while this did lead to more extreme estimates of error likelihood, it didn’t change the basic story.</p>

<p>{% codeblock lang:python %}
import pymc3 as pm
import numpy as np
import theano.tensor as tt</p>

<p>with pm.Model() as model:</p>

<pre><code>phi = pm.Uniform('phi', lower=0.0, upper=1.0)

kappa_log = pm.Exponential('kappa_log', lam=25.)
kappa = pm.Deterministic('kappa', tt.exp(kappa_log))

rates = pm.Beta('rates', alpha=phi*kappa, beta=(1.0-phi)*kappa, shape=len(GROUPED_DF))

trials = np.array(GROUPED_DF["bip"])
successes = np.array(GROUPED_DF["errors"])
 
obs = pm.Binomial('observed_values', trials, rates, observed=successes)
trace = pm.sample(2000, tune=1000, chains=2, cores=2, nuts_kwargs={'target_accept': .95}) {% endcodeblock %}

Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [rates, kappa_log, phi]
Sampling 2 chains: 100%|██████████| 6000/6000 [01:47&lt;00:00, 28.06draws/s] 
</code></pre>

<p>Check whether the model converged.</p>

<p>{% codeblock lang:python %}
max(np.max(score) for score in pm.gelman_rubin(trace).values())
{% endcodeblock %}</p>

<pre><code>1.0022635936332533
</code></pre>

<p>{% codeblock lang:python %}
bfmi = pm.bfmi(trace)
max_gr = max(np.max(gr_stats) for gr_stats in pm.gelman_rubin(trace).values())
(pm.energyplot(trace, figsize=(6, 4)).set_title(“BFMI = {}\nGelman-Rubin = {}”.format(bfmi, max_gr)));
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/energy.png" /></p>

<p>The most challenging parameter to fit is <em>kappa</em> which modulates for the variance in the likelihood to hit into an error. I take a look at it to make sure things look as expected.</p>

<p>{% codeblock lang:python %}
pm.summary(trace, varnames=[“kappa”])
{% endcodeblock %}</p>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>mc_error</th>
      <th>hpd_2.5</th>
      <th>hpd_97.5</th>
      <th>n_eff</th>
      <th>Rhat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>kappa</th>
      <td>927.587178</td>
      <td>141.027597</td>
      <td>4.373954</td>
      <td>657.066554</td>
      <td>1201.922608</td>
      <td>980.288914</td>
      <td>1.000013</td>
    </tr>
  </tbody>
</table>
</div>

<p>{% codeblock lang:python %}
pm.traceplot(trace, varnames=[‘kappa’]);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/kappa.png" /></p>

<p>I can also look at <em>phi</em>, the estimated global likelihood to hit into an error.</p>

<p>{% codeblock lang:python %}
pm.traceplot(trace, varnames=[‘phi’]);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/phi.png" /></p>

<p>Finally, I can look at how all players vary in their likelihood to hit into an error.</p>

<p>{% codeblock lang:python %}
pm.traceplot(trace, varnames=[‘rates’]);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/rate_trace.png" /></p>

<p>Obviously, the above plot is a lot to look at it, so let’s order players by how likely the model believes they are to hit in an error.</p>

<p>{% codeblock lang:python %}
from matplotlib import pyplot as plt</p>

<p>rate_means = trace[‘rates’, 1000:].mean(axis=0)
rate_se = trace[‘rates’, 1000:].std(axis=0)</p>

<p>mean_se = [(x, y, i) for i, x, y in zip(GROUPED_DF.index, rate_means, rate_se)]
sorted_means_se = sorted(mean_se, key=lambda x: x[0])
sorted_means = [x[0] for x in sorted_means_se]
sorted_se = [x[1] for x in sorted_means_se]</p>

<p>x = np.arange(len(sorted_means))</p>

<p>plt.plot(x, sorted_means, ‘o’, alpha=0.25);</p>

<p>for x_val, m, se in zip(x, sorted_means, sorted_se):
    plt.plot([x_val, x_val], [m-se, m+se], ‘b-‘, alpha=0.5)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/players_ranked.png" /></p>

<p>Now, the ten players who are most likely to hit into an error.</p>

<p>{% codeblock lang:python %}
estimated_mean = pm.summary(trace, varnames=[“rates”]).iloc[[x[2] for x in sorted_means_se[-10:]]][“mean”]</p>

<p>GROUPED_DF.loc[[x[2] for x in sorted_means_se[-10:]], :].assign(estimated_mean=estimated_mean.values).iloc[::-1]
{% endcodeblock %}</p>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>playerid</th>
      <th>player_name</th>
      <th>errors</th>
      <th>bip</th>
      <th>prop_error</th>
      <th>estimated_mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>71</th>
      <td>corrc001</td>
      <td>Carlos Correa</td>
      <td>30</td>
      <td>1368</td>
      <td>0.021930</td>
      <td>0.017838</td>
    </tr>
    <tr>
      <th>227</th>
      <td>myerw001</td>
      <td>Wil Myers</td>
      <td>27</td>
      <td>1214</td>
      <td>0.022241</td>
      <td>0.017724</td>
    </tr>
    <tr>
      <th>15</th>
      <td>andre001</td>
      <td>Elvis Andrus</td>
      <td>37</td>
      <td>1825</td>
      <td>0.020274</td>
      <td>0.017420</td>
    </tr>
    <tr>
      <th>258</th>
      <td>plawk001</td>
      <td>Kevin Plawecki</td>
      <td>14</td>
      <td>528</td>
      <td>0.026515</td>
      <td>0.017200</td>
    </tr>
    <tr>
      <th>285</th>
      <td>rojam002</td>
      <td>Miguel Rojas</td>
      <td>21</td>
      <td>952</td>
      <td>0.022059</td>
      <td>0.017001</td>
    </tr>
    <tr>
      <th>118</th>
      <td>garca003</td>
      <td>Avisail Garcia</td>
      <td>28</td>
      <td>1371</td>
      <td>0.020423</td>
      <td>0.016920</td>
    </tr>
    <tr>
      <th>244</th>
      <td>pench001</td>
      <td>Hunter Pence</td>
      <td>22</td>
      <td>1026</td>
      <td>0.021442</td>
      <td>0.016875</td>
    </tr>
    <tr>
      <th>20</th>
      <td>baezj001</td>
      <td>Javier Baez</td>
      <td>23</td>
      <td>1129</td>
      <td>0.020372</td>
      <td>0.016443</td>
    </tr>
    <tr>
      <th>335</th>
      <td>turnt001</td>
      <td>Trea Turner</td>
      <td>23</td>
      <td>1140</td>
      <td>0.020175</td>
      <td>0.016372</td>
    </tr>
    <tr>
      <th>50</th>
      <td>cainl001</td>
      <td>Lorenzo Cain</td>
      <td>32</td>
      <td>1695</td>
      <td>0.018879</td>
      <td>0.016332</td>
    </tr>
  </tbody>
</table>
</div>

<p>And the 10 players who are least likely to hit in an error.</p>

<p>{% codeblock lang:python %}
estimated_mean = pm.summary(trace, varnames=[“rates”]).iloc[[x[2] for x in sorted_means_se[:10]]][“mean”]</p>

<p>GROUPED_DF.loc[[x[2] for x in sorted_means_se[:10]], :].assign(estimated_mean=estimated_mean.values)
{% endcodeblock %}</p>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>playerid</th>
      <th>player_name</th>
      <th>errors</th>
      <th>bip</th>
      <th>prop_error</th>
      <th>estimated_mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>226</th>
      <td>murpd006</td>
      <td>Daniel Murphy</td>
      <td>4</td>
      <td>1680</td>
      <td>0.002381</td>
      <td>0.005670</td>
    </tr>
    <tr>
      <th>223</th>
      <td>morrl001</td>
      <td>Logan Morrison</td>
      <td>4</td>
      <td>1241</td>
      <td>0.003223</td>
      <td>0.006832</td>
    </tr>
    <tr>
      <th>343</th>
      <td>vottj001</td>
      <td>Joey Votto</td>
      <td>8</td>
      <td>1724</td>
      <td>0.004640</td>
      <td>0.007112</td>
    </tr>
    <tr>
      <th>239</th>
      <td>panij002</td>
      <td>Joe Panik</td>
      <td>7</td>
      <td>1542</td>
      <td>0.004540</td>
      <td>0.007245</td>
    </tr>
    <tr>
      <th>51</th>
      <td>calhk001</td>
      <td>Kole Calhoun</td>
      <td>9</td>
      <td>1735</td>
      <td>0.005187</td>
      <td>0.007413</td>
    </tr>
    <tr>
      <th>55</th>
      <td>carpm002</td>
      <td>Matt Carpenter</td>
      <td>8</td>
      <td>1566</td>
      <td>0.005109</td>
      <td>0.007534</td>
    </tr>
    <tr>
      <th>142</th>
      <td>hamib001</td>
      <td>Billy Hamilton</td>
      <td>8</td>
      <td>1476</td>
      <td>0.005420</td>
      <td>0.007822</td>
    </tr>
    <tr>
      <th>289</th>
      <td>rosae001</td>
      <td>Eddie Rosario</td>
      <td>8</td>
      <td>1470</td>
      <td>0.005442</td>
      <td>0.007855</td>
    </tr>
    <tr>
      <th>275</th>
      <td>renda001</td>
      <td>Anthony Rendon</td>
      <td>9</td>
      <td>1564</td>
      <td>0.005754</td>
      <td>0.007966</td>
    </tr>
    <tr>
      <th>8</th>
      <td>alony001</td>
      <td>Yonder Alonso</td>
      <td>8</td>
      <td>1440</td>
      <td>0.005556</td>
      <td>0.008011</td>
    </tr>
  </tbody>
</table>
</div>

<p>It looks to me like players who hit more ground balls are more likely to hit into an error than players who predominately hits fly balls and line-drives. This makes sense since infielders make more errors than outfielders.</p>

<p>Using the posterior distribution of estimated likelihoods to hit into an error, I can assign a probability to whether Carlos Correa is more likely to hit into an error than Daniel Murphy.</p>

<p>{% codeblock lang:python %}
np.mean(trace[‘rates’, 1000:][:, 71] &lt;= trace[‘rates’, 1000:][:, 226])
{% endcodeblock %}</p>

<pre><code>0.0
</code></pre>

<p>The model believes Correa is much more likely to hit into an error than Murphy!</p>

<p>I can also plot these players’ posterior distributions.</p>

<p>{% codeblock lang:python %}
import seaborn as sns</p>

<p>sns.kdeplot(trace[‘rates’, 1000:][:, 226], shade=True, label=”Daniel Murphy”);
sns.kdeplot(trace[‘rates’, 1000:][:, 71], shade=True, label=”Carlos Correa”);
sns.kdeplot(trace[‘rates’, 1000:].flatten(), shade=True, label=”Overall”);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/play_comparison.png" /></p>

<p>Finally, I can look exclusively at how the posterior distributions of the ten most likely and 10 least likely players to hit into an error compare.</p>

<p>{% codeblock lang:python %}
sns.kdeplot(trace[‘rates’, 1000:][:, [x[2] for x in sorted_means_se[-10:]]].flatten(), shade=True, label=”10 Least Likely”);
sns.kdeplot(trace[‘rates’, 1000:][:, [x[2] for x in sorted_means_se[:10]]].flatten(), shade=True, label=”10 Most Likely”);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/mlb/top10.png" /></p>

<p>All in all, this analysis makes it obvious that some players are more likely to hit into errors than other players. This is probably driven by how often players hit ground balls.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Are Some Mlb Players More Likely to Hit Into Errors Than Others: Data Munging]]></title>
    <link href="https://danvatterott.com/blog/2019/04/19/are-some-mlb-players-more-likely-to-hit-into-errors-munging/"/>
    <updated>2019-04-19T11:02:56-05:00</updated>
    <id>https://danvatterott.com/blog/2019/04/19/are-some-mlb-players-more-likely-to-hit-into-errors-munging</id>
    <content type="html"><![CDATA[<p>I recently found myself wondering if some baseball players are more likely to hit into errors than others. In theory, the answer should be “no” since fielders produce errors regardless of who is hitting. Nonetheless, it’s also possible that some hitters “force” errors by hitting the ball harder or running to first base faster.</p>

<p>In order to evaluate this possibility, I found play-by-play data on <a href="https://www.retrosheet.org/">retrosheet.org</a>. This data contains row by row data describing each event (e.g., a hit, stolen base etc) in a baseball game. I’ve posted this analysis on <a href="https://github.com/dvatterott/mlb_errors">github</a> and will walk through it here.</p>

<p>The user is expected to input what year’s data they want. I write the code’s output for the year 2018 as comments. The code starts by downloading and unzipping the data.</p>

<p>{% codeblock lang:bash %}
YEAR=$1
FILE_LOC=https://www.retrosheet.org/events/${YEAR}eve.zip</p>

<p>echo “———DOWNLOAD———-“
wget -nc $FILE_LOC -O ./raw_data/${YEAR}.zip</p>

<p>echo “———UNPACK———-“
mkdir raw_data/${YEAR}/
unzip -o raw_data/${YEAR}.zip -d raw_data/${YEAR}/
{% endcodeblock %}</p>

<p>The unzipped data contain play-by-play data in files with the EVN or EVA extensions. Each team’s home stadium has its own file. I combine all the play-by play eveSSplants (.EVN and .EVA files) into a single file. I then remove all non batting events (e.g., balk, stolen base etc).</p>

<p>I also combine all the roster files (.ROS) into a single file.</p>

<p>{% codeblock lang:bash %}
# export playbyplay to single file
mkdir processed_data/${YEAR}/
find raw_data/${YEAR}/ -regex ‘.*EV[A|N]’ | \
	xargs grep play &gt; \
	./processed_data/${YEAR}/playbyplay.out</p>

<h1 id="get-all-plate-appearances-from-data-and-hitter-remove-all-non-plate-appearance-rows">get all plate appearances from data (and hitter). remove all non plate appearance rows</h1>
<p>cat ./processed_data/${YEAR}/playbyplay.out | \
	awk -F’,’ ‘{print $4”,”$7}’ | \
	grep -Ev ‘,[A-Z]{3}[0-9]{2}’ | \
	grep -Ev ‘,(NP|BK|CS|DI|OA|PB|WP|PO|POCS|SB|FLE)’ &gt; \
	./processed_data/${YEAR}/batters.out</p>

<h1 id="one-giant-roster-file">one giant roster file</h1>
<p>find raw_data/${YEAR}/ -name ‘*ROS’ | \
	xargs awk -F’,’ ‘{print $1” “$2” “$3}’ &gt; \
	./processed_data/${YEAR}/players.out
{% endcodeblock %}</p>

<p>In this next few code blocks I print some data just to see what I am working with. For instance, I print out players with the most plate appearances. I was able to confirm these numbers with <a href="https://baseball-reference.com">baseball-reference</a>. This operation requires me to groupby player and count the rows. I join this file with the roster file to get player’s full names.</p>

<p>{% codeblock lang:bash %}
echo “———PLAYERS WITH MOST PLATE APPEARANCES———-“
cat ./processed_data/${YEAR}/batters.out | \
	awk -F, ‘{a[$1]++;}END{for (i in a)print i, a[i];}’ | \
	sort -k2 -nr | \
	head &gt; \
	./processed_data/${YEAR}/most_pa.out
join &lt;(sort -k 1 ./processed_data/${YEAR}/players.out) &lt;(sort -k 1 ./processed_data/${YEAR}/most_pa.out) | \
	uniq | \
	sort -k 4 -nr | \
	head | \
	awk ‘{print $3”, “$2”, “$4}’</p>

<h1 id="players-with-most-plate-appearances----------">———PLAYERS WITH MOST PLATE APPEARANCES———-</h1>
<p>#Francisco, Lindor, 745
#Trea, Turner, 740
#Manny, Machado, 709
#Cesar, Hernandez, 708
#Whit, Merrifield, 707
#Freddie, Freeman, 707
#Giancarlo, Stanton, 706
#Nick, Markakis, 705
#Alex, Bregman, 705
#Marcus, Semien, 703
{% endcodeblock %}</p>

<p>Here’s the players with the most hits. Notice that I filter out all non-hits in the grep, then group by player.</p>

<p>{% codeblock lang:bash %}
echo “———PLAYERS WITH MOST HITS———-“
cat ./processed_data/${YEAR}/batters.out | \
	grep -E ‘,(S|D|T|HR)’ | \
	awk -F, ‘{a[$1]++;}END{for (i in a)print i, a[i];}’ | \
	sort -k2 -nr | \
	head</p>

<h1 id="players-with-most-hits----------">———PLAYERS WITH MOST HITS———-</h1>
<p>#merrw001 192
#freef001 191
#martj006 188
#machm001 188
#yelic001 187
#markn001 185
#castn001 185
#lindf001 183
#peraj003 182
#blacc001 182
{% endcodeblock %}</p>

<p>Here’s the players with the most at-bats.</p>

<p>{% codeblock lang:bash %}
echo “———PLAYERS WITH MOST AT BATS———-“
cat ./processed_data/${YEAR}/batters.out | \
	grep -Ev ‘SF|SH’ | \
	grep -E ‘,(S|D|T|HR|K|[0-9]|E|DGR|FC)’ | \
	awk -F, ‘{a[$1]++;}END{for (i in a)print i, a[i];}’ &gt; \
	./processed_data/${YEAR}/abs.out
cat ./processed_data/${YEAR}/abs.out | sort -k2 -nr | head</p>

<h1 id="players-with-most-at-bats----------">———PLAYERS WITH MOST AT BATS———-</h1>
<p>#turnt001 664
#lindf001 661
#albio001 639
#semim001 632
#peraj003 632
#merrw001 632
#machm001 632
#blacc001 626
#markn001 623
#castn001 620
{% endcodeblock %}</p>

<p>And, finally, here’s the players who hit into the most errors.</p>

<p>{% codeblock lang:bash %}
echo “———PLAYERS WHO HIT INTO THE MOST ERRORS———-“
cat ./processed_data/${YEAR}/batters.out | \
    	grep -Ev ‘SF|SH’ | \
	grep ‘,E[0-9]’ | \
	awk -F, ‘{a[$1]++;}END{for (i in a)print i, a[i];}’ &gt; \
	./processed_data/${YEAR}/errors.out
cat ./processed_data/${YEAR}/errors.out | sort -k2 -nr | head</p>

<h1 id="players-who-hit-into-the-most-errors----------">———PLAYERS WHO HIT INTO THE MOST ERRORS———-</h1>
<p>#gurry001 13
#casts001 13
#baezj001 12
#goldp001 11
#desmi001 11
#castn001 10
#bogax001 10
#andum001 10
#turnt001 9
#rojam002 9
{% endcodeblock %}</p>

<p>Because players with more at-bats hit into more errors, I need to take the number of at-bats into account. I also filter out all players with less than 250 at bats. I figure we only want players with lots of opportunities to create errors.</p>

<p>{% codeblock lang:bash %}
echo “———PLAYERS WITH MOST ERRORS PER AT BAT———-“
join -e”0” -a1 -a2 &lt;(sort -k 1 ./processed_data/${YEAR}/abs.out) -o 0 1.2 2.2 &lt;(sort -k 1 ./processed_data/${YEAR}/errors.out) | \
	uniq | \
	awk -v OFS=’, ‘ ‘$2 &gt; 250 {print $1, $3, $2, $3/$2}’ &gt;  \
	./processed_data/${YEAR}/errors_abs.out
cat ./processed_data/${YEAR}/errors_abs.out | sort -k 4 -nr | head</p>

<h1 id="players-with-most-errors-per-at-bat----------">———PLAYERS WITH MOST ERRORS PER AT BAT———-</h1>
<p>#pereh001, 8, 316, 0.0253165
#gurry001, 13, 537, 0.0242086
#andre001, 9, 395, 0.0227848
#casts001, 13, 593, 0.0219224
#desmi001, 11, 555, 0.0198198
#baezj001, 12, 606, 0.019802
#garca003, 7, 356, 0.0196629
#bogax001, 10, 512, 0.0195312
#goldp001, 11, 593, 0.0185497
#iglej001, 8, 432, 0.0185185
{% endcodeblock %}</p>

<p>At-bats is great but even better is to remove strike-outs and just look at occurences when a player hit the ball into play. I remove all players with less than 450 balls hit into play which limits us to just 37 players but the players have enough reps to make the estimates more reliable.</p>

<p>{% codeblock lang:bash %}
echo “———PLAYERS WITH MOST ERRORS PER BALL IN PLAY———-“
cat ./processed_data/${YEAR}/batters.out | \
	grep -Ev ‘SF|SH’ | \
	grep -E ‘,(S|D|T|HR|[0-9]|E|DGR|FC)’ | \
	awk -F, ‘{a[$1]++;}END{for (i in a)print i, a[i];}’ &gt; \
	./processed_data/${YEAR}/bip.out
join -e”0” -a1 -a2 &lt;(sort -k 1 ./processed_data/${YEAR}/bip.out) -o 0 1.2 2.2 &lt;(sort -k 1 ./processed_data/${YEAR}/errors.out) | \
	uniq | \
	awk -v OFS=’, ‘ ‘$2 &gt; 450 {print $1, $3, $2, $3/$2}’ &gt; \
	./processed_data/${YEAR}/errors_bip.out
cat ./processed_data/${YEAR}/errors_bip.out | sort -k 4 -nr | head</p>

<h1 id="players-with-most-errors-per-ball-in-play----------">———PLAYERS WITH MOST ERRORS PER BALL IN PLAY———-</h1>
<p>#casts001, 13, 469, 0.0277186
#gurry001, 13, 474, 0.0274262
#castn001, 10, 469, 0.021322
#andum001, 10, 476, 0.0210084
#andeb006, 9, 461, 0.0195228
#turnt001, 9, 532, 0.0169173
#simma001, 8, 510, 0.0156863
#lemad001, 7, 451, 0.0155211
#sancc001, 7, 462, 0.0151515
#freef001, 7, 486, 0.0144033
{% endcodeblock %}</p>

<p>Now we have some data. In future posts I will explore how we can use statistics to evaluate whether some players are more likely to hit into errors than others.</p>

<p>Check out the <a href="https://danvatterott.com/blog/2019/06/04/are-some-mlb-players-more-likely-to-hit-into-errors-statistics/">companion post</a> that statistically explores this question.</p>
]]></content>
  </entry>
  
</feed>
