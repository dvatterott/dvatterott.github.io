<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Open Science | Dan Vatterott]]></title>
  <link href="https://danvatterott.com/blog/categories/open-science/atom.xml" rel="self"/>
  <link href="https://danvatterott.com/"/>
  <updated>2022-02-25T14:33:11-06:00</updated>
  <id>https://danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Exploring ROC Curves]]></title>
    <link href="https://danvatterott.com/blog/2018/03/17/exploring-roc-curves/"/>
    <updated>2018-03-17T14:06:15-05:00</updated>
    <id>https://danvatterott.com/blog/2018/03/17/exploring-roc-curves</id>
    <content type="html"><![CDATA[<p>I’ve always found ROC curves a little confusing. Particularly when it comes to ROC curves with imbalanced classes. This blog post is an exploration into receiver operating characteristic (i.e. <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py">ROC</a>) curves and how they react to imbalanced classes.</p>

<p>I start by loading the necessary libraries.</p>

<p>{% codeblock lang:python %}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
%matplotlib inline
{% endcodeblock %}</p>

<p>Seed the random number generator so that everything here is reproducible.</p>

<p>{% codeblock lang:python %}
np.random.seed(seed=1)
{% endcodeblock %}</p>

<p>I write a few functions that will create fake date, plot fake date, and plot ROC curves.</p>

<p>I describe each function in turn below:</p>
<ul style="padding-left: 25px;">
 <li><strong>grab_probability</strong> draws a sample of "probabilities" drawn from a uniform distribution bound between 0 and 1.</li>
 <li><strong>create_fake_binary_data</strong> creates a vector of 0s and 1s. The mean of the vector is controlled by the positive input.</li>
 <li><strong>probability_hist</strong> plots a normalized histogram (each bar depicts the proportion of data in it) bound between 0 and 1. </li>
 <li><strong>plot_roc_curve</strong> does not need an explanation.</li>
</ul>

<p>{% codeblock lang:python %}
def grab_probability(sample_size):
    “"”Draw probabilties”””
    return np.random.random(size=(sample_size,))</p>

<p>def create_fake_binary_data(positive, sample_size):
    “"”Create a vector of binary data with the mean specified in positive”””
    negative = 1-positive
    y = np.ones(sample_size)
    y[:int(negative*sample_size)] = 0
    np.random.shuffle(y)
    return y</p>

<p>def probability_hist(probs):
    “"”Create histogram of probabilities”””
    fig = plt.Figure()
    weights = np.ones_like(probs)/float(len(probs))
    plt.hist(probs, weights=weights)
    plt.xlim(0, 1)
    plt.ylim(0, 1);</p>

<p>def plot_roc_curve(fpr, tpr, roc_auc, lw=2):
    “"”Plot roc curve”””
    lw = lw
    fig = plt.Figure()
    plt.plot(fpr, tpr, color=’darkorange’,
             lw=lw, label=’ROC curve (area = %0.2f)’ % roc_auc)
    plt.plot([0, 1], [0, 1], color=’navy’, lw=2, linestyle=’–’)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel(‘False Positive Rate’)
    plt.ylabel(‘True Positive Rate’)
    plt.title(‘Receiver operating characteristic example’)
    plt.legend(loc=”lower right”);
{% endcodeblock %}</p>

<p>I have found one of the best ways to learn about an algorithm is to give it fake data. That way, I know the data, and can examine exactly what the algorithm does with the data. I then change the data and examine how the algorithm reacts to this change.</p>

<p>The first dataset I create is random data with balanced classes.</p>

<p>I create <em>probability</em> with the grab_probability function. This is a vector of numbers between 0 and 1. These data are meant to simulate the probabilities that would be produced by a model that is no better than chance.</p>

<p>I also create the vector <em>y</em> which is random ones and zeroes. I will call the ones the positive class and the zeroes the negative class.</p>

<p>The plot below is a histogram of <em>probability</em>. The y-axis is the proportion of samples in each bin. The x-axis is probability levels. You can see the probabilities appear to be from a uniform distribution.</p>

<p>{% codeblock lang:python %}
sample_size = 1000
positive = 0.5</p>

<p>y = create_fake_binary_data(positive, sample_size)
probability = grab_probability(sample_size)</p>

<p>probability_hist(probability)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_7_0.png" /></p>

<p>There’s no association between <em>y</em> and the <em>probability</em>, so I don’t expect the area under the curve to be different than chance (i.e., have an area under the curve of about 0.5). I plot the ROC curve to confirm this below.</p>

<p>{% codeblock lang:python %}
fpr, tpr, thresholds = roc_curve(y, probability)
roc_auc = auc(fpr, tpr)</p>

<p>plot_roc_curve(fpr, tpr, roc_auc)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_9_0.png" /></p>

<p>Let’s talk about the axes here. The y-axis is the proportion of true positives (i.e., <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">TPR</a> - True Positive Rate). This is how often the model correctly identifies members of the positive class. The x-axis is the proportion of false positives (FPR - False Positive Rate). This how often the model incorrectly assigns examples to the positive class.</p>

<p>One might wonder how the TPR and FPR can change. Doesn’t a model always produce the same guesses? The TPR and FPR can change because we can choose how liberal or conservative the model should be with assigning examples to the positive class. The lower left-hand corner of the plot above is when the model is maximally conservative (and assigns no examples to the positive class). The upper right-hand corner is when the model is maximally liberal and assigns every example to the positive class.</p>

<p>I used to assume that when a model is neutral in assigning examples to the positive class, that point would like halfway between the end points, but this is not the case. The threshold creates points along the curve, but doesn’t dictate where these points lie. If this is confusing, continue to think about it as we march through the proceeding plots.</p>

<p>The ROC curve is the balance between true and false positives as a threshold varies. To help visualize this balance, I create a function which plots the two classes as a stacked histogram, cumulative density functions, and the relative balance between the two classes.</p>

<p>{% codeblock lang:python %}
def probability_histogram_class(probability, y):
    plt.subplot(221)
    counts, bins, _ = plt.hist([probability[y==0], probability[y==1]], stacked=True)
    plt.xlim(np.min(bins),np.max(bins))
    plt.xticks([])</p>

<pre><code>plt.subplot(222)
plt.hist(probability[y==1], cumulative=True, normed=True, color='tab:orange')
plt.xlim(np.min(bins),np.max(bins))
plt.xticks([])
plt.ylim(0,1)

plt.subplot(224)
plt.hist(probability[y==0], cumulative=True, normed=True, color='tab:blue')
plt.xlim(np.min(bins),np.max(bins))
plt.xticks()
plt.ylim(0,1)

plt.subplot(223)
proportion = counts[0]/[max(0.0001, x) for x in counts[1]]
plt.plot(bins[:-1], 1-proportion)
plt.xlim(np.min(bins),np.max(bins))
plt.ylim(0,1); {% endcodeblock %}
</code></pre>

<p>The idea behind this plot is we can visualize the model’s threshold moving from LEFT to RIGHT through the plots. As the threshold decreases, the model will guess the positive class more often. This means more and more of each class will be included when calculating the numerator of TPR and FPR.</p>

<p>The top left plot is a stacked histogram. Orange depicts members of the positive class and blue depicts members of the negative class. On the x-axis (of all four plots) is probability.</p>

<p>If we continue thinking about the threshold as decreasing as the plots moves from left to right, we can think of the top right plot (a reversed <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">CDF</a> of the positive class) as depicting the proportion of the positive class assigned to the positive class as the threshold varies (setting the TPR). We can think of the bottom right plot (a reversed CDF of the negative class) as depicting the proportion of the negative class assigned to the positive class as the threshold varies (setting the FPR).</p>

<p>In the bottom left plot, I plot the proportion of positive class that falls in each bin from the histogram in the top plot.  Because the proportion of positive and negative class are equal as the threshold varies (as depicted in the bottom plot) we consistently assign both positive and negative examples to the positive class at equal rates and the ROC stays along the identity and the area under the curve is 0.5.</p>

<p>{% codeblock lang:python %}
probability = grab_probability(sample_size)</p>

<p>probability_histogram_class(probability, y)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_13_0.png" /></p>

<p>Next, I do the same process as above but with fake probabilities that are predictive of the label. The function biased_probability produces probabilities that tend to be greater for the positive class and lesser for the negative class.</p>

<p>{% codeblock lang:python %}
def biased_probability(y):
    “"”Return probabilities biased towards correct answer”””
    probability = np.random.random(size=(len(y),))
    probability[y==1] = np.random.random(size=(int(sum(y)),)) + 0.25
    probability[y==0] = np.random.random(size=(int(sum(y==0)),)) - 0.25
    probability = np.array([max(0, min(1, i)) for i in probability])
    return probability
{% endcodeblock %}</p>

<p>I create this data for a balanced class problem again. using the same y vector, I adjust the probabilities so that they are predcitive of the values in this y vector. Below, you can see the probability data as a histogram. The data no longer appear to be drawn from a uniform distribution. Instead, there are modes near 0 and 1.</p>

<p>{% codeblock lang:python %}
probability = biased_probability(y)</p>

<p>probability_hist(probability)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_17_0.png" /></p>

<p>Now, we get a nice roc curve which leaves the identity line. Not surprising since I designed the probabilities to be predictive. Notice how quickly the model acheives a TPR of 1. Remember this when looking at the plots below.</p>

<p>{% codeblock lang:python %}
fpr, tpr, _ = roc_curve(y, probability)
roc_auc = auc(fpr, tpr)</p>

<p>plot_roc_curve(fpr, tpr, roc_auc)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_19_0.png" /></p>

<p>In the upper left plot below, we can clearly see that the positive class occurs more often than the negative class on the right side of the plot.</p>

<p>Now remember that the lower left hand side of the roc plot is when we are most conservative. This corresponds to the right hand side of these plots where the model is confident that these examples are from the positive class.</p>

<p>If we look at the cdfs of right side. We can see the positive class (in orange) has many examples on the right side of these plots while the negative class (in blue) has no examples on this side. This is why the TPR immediately jumps to about 0.5 in the roc curve above. We also see the positive class has no examples on the left side of these plots while the negative class has many. This is why the TPR saturates at 1 well before the FPR does.</p>

<p>In other words, because there model is quite certain that some examples are from the positive class the ROC curve quickly jumps up on the y-axis. Because the model is quite certain as to which examples are from the negative class, the ROC curves saturates on the y-axis well before the end of the x-axis.</p>

<p>{% codeblock lang:python %}
probability = biased_probability(y)</p>

<p>probability_histogram_class(probability, y)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_21_0.png" /></p>

<p>After those two examples, I think we have a good handle on the ROC curve in the balanced class situation. Now let’s make some fake data when the classes are unbalanced. The probabilities will be completely random.</p>

<p>{% codeblock lang:python %}
sample_size = 1000
positive = 0.7</p>

<p>y = create_fake_binary_data(positive, sample_size)
probability = grab_probability(sample_size)</p>

<p>print(‘Average Test Value: %0.2f’ % np.mean(y))
print(‘Average Probability: %0.2f’ % np.mean(probability))</p>

<p>probability_hist(probability)
{% endcodeblock %}</p>

<pre><code>Average Test Value: 0.70
Average Probability: 0.49
</code></pre>

<p><img src="{{ root_url }}/images/roc_post/output_23_1.png" /></p>

<p>Again, this is fake data, so the probabilities do not reflect the fact that the classes are imbalanced.</p>

<p>Below, we can see that the ROC curve agrees that the data are completely random.</p>

<p>{% codeblock lang:python %}
fpr, tpr, _ = roc_curve(y, probability)
roc_auc = auc(fpr, tpr)</p>

<p>plot_roc_curve(fpr, tpr, roc_auc)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_25_0.png" /></p>

<p>{% codeblock lang:python %}
probability_histogram_class(probability, y)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_26_0.png" /></p>

<p>Now, lets create biased probabilities and see if the ROC curve differs from chance</p>

<p>{% codeblock lang:python %}
from sklearn.utils import shuffle</p>

<p>probability = biased_probability(y)</p>

<p>fpr, tpr, _ = roc_curve(y, probability)
roc_auc = auc(fpr, tpr)</p>

<p>plot_roc_curve(fpr, tpr, roc_auc)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_28_0.png" /></p>

<p>It does as we expect.</p>

<p>{% codeblock lang:python %}
probability_histogram_class(probability, y)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_30_0.png" /></p>

<p>Importantly, the probabilities now reflect the biased classes</p>

<p>{% codeblock lang:python %}
print(np.mean(probability))
{% endcodeblock %}</p>

<pre><code>0.602536255717
</code></pre>

<p>Using these same probabilities, lets remove the relationship between the probabilities and the output variable by shuffling the data.</p>

<p>{% codeblock lang:python %}
y = shuffle(y)
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
fpr, tpr, _ = roc_curve(y, probability)
roc_auc = auc(fpr, tpr)</p>

<p>plot_roc_curve(fpr, tpr, roc_auc)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_35_0.png" /></p>

<p>Beautiful! the ROC curve stays on the identity line. We can see that this is because while the positive class is predicted more often, the positive class is evently distributed across the different thresholds.</p>

<p>{% codeblock lang:python %}
probability_histogram_class(probability, y)
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/roc_post/output_37_0.png" /></p>

<p>Importantly, this demonstrates that even with imbalanced classes, if a model is at chance, then the ROC curve will reflect this chance perforomance. I do a similar demonstration with fake data <a href="https://github.com/dvatterott/jupyter_notebooks/blob/master/ROC_curves_realData.ipynb">here</a>.</p>

<p>{% codeblock lang:python %}
%load_ext watermark
%watermark -v -m -p numpy,matplotlib,sklearn
{% endcodeblock %}</p>

<pre><code>CPython 3.6.3
IPython 6.1.0

numpy 1.13.3
matplotlib 2.0.2
sklearn 0.19.1

compiler   : GCC 7.2.0
system     : Linux
release    : 4.13.0-36-generic
machine    : x86_64
processor  : x86_64
CPU cores  : 4
interpreter: 64bit
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Repeated Measures ANOVA in Python (Kinda)]]></title>
    <link href="https://danvatterott.com/blog/2016/02/28/repeated-measures-anova-in-python-kinda/"/>
    <updated>2016-02-28T20:52:03-06:00</updated>
    <id>https://danvatterott.com/blog/2016/02/28/repeated-measures-anova-in-python-kinda</id>
    <content type="html"><![CDATA[<p><em>If you’re just finding this post, please check out Erik Marsja’s <a href="https://www.marsja.se/repeated-measures-anova-in-python-using-statsmodels/">post</a> describing the same functionality in well-maintained python software that wasn’t available when I originally wrote this post.</em></p>

<p>I love doing data analyses with pandas, numpy, sci-py etc., but I often need to run <a href="https://en.wikipedia.org/wiki/Repeated_measures_design">repeated measures ANOVAs</a>, which are not implemented in any major python libraries. <a href="http://pythonpsychologist.tumblr.com/post/139246503057/repeated-measures-anova-using-python">Python Psychologist</a> shows how to do repeated measures ANOVAs yourself in python, but I find using a widley distributed implementation comforting…</p>

<p>In this post I show how to execute a repeated measures ANOVAs using the <a href="http://rpy2.bitbucket.org/">rpy2</a> library, which allows us to move data between python and R, and execute R commands from python. I use rpy2 to load the <a href="http://www.inside-r.org/packages/cran/car/docs/Anova">car</a> library and run the ANOVA.</p>

<p>I will show how to run a one-way repeated measures ANOVA and a two-way repeated measures ANOVA.</p>

<p>{% codeblock lang:python %}
#first import the libraries I always use.
import numpy as np, scipy.stats, pandas as pd</p>

<p>import matplotlib as mpl
import matplotlib.pyplot as plt
import pylab as pl
%matplotlib inline
pd.options.display.mpl_style = ‘default’
plt.style.use(‘ggplot’)
mpl.rcParams[‘font.family’] = [‘Bitstream Vera Sans’]</p>

<p>{% endcodeblock %}</p>

<p>Below I use the random library to generate some fake data. I seed the random number generator with a one so that this analysis can be replicated.</p>

<p>I will generated 3 conditions which represent 3 levels of a single variable.</p>

<p>The data are generated from a gaussian distribution. The second condition has a higher mean than the other two conditions.</p>

<p>{% codeblock lang:python %}
import random</p>

<p>random.seed(1) #seed random number generator
cond_1 = [random.gauss(600,30) for x in range(30)] #condition 1 has a mean of 600 and standard deviation of 30
cond_2 = [random.gauss(650,30) for x in range(30)] #u=650 and sd=30
cond_3 = [random.gauss(600,30) for x in range(30)] #u=600 and sd=30</p>

<p>plt.bar(np.arange(1,4),[np.mean(cond_1),np.mean(cond_2),np.mean(cond_3)],align=’center’) #plot data
plt.xticks([1,2,3]);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/rmANOVA_1.png" /></p>

<p>Next, I load rpy2 for ipython. I am doing these analyses with ipython in a <a href="http://jupyter.org/">jupyter notebook</a> (highly recommended).</p>

<p>{% codeblock lang:python %}
%load_ext rpy2.ipython
{% endcodeblock %}</p>

<p>Here’s how to run the ANOVA. Note that this is a one-way anova with 3 levels of the factor.</p>

<p>{% codeblock lang:python %}
#pop the data into R
%Rpush cond_1 cond_2 cond_3</p>

<h1 id="label-the-conditions">label the conditions</h1>
<p>%R Factor &lt;- c(‘Cond1’,’Cond2’,’Cond3’)
#create a vector of conditions
%R idata &lt;- data.frame(Factor)</p>

<h1 id="combine-data-into-single-matrix">combine data into single matrix</h1>
<p>%R Bind &lt;- cbind(cond_1,cond_2,cond_3)
#generate linear model
%R model &lt;- lm(Bind~1)</p>

<h1 id="load-the-car-library-note-this-library-must-be-installed">load the car library. note this library must be installed.</h1>
<p>%R library(car)
#run anova
%R analysis &lt;- Anova(model,idata=idata,idesign=~Factor,type=”III”)
#create anova summary table
%R anova_sum = summary(analysis)</p>

<h1 id="move-the-data-from-r-to-python">move the data from R to python</h1>
<p>%Rpull anova_sum
print anova_sum
{% endcodeblock %}</p>

<pre><code>Type III Repeated Measures MANOVA Tests:

------------------------------------------

Term: (Intercept)

 Response transformation matrix:
       (Intercept)
cond_1           1
cond_2           1
cond_3           1

Sum of squares and products for the hypothesis:
            (Intercept)
(Intercept)   102473990

Sum of squares and products for error:
            (Intercept)
(Intercept)     78712.7

Multivariate Tests: (Intercept)
                 Df test stat approx F num Df den Df     Pr(&gt;F)    
Pillai            1    0.9992 37754.33      1     29 &lt; 2.22e-16 ***
Wilks             1    0.0008 37754.33      1     29 &lt; 2.22e-16 ***
Hotelling-Lawley  1 1301.8736 37754.33      1     29 &lt; 2.22e-16 ***
Roy               1 1301.8736 37754.33      1     29 &lt; 2.22e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor

 Response transformation matrix:
       Factor1 Factor2
cond_1       1       0
cond_2       0       1
cond_3      -1      -1

Sum of squares and products for the hypothesis:
          Factor1   Factor2
Factor1  3679.584  19750.87
Factor2 19750.870 106016.58

Sum of squares and products for error:
         Factor1  Factor2
Factor1 40463.19 27139.59
Factor2 27139.59 51733.12

Multivariate Tests: Factor
                 Df test stat approx F num Df den Df    Pr(&gt;F)    
Pillai            1 0.7152596 35.16759      2     28 2.303e-08 ***
Wilks             1 0.2847404 35.16759      2     28 2.303e-08 ***
Hotelling-Lawley  1 2.5119704 35.16759      2     28 2.303e-08 ***
Roy               1 2.5119704 35.16759      2     28 2.303e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

                  SS num Df Error SS den Df         F    Pr(&gt;F)    
(Intercept) 34157997      1    26238     29 37754.334 &lt; 2.2e-16 ***
Factor         59964      2    43371     58    40.094 1.163e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Mauchly Tests for Sphericity

       Test statistic p-value
Factor        0.96168 0.57866


Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

        GG eps Pr(&gt;F[GG])    
Factor 0.96309  2.595e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

        HF eps   Pr(&gt;F[HF])
Factor 1.03025 1.163294e-11
</code></pre>

<p>The ANOVA table isn’t pretty, but it works. As you can see, the ANOVA was wildly significant.</p>

<p>Next, I generate data for a two-way (2x3) repeated measures ANOVA. Condition A is the same data as above. Condition B has a different pattern (2 is lower than 1 and 3), which should produce an interaction.</p>

<p>{% codeblock lang:python %}
random.seed(1)</p>

<p>cond_1a = [random.gauss(600,30) for x in range(30)] #u=600,sd=30
cond_2a = [random.gauss(650,30) for x in range(30)] #u=650,sd=30
cond_3a = [random.gauss(600,30) for x in range(30)] #u=600,sd=30</p>

<p>cond_1b = [random.gauss(600,30) for x in range(30)] #u=600,sd=30
cond_2b = [random.gauss(550,30) for x in range(30)] #u=550,sd=30
cond_3b = [random.gauss(650,30) for x in range(30)] #u=650,sd=30</p>

<p>width = 0.25
plt.bar(np.arange(1,4)-width,[np.mean(cond_1a),np.mean(cond_2a),np.mean(cond_3a)],width)
plt.bar(np.arange(1,4),[np.mean(cond_1b),np.mean(cond_2b),np.mean(cond_3b)],width,color=plt.rcParams[‘axes.color_cycle’][0])
plt.legend([‘A’,’B’],loc=4)
plt.xticks([1,2,3]);
{% endcodeblock %}</p>

<p><img src="{{ root_url }}/images/rmANOVA_2.png" /></p>

<p>{% codeblock lang:python %}
%Rpush cond_1a cond_1b cond_2a cond_2b cond_3a cond_3b</p>

<p>%R Factor1 &lt;- c(‘A’,’A’,’A’,’B’,’B’,’B’)
%R Factor2 &lt;- c(‘Cond1’,’Cond2’,’Cond3’,’Cond1’,’Cond2’,’Cond3’)
%R idata &lt;- data.frame(Factor1, Factor2)</p>

<h1 id="make-sure-the-vectors-appear-in-the-same-order-as-they-appear-in-the-dataframe">make sure the vectors appear in the same order as they appear in the dataframe</h1>
<p>%R Bind &lt;- cbind(cond_1a, cond_2a, cond_3a, cond_1b, cond_2b, cond_3b)
%R model &lt;- lm(Bind~1)</p>

<p>%R library(car)
%R analysis &lt;- Anova(model, idata=idata, idesign=~Factor1*Factor2, type=”III”)
%R anova_sum = summary(analysis)
%Rpull anova_sum</p>

<p>print anova_sum
{% endcodeblock %}</p>

<pre><code>Type III Repeated Measures MANOVA Tests:

------------------------------------------

Term: (Intercept)

 Response transformation matrix:
        (Intercept)
cond_1a           1
cond_2a           1
cond_3a           1
cond_1b           1
cond_2b           1
cond_3b           1

Sum of squares and products for the hypothesis:
            (Intercept)
(Intercept)   401981075

Sum of squares and products for error:
            (Intercept)
(Intercept)    185650.5

Multivariate Tests: (Intercept)
                 Df test stat approx F num Df den Df     Pr(&gt;F)    
Pillai            1    0.9995 62792.47      1     29 &lt; 2.22e-16 ***
Wilks             1    0.0005 62792.47      1     29 &lt; 2.22e-16 ***
Hotelling-Lawley  1 2165.2575 62792.47      1     29 &lt; 2.22e-16 ***
Roy               1 2165.2575 62792.47      1     29 &lt; 2.22e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor1

 Response transformation matrix:
        Factor11
cond_1a        1
cond_2a        1
cond_3a        1
cond_1b       -1
cond_2b       -1
cond_3b       -1

Sum of squares and products for the hypothesis:
         Factor11
Factor11 38581.51

Sum of squares and products for error:
         Factor11
Factor11 142762.3

Multivariate Tests: Factor1
                 Df test stat approx F num Df den Df    Pr(&gt;F)   
Pillai            1 0.2127533 7.837247      1     29 0.0090091 **
Wilks             1 0.7872467 7.837247      1     29 0.0090091 **
Hotelling-Lawley  1 0.2702499 7.837247      1     29 0.0090091 **
Roy               1 0.2702499 7.837247      1     29 0.0090091 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor2

 Response transformation matrix:
        Factor21 Factor22
cond_1a        1        0
cond_2a        0        1
cond_3a       -1       -1
cond_1b        1        0
cond_2b        0        1
cond_3b       -1       -1

Sum of squares and products for the hypothesis:
         Factor21 Factor22
Factor21 91480.01 77568.78
Factor22 77568.78 65773.02

Sum of squares and products for error:
         Factor21 Factor22
Factor21 90374.60 56539.06
Factor22 56539.06 87589.85

Multivariate Tests: Factor2
                 Df test stat approx F num Df den Df    Pr(&gt;F)    
Pillai            1 0.5235423 15.38351      2     28 3.107e-05 ***
Wilks             1 0.4764577 15.38351      2     28 3.107e-05 ***
Hotelling-Lawley  1 1.0988223 15.38351      2     28 3.107e-05 ***
Roy               1 1.0988223 15.38351      2     28 3.107e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor1:Factor2

 Response transformation matrix:
        Factor11:Factor21 Factor11:Factor22
cond_1a                 1                 0
cond_2a                 0                 1
cond_3a                -1                -1
cond_1b                -1                 0
cond_2b                 0                -1
cond_3b                 1                 1

Sum of squares and products for the hypothesis:
                  Factor11:Factor21 Factor11:Factor22
Factor11:Factor21          179585.9            384647
Factor11:Factor22          384647.0            823858

Sum of squares and products for error:
                  Factor11:Factor21 Factor11:Factor22
Factor11:Factor21          92445.33          45639.49
Factor11:Factor22          45639.49          89940.37

Multivariate Tests: Factor1:Factor2
                 Df test stat approx F num Df den Df     Pr(&gt;F)    
Pillai            1  0.901764 128.5145      2     28 7.7941e-15 ***
Wilks             1  0.098236 128.5145      2     28 7.7941e-15 ***
Hotelling-Lawley  1  9.179605 128.5145      2     28 7.7941e-15 ***
Roy               1  9.179605 128.5145      2     28 7.7941e-15 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

                      SS num Df Error SS den Df          F    Pr(&gt;F)    
(Intercept)     66996846      1    30942     29 62792.4662 &lt; 2.2e-16 ***
Factor1             6430      1    23794     29     7.8372  0.009009 **
Factor2            26561      2    40475     58    19.0310  4.42e-07 ***
Factor1:Factor2   206266      2    45582     58   131.2293 &lt; 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Mauchly Tests for Sphericity

                Test statistic p-value
Factor2                0.96023 0.56654
Factor1:Factor2        0.99975 0.99648


Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

                 GG eps Pr(&gt;F[GG])    
Factor2         0.96175  6.876e-07 ***
Factor1:Factor2 0.99975  &lt; 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

                  HF eps   Pr(&gt;F[HF])
Factor2         1.028657 4.420005e-07
Factor1:Factor2 1.073774 2.965002e-22
</code></pre>

<p>Again, the anova table isn’t too pretty.</p>

<p>This obviously isn’t the most exciting post in the world, but its a nice bit of code to have in your back pocket if you’re doing experimental analyses in python.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Complete Amnesia for Object Attributes?]]></title>
    <link href="https://danvatterott.com/blog/2015/12/12/complete-amnesia-for-object-attributes/"/>
    <updated>2015-12-12T12:10:09-06:00</updated>
    <id>https://danvatterott.com/blog/2015/12/12/complete-amnesia-for-object-attributes</id>
    <content type="html"><![CDATA[<h3 id="reanalysis-of-chen--wyble-2015">Reanalysis of Chen &amp; Wyble, 2015</h3>

<p>Chen and Wyble published an interesting paper (2015) where they demonstrate that participants cannot report attributes of attended stimuli unless the participants are previously informed that this attribute is important. For instance, you wouldn’t remember the color of the apple if you had had just told someone the shape. I would have expected the opposite, so … cool!</p>

<p>After reading the paper (you can check it out at http://wyblelab.com/publications), I became curious whether participants might unconsciously retain some information about these forgotten attributes. Chen and Wyble posted their data to databrary.com (https://nyu.databrary.org/volume/79), so I downloaded the data and did some quick analyses that you see here! I want to commend Chen and Wyble for sharing their data. This is something everyone should start doing (including me).</p>

<p>Below, I will start by showing I can replicate Chen and Wyble’s analyses, then I will investigate whether there’s a trace of unconscious memory for the “forgotten” features.</p>

<p>EDIT -12/22/15-
Brad Wyble recently pointed out that I overstated the claim in their paper. They do not claim participants have complete amnesia for unqueried object attributes. Rather, Chen and Wyble focus on the dramatic performance change between the first and second trial following the initial query about an object attribute. This performance change demonstrates amnesia, but not necessarily complete amnesia.</p>

<h5 id="references">References</h5>
<p>Chen, H., &amp; Wyble, B. (2015). Amnesia for Object Attributes Failure to Report Attended Information That Had Just Reached Conscious Awareness. Psychological science, 26(2),203-210.</p>

<p>Wyble, B. (2014). Amnesia for object attributes: Failure to report attended information that had just reached conscious awareness. Databrary. Retrieved November 22, 2015 from http://doi.org/10.17910/B7G010</p>

<h4 id="load-relevant-libraries-and-write-analysis-functions">Load relevant libraries and write analysis functions</h4>

<p>I’ll start by loading the python libraries that I’ll use throughout analyses.</p>

<p>{% codeblock lang:python %}
import numpy as np, sys, scipy.stats, pandas as pd, os, os.path, csv #loading useful libraries</p>

<p>import matplotlib as mpl
import matplotlib.pyplot as plt
import pylab as pl
%matplotlib inline
pd.options.display.mpl_style = ‘default’ #load matplotlib for plotting
plt.style.use(‘ggplot’) #im addicted to ggplot. so pretty.
mpl.rcParams[‘font.family’] = [‘Bitstream Vera Sans’]
{% endcodeblock %}</p>

<p>Here are some quick functions I wrote for running different statistical tests and plotting the data. I won’t explain this code, but encourage you to look through it later if you’re wondering how I did any of the analyses.</p>

<p>{% codeblock lang:python %}
def print_tests(series1, series2): #this function just presents normality and t-tests.
    import scipy, numpy as np #the function does these tests with arcsin(sqrt(acc)) to help with normality
    normTest2 = scipy.stats.shapiro(np.arcsin(np.sqrt(series1))-np.arcsin(np.sqrt(series2)))
    Test2 = scipy.stats.ttest_rel(np.arcsin(np.sqrt(series1)), np.arcsin(np.sqrt(series2)))
    Test3 = scipy.stats.wilcoxon(np.arcsin(np.sqrt(series1)), y=np.arcsin(np.sqrt(series2)))
    print ‘\t normality test adj. Test value: %s P-value: %s’ % (str(np.round(normTest2[0],2)),
                                                                 str(np.round(normTest2[1],4)))
    if normTest2[1] &gt; 0.1: print ‘\t T-test adj. Test value: %s P-value: %s’ % (str(np.round(Test2[0],2)),
                                                                                str(np.round(Test2[1],4)))
    if normTest2[1] &lt;= 0.1: print ‘\t Wilcoxon. Test value: %s P-value: %s’ % (str(np.round(Test3[0],2)),
                                                                               str(np.round(Test3[1],2)))</p>

<p>def print_tests_ChiSq(series): #this function just presents normality and t-tests.
    import scipy, numpy as np #the function does these tests with arcsin(sqrt(acc)) to help with normality
    Test1 = scipy.stats.chisquare(series[1], f_exp = 0.25)
    Test2 = scipy.stats.chisquare(series[2], f_exp = 0.25)
    print ‘\t Surprise Test. Comparison to Chance: %s P-value: %s’ % (str(np.round(Test1[0],4)),
                                                                      str(np.round(Test1[1],4)))
    print ‘\t After Surprise Test. Comparison to Chance: %s P-value: %s’ % (str(np.round(Test2[0],4)),
                                                                            str(np.round(Test2[1],4)))
    x = scipy.stats.chi2_contingency([[sum(series[1]==1),sum(series[2]==1)], [sum(series[1]==0),sum(series[2]==0)]],
                                     correction=False)
    print ‘\t Chi-Square Comparison: %s P-value: %s’ % (str(np.round(x[0],4)),str(np.round(x[1],4)))</p>

<p>def Analysis_and_Plot(tableRT2, CIs): #this function plots the data and writes the results, including stats tests
    PlotFrame = pd.DataFrame([tableRT2.mean()])
    PlotFrame.columns = [‘Presurprise’, ‘Surprise’, ‘Post surprise’]
    PlotFrame2 = pd.DataFrame([CIs])
    PlotFrame2.columns = [‘Presurprise’, ‘Surprise’, ‘Post surprise’]
    PlotFrame.plot(ylim = [0, 1], yerr=PlotFrame2, kind=’bar’)
    plt.xticks(range(1), [‘Trial Type’], rotation=0);</p>

<pre><code>print '---------------------------------'
print 'Mean Presurprise: %s' % (str(round(np.mean(tableRT2[0]),2)))
print 'Mean Surprise: %s' % (str(round(np.mean(tableRT2[1]),2)))
print 'Mean Postsurprise: %s' % (str(round(np.mean(tableRT2[2]),2)))
print 'Presurprise - Surprise: %s' % (str(round(np.mean(tableRT2[0])-np.mean(tableRT2[1]),2)))
print 'Postsurprise - Surprise: %s' % (str(round(np.mean(tableRT2[2])-np.mean(tableRT2[1]),2)))
print 'Postsurprise - Presurprise: %s' % (str(round(np.mean(tableRT2[0])-np.mean(tableRT2[1]),2)))
print '---------------------------------'
print 'Presurprise vs Surprise'
print_tests(tableRT2[1],tableRT2[0])
print 'Postsuprise vs Surprise'
print_tests(tableRT2[2],tableRT2[1])
print 'Presurprise vs Postsurprise'
print_tests(tableRT2[0],tableRT2[2])
</code></pre>

<p>def Analysis_and_Plot_2(tableRT2, CIs): #this function also plots the data and prints results.
    PlotFrame = pd.DataFrame([tableRT2.mean()]) #I could probably consolidate these functions, but whatever. This works.
    PlotFrame.columns = [‘Surprise’, ‘Postsurprise’]
    PlotFrame.plot(ylim = [0, 1], kind=’bar’)#yerr=PlotFrame2, kind=’bar’)
    plt.xticks(range(1), [‘Trial Type’], rotation=0);</p>

<pre><code>print '---------------------------------'
print 'Mean Surprise: %s' % (str(round(np.mean(tableRT2[1]),2)))
print 'Mean Postsurprise: %s' % (str(round(np.mean(tableRT2[2]),2)))
print 'Postsurprise - Surprise: %s' % (str(round(np.mean(tableRT2[2])-np.mean(tableRT2[1]),2)))
print '---------------------------------'
print 'Postsurprise vs Surprise'
print_tests_ChiSq(tableRT2)
</code></pre>

<p>def Analysis_and_Plot_3(tableRT2, CIs): #another plotting function…i should really consolidate these.
    PlotFrame = pd.DataFrame([tableRT2.mean()])
    PlotFrame.columns = [‘Match’, ‘Mismatch’]
    PlotFrame2 = pd.DataFrame()
    PlotFrame2[‘Match’] = pd.DataFrame([CIs])
    PlotFrame2[‘Mismatch’] = pd.DataFrame([CIs])
    PlotFrame.plot(ylim = [0, 1], yerr=PlotFrame2, kind=’bar’)
    plt.xticks(range(1), [‘Trial Type’], rotation=0);</p>

<pre><code>#disp_tab = np.round(tableRT2,2)
#disp_tab['Match-Mismatch'] = disp_tab[1] - disp_tab[2]
#print disp_tab

print '---------------------------------'
print 'Mean match: %s' % (str(round(np.mean(tableRT2[1]),4)))
print 'Mean mismatch: %s' % (str(round(np.mean(tableRT2[2]),4)))
print 'Match - Mismatch: %s' % (str(round(np.mean(tableRT2[1])-np.mean(tableRT2[2]),4)))
print '---------------------------------'
print 'Match vs Mismatch'
print_tests(tableRT2[1],tableRT2[2])
</code></pre>

<p>def OneWayConfInterval(table): #Calculates confidence intervals for a one way anova, this is Cousineau, Morey ect
    import scipy.stats, numpy
    ParticipantsMeans = []
    STEs = []
    CIs = []
    for participant in table.index:
        mean = []
        for condition in xrange(numpy.shape(table)[1]): #there’s definitely a better way to do this, but this works…
            mean.append(np.array(table[table.index==participant][condition]))
        ParticipantsMeans.append(sum(mean)/len(mean))
    ConfMeans = numpy.zeros(shape=numpy.shape(table))
    for counter, participant in enumerate(table.index):
        for condition in xrange(numpy.shape(table)[1]):
            ConfMeans[counter][condition] = table[table.index==participant][condition]-\
            ParticipantsMeans[counter]+numpy.array(ParticipantsMeans).mean()
    for counter, column in enumerate(ConfMeans.T):
        STEs.append(numpy.std(column, ddof=1)/numpy.sqrt(len(column)))
        CIs.append(STEs[counter]*scipy.stats.t.isf(0.025, len(ConfMeans)-1))
    return CIs</p>

<p>def SimpleComparisonCI(table): #confidence interval for pairwise comparisons - masson &amp; loftus, Baguley (2012)
    import scipy.stats, math
    ttest = scipy.stats.ttest_rel(table[1], table[2])
    MeanDiff_byT = abs((table[1].mean()-table[2].mean())/ttest[0])
    CI = MeanDiff_byT<em>scipy.stats.t.isf(0.025, len(table)-1)</em>math.pow(2,0.05)/2
    return CI
{% endcodeblock %}</p>

<h4 id="experiment-1">Experiment 1</h4>

<p>Next, load Experiment 1 data</p>

<p>{% codeblock lang:python %}
filename = ‘Exp1.csv’ #looking at Exp1 data first.</p>

<p>if sys.platform == ‘linux2’: #is this my linux laptop
    path = ‘/home/dan-laptop/Documents/Databrary/Wyble_PsychSci’
elif sys.platform == ‘darwin’: #is this my mac work comp  <br />
    path = ‘/Users/danvatterott/Dropbox Encore/Dropbox/Databrary/Wyble_PsychSci’</p>

<p>os.chdir(path)</p>

<p>df = pd.read_csv(filename)</p>

<p>df.columns = [‘Sub#’, ‘Block’, ‘Trial#’, ‘TarCol’, ‘Tar_Iden’,’Tar_Loc’, ‘Col_Resp’, ‘Iden_Resp’, ‘Loc_Resp’,
             ‘Col_Acc’, ‘Iden_Acc’, ‘Loc_Acc’] #naming the columns of the data file.
{% endcodeblock %}</p>

<p>The data is loaded, lets just take a quick look at the data after loading it in.</p>

<p>{% codeblock lang:python %}
df[0:5]
{% endcodeblock %}</p>

<table>
  <thead>
    <tr>
      <th></th>
      <th>Sub#</th>
      <th>Block</th>
      <th>Trial#</th>
      <th>TarCol</th>
      <th>Tar_Iden</th>
      <th>Tar_Loc</th>
      <th>Col_Resp</th>
      <th>Iden_Resp</th>
      <th>Loc_Resp</th>
      <th>Col_Acc</th>
      <th>Iden_Acc</th>
      <th>Loc_Acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>1</td>
      <td>5</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6</td>
      <td>1</td>
      <td>6</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>I want to create a new variable.</p>

<p>Before explaining the new variable, I should explain a little about Chen and Wyble’s experiment. Half the participants were instructed to find the letter among numbers and the other half were instructed to find the number among letters. 4 items were briefly flashed on the screen (150 ms) then participants reported where the target item had been. Each of the 4 items was a different color.</p>

<p>Participants reported target location for 155 trials. On the 156th trial, the participants reported the target location then (to their surprise) reported what specific letter/number the target was and what color it was. Even though participants knew where the target was, they had no idea what either the target’s letter/number or color were. They had “forgotten” what the target was (even though they must have known what the target was when they initially located it).</p>

<p>The new variable will code whether the trial is a “pre-surprise” trial (trials 1-155), a surprise trial (trial 156) or the trial after the surprise (trial 157).</p>

<p>I’ll call this variable “TrialType”</p>

<p>TrialType: 0=Presurprise; 1=Surprise Trial; 2=Postsurprise trials.</p>

<p>{% codeblock lang:python %}
df[‘TrialType’] = np.where(df[‘Trial#’]&lt;156, 0, np.where(df[‘Trial#’]==156, 1, 2))
df[‘TrialType2’] = np.where(df[‘Trial#’]&lt;156, 0, np.where(df[‘Trial#’]==156, 1, np.where(df[‘Trial#’]==157, 2, -1)))
{% endcodeblock %}</p>

<h4 id="experiment-1-replicating-chen--wybles-2015-analyses">Experiment 1: Replicating Chen &amp; Wyble’s (2015) analyses</h4>

<p>Lets just take a quick look at overall accuracy. Make sure everyone is doing ok on the task. Below I plot the mean accuracy of each participant…looks like participant 23 struggled a little. Chen and Wyble (2015) notes that no participants were excluded or replaced.</p>

<p>I might have replaced participant 23 since his/her accuracy is easily 2.5 standard deviations below the mean accuracy (I print this value below)…seems like participant 23 was doing something different in this task.</p>

<p>{% codeblock lang:python %}
tableAcc = df.pivot_table(values=’Loc_Acc’, index=’Sub#’, aggfunc=np.mean)
#print tableAcc
print ‘mean accuracy’
print np.round(np.mean(tableAcc),2)
print ‘standard deviation of accuracies’
print np.round(np.std(tableAcc),2)</p>

<p>print ‘2.5 standard deviations below mean accuracy’
print np.round(np.mean(tableAcc)-2.5*np.std(tableAcc),2)</p>

<p>tableAcc.plot(ylim = [0, 1], kind=’bar’);
{% endcodeblock %}</p>

<pre><code>mean accuracy
0.89
standard deviation of accuracies
0.07
2.5 standard deviations below mean accuracy
0.71
</code></pre>

<p><img src="{{ root_url }}/images/Wyble1.png" /></p>

<p>Lets look at participants’ performance when asked to identify the target’s location. I will plot performance as mean accuracy in the presurprise,surprise, and postsurprose trials.</p>

<p>I will also run some quick statistical tests. For these tests, I take the arcsine of the square root of the accuracies (Rao, 1960) to increase the accuracies’ normality (I use adj. to indiciate that the tested data is transformed). I test whether this worked with a Shapiro-Wilk test of normality. If the p-value of the Shapiro-Wilk test is greater than 0.1, I run a t test to see if the accuracy in the two conditions is significantly different. If the p-value of the Shapiro-Wilk test is less than or equal to 0.1, then I run a Wilcoxon signed rank test since this test does not care about normality.</p>

<p>{% codeblock lang:python %}
Loc_Acc = df.pivot_table(values=’Loc_Acc’, index=’Sub#’, columns=’TrialType’, aggfunc=np.mean)
CIs = np.array(OneWayConfInterval(Loc_Acc))
Analysis_and_Plot(Loc_Acc, CIs)
{% endcodeblock %}</p>

<pre><code>---------------------------------
Mean Presurprise: 0.89
Mean Surprise: 0.8
Mean Postsurprise: 0.79
Presurprise - Surprise: 0.09
Postsurprise - Surprise: -0.01
Postsurprise - Presurprise: 0.09
---------------------------------
Presurprise vs Surprise
	 normality test adj. Test value: 0.64 P-value: 0.0
	 Wilcoxon. Test value: 74.0 P-value: 0.25
Postsuprise vs Surprise
	 normality test adj. Test value: 0.8 P-value: 0.001
	 Wilcoxon. Test value: 33.0 P-value: 0.63
Presurprise vs Postsurprise
	 normality test adj. Test value: 0.94 P-value: 0.2857
	 T-test adj. Test value: 0.92 P-value: 0.3695
</code></pre>

<p><img src="{{ root_url }}/images/Wyble2.png" /></p>

<p>The y-axis represents percent correct. All graphs in this post will have percent correct on the y-axis.</p>

<p>Replicating Chen and Wyble, participants perform no worse in the surprise and post surprise trials, indicating that they succesfully found the target.</p>

<p>Now lets look at participants’ ability to report the target’s color in the surprise trial and the trial immediately following the surprise test.</p>

<p>Below I plot the percent of participants that correctly identified the target’s color in the surprise and post-surprise trials</p>

<p>{% codeblock lang:python %}
Trial_Trimmer = df[‘TrialType2’] &gt; 0
Col_Acc = df[Trial_Trimmer].pivot_table(values=’Col_Acc’, index=’Sub#’, columns=’TrialType2’, aggfunc=np.mean)
CIs = SimpleComparisonCI(Col_Acc)
Analysis_and_Plot_2(Col_Acc, CIs)
{% endcodeblock %}</p>

<pre><code>---------------------------------
Mean Surprise: 0.3
Mean Postsurprise: 0.7
Postsurprise - Surprise: 0.4
---------------------------------
Postsurprise vs Surprise
	 Surprise Test. Comparison to Chance: 17.0 P-value: 0.5899
	 After Surprise Test. Comparison to Chance: 33.0 P-value: 0.024
	 Chi-Square Comparison: 6.4 P-value: 0.0114
</code></pre>

<p><img src="{{ root_url }}/images/Wyble3.png" /></p>

<p>We perfectly replicate Chen and Wyble; participants respond more accurarely in the post-surprise trial than in the surprise trial.</p>

<p>The next cell examines participants’ ability to report the target’s identity on the surprise trial and the trial immediately following the surprise trial. Remember, the participants locate the target based on whether its a letter or number, so they know the broad category of the target. Nonetheless, they cannot report the target’s identity on the surprise trial</p>

<p>{% codeblock lang:python %}
Trial_Trimmer = df[‘TrialType2’] &gt; 0
Iden_Acc = df[Trial_Trimmer].pivot_table(values=’Iden_Acc’, index=’Sub#’, columns=’TrialType2’, aggfunc=np.mean)
CIs = SimpleComparisonCI(Iden_Acc)
Analysis_and_Plot_2(Iden_Acc, CIs)
{% endcodeblock %}</p>

<pre><code>---------------------------------
Mean Surprise: 0.25
Mean Postsurprise: 0.75
Postsurprise - Surprise: 0.5
---------------------------------
Postsurprise vs Surprise
	 Surprise Test. Comparison to Chance: 15.0 P-value: 0.7226
	 After Surprise Test. Comparison to Chance: 35.0 P-value: 0.014
	 Chi-Square Comparison: 10.0 P-value: 0.0016
</code></pre>

<p><img src="{{ root_url }}/images/Wyble4.png" /></p>

<h4 id="experiment-1---intertrial-analyses">Experiment 1 - Intertrial analyses</h4>

<p>So far, I’ve perfectly replicated Chen &amp; Wyble (which is good since this is their data).</p>

<p>Now I want to see if the target’s color or identity on the previous trial influences the current trial’s performance in the location task. I am only examining presurprise trials, so this should be trials when the participants don’t “remember” the target’s color or identity.</p>

<p>First I want to make some variables representing whether the target’s color and identity repeat across trials.</p>

<p>{% codeblock lang:python %}
df[‘Prev_TarCol’] = df[‘TarCol’].shift(periods=1)
df[‘Prev_TarCol_match’] = np.where(df[‘Prev_TarCol’]==df[‘TarCol’], 1, 2)
df[‘Prev_Iden’] = df[‘Tar_Iden’].shift(periods=1)
df[‘Prev_Iden_match’] = np.where(df[‘Prev_Iden’]==df[‘Tar_Iden’], 1, 2)
df[‘Prev_Col+Iden_match’] = np.where((df[‘Prev_Iden_match’]==1) &amp; (df[‘Prev_TarCol_match’]==1), 1, 2)
{% endcodeblock %}</p>

<p>Lets see what happens when the target’s color and identity repeat.</p>

<p>{% codeblock lang:python %}
Trial_Trimmer = df[‘TrialType’] == 0
ColandIden_Acc1 = df[Trial_Trimmer].pivot_table(values=’Loc_Acc’, index=’Sub#’, columns=’Prev_Col+Iden_match’,
                                                aggfunc=np.mean)
CIs = SimpleComparisonCI(ColandIden_Acc1)
Analysis_and_Plot_3(ColandIden_Acc1, CIs)
{% endcodeblock %}</p>

<pre><code>---------------------------------
Mean match: 0.918
Mean mismatch: 0.8925
Match - Mismatch: 0.0255
---------------------------------
Match vs Mismatch
	 normality test adj. Test value: 0.92 P-value: 0.0821
	 Wilcoxon. Test value: 51.0 P-value: 0.04
</code></pre>

<p><img src="{{ root_url }}/images/Wyble5.png" /></p>

<p>Looks like a 2.5% increase in accuracy. Now, this wasn’t really a planned comparison, so please take this result with a grain of salt.</p>

<p>As a sanity check, lets look at how repetitions in the target’s location (the reported feature) effect performance.</p>

<p>We have to quickly create a new variable coding target location repetitions</p>

<p>{% codeblock lang:python %}
df[‘Prev_Loc’] = df[‘Tar_Loc’].shift(periods=1)
df[‘Prev_Loc_match’] = np.where(df[‘Prev_Loc’]==df[‘Tar_Loc’], 1, 2)
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
Trial_Trimmer = df[‘TrialType’] == 0
Loc_Acc1 = df[Trial_Trimmer].pivot_table(values=’Loc_Acc’, index=’Sub#’, columns=’Prev_Loc_match’, aggfunc=np.mean)
CIs = SimpleComparisonCI(Loc_Acc1)
Analysis_and_Plot_3(Loc_Acc1, CIs)
{% endcodeblock %}</p>

<pre><code>---------------------------------
Mean match: 0.9101
Mean mismatch: 0.8883
Match - Mismatch: 0.0218
---------------------------------
Match vs Mismatch
	 normality test adj. Test value: 0.93 P-value: 0.1812
	 T-test adj. Test value: 2.62 P-value: 0.0168
</code></pre>

<p><img src="{{ root_url }}/images/Wyble6.png" /></p>

<p>Target location repetitions lead to a 2% increase in performance. Again, this result is robust.</p>

<p>It’s a good sign that this effect is about the same size as repetitions in the unreported features.</p>

<h3 id="replicate-experiments-1-intertrial-analyses-with-experiment-1b">Replicate Experiments 1 Intertrial Analyses with Experiment 1b</h3>

<p>Experiment 1 had some evidence that participants unconsciously knew the color and identity of the target, since they performed a little better when the color and identity repeated. The effect was small, so I am not 100% confident that it’s robust.</p>

<p>The best way to demonstrate that this effect is real would be to show that it also exists in another similar Experiment. Chen and Wyble provide a replication of Experiment 1. In this experiment, the only difference is the target and distractors appear for longer and are not masked (making them easier to see).</p>

<p>If participants response more accurately when the target color and identity repeat in Experiment 1b, then we can be a little more confident that participants are unconsciously aware of the target’s color and identity.</p>

<p>{% codeblock lang:python %}
filename = ‘Exp1b.csv’
df = pd.read_csv(filename)
df.columns = [‘Sub#’, ‘Block’, ‘Trial#’, ‘TarCol’, ‘Tar_Iden’,’Tar_Loc’, ‘Col_Resp’, ‘Iden_Resp’, ‘Loc_Resp’,
             ‘Col_Acc’, ‘Iden_Acc’, ‘Loc_Acc’] #naming the columns of the data file.</p>

<p>df[‘TrialType’] = np.where(df[‘Trial#’]&lt;156, 0, np.where(df[‘Trial#’]==156, 1, 2))
df[‘TrialType2’] = np.where(df[‘Trial#’]&lt;156, 0, np.where(df[‘Trial#’]==156, 1, np.where(df[‘Trial#’]==157, 2, -1)))</p>

<p>df[‘Prev_TarCol’] = df[‘TarCol’].shift(periods=1)
df[‘Prev_TarCol_match’] = np.where(df[‘Prev_TarCol’]==df[‘TarCol’], 1, 2)
df[‘Prev_Iden’] = df[‘Tar_Iden’].shift(periods=1)
df[‘Prev_Iden_match’] = np.where(df[‘Prev_Iden’]==df[‘Tar_Iden’], 1, 2)
df[‘Prev_Col+Iden_match’] = np.where((df[‘Prev_Iden_match’]==1) &amp; (df[‘Prev_TarCol_match’]==1), 1, 2)</p>

<p>Trial_Trimmer = df[‘TrialType’] == 0 #only interested in pre-surprise trials
ColandIden_Acc = df[Trial_Trimmer].pivot_table(values=’Loc_Acc’, index=’Sub#’,
                                               columns=’Prev_Col+Iden_match’, aggfunc=np.mean)
{% endcodeblock %}</p>

<p>{% codeblock lang:python %}
CIs = SimpleComparisonCI(ColandIden_Acc)
Analysis_and_Plot_3(ColandIden_Acc, CIs)
{% endcodeblock %}</p>

<pre><code>---------------------------------
Mean match: 0.9716
Mean mismatch: 0.9644
Match - Mismatch: 0.0072
---------------------------------
Match vs Mismatch
	 normality test adj. Test value: 0.93 P-value: 0.1875
	 T-test adj. Test value: 2.81 P-value: 0.0112
</code></pre>

<p><img src="{{ root_url }}/images/Wyble7.png" /></p>

<p>Wow. Only a 1% change in accuracy, so again not big. Nonetheless, this result is signficant. So, Some evidence that participants perform a little better when the targets’ color and identity repeat.</p>

<p>This suggests that participants retain some information about the targets’ color and identity even though they cannot explicitly report these attributes.</p>

<p>Now, I would probably want to replicate this result again before trusting it, but I’m relatively confident that participants unconsciously retain some information about the target’s color and identity.</p>
]]></content>
  </entry>
  
</feed>
