<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Open Science | Dan Vatterott]]></title>
  <link href="http://www.danvatterott.com/blog/categories/open-science/atom.xml" rel="self"/>
  <link href="http://www.danvatterott.com/"/>
  <updated>2016-02-28T21:59:02-05:00</updated>
  <id>http://www.danvatterott.com/</id>
  <author>
    <name><![CDATA[Dan Vatterott]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Repeated Measures ANOVA in Python (Kinda)]]></title>
    <link href="http://www.danvatterott.com/blog/2016/02/28/repeated-measures-anova-in-python-kinda/"/>
    <updated>2016-02-28T21:52:03-05:00</updated>
    <id>http://www.danvatterott.com/blog/2016/02/28/repeated-measures-anova-in-python-kinda</id>
    <content type="html"><![CDATA[<p>I love doing data analyses with pandas, numpy, sci-py etc., but I often need to run <a href="https://en.wikipedia.org/wiki/Repeated_measures_design">repeated measures ANOVAs</a>, which are not implemented in any major python libraries. <a href="http://pythonpsychologist.tumblr.com/post/139246503057/repeated-measures-anova-using-python">Python Psychologist</a> shows how to do repeated measures ANOVAs yourself in python, but I find using a widley distributed implementation comforting&hellip;</p>

<p>In this post I show how to execute a repeated measures ANOVAs using the <a href="http://rpy2.bitbucket.org/">rpy2</a> library, which allows us to move data between python and R, and execute R commands from python. I use rpy2 to load the <a href="http://www.inside-r.org/packages/cran/car/docs/Anova">car</a> library and run the ANOVA.</p>

<p>I will show how to run a one-way repeated measures ANOVA and a two-way repeated measures ANOVA.</p>

<pre><code class="python">#first import the libraries I always use. 
import numpy as np, scipy.stats, pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt
import pylab as pl
%matplotlib inline
pd.options.display.mpl_style = 'default'
plt.style.use('ggplot')
mpl.rcParams['font.family'] = ['Bitstream Vera Sans']
</code></pre>

<p>Below I use the random library to generate some fake data. I seed the random number generator with a one so that this analysis can be replicated.</p>

<p>I will generated 3 conditions which represent 3 levels of a single variable.</p>

<p>The data are generated from a gaussian distribution. The second condition has a higher mean than the other two conditions.</p>

<pre><code class="python">import random

random.seed(1) #seed random number generator
cond_1 = [random.gauss(600,30) for x in range(30)] #condition 1 has a mean of 600 and standard deviation of 30
cond_2 = [random.gauss(650,30) for x in range(30)] #u=650 and sd=30
cond_3 = [random.gauss(600,30) for x in range(30)] #u=600 and sd=30

plt.bar(np.arange(1,4),[np.mean(cond_1),np.mean(cond_2),np.mean(cond_3)],align='center') #plot data
plt.xticks([1,2,3]);
</code></pre>

<p><img src="/images/rmANOVA_1.png" /></p>

<p>Next, I load rpy2 for ipython. I am doing these analyses with ipython in a <a href="http://jupyter.org/">jupyter notebook</a> (highly recommended).</p>

<pre><code class="python">%load_ext rpy2.ipython
</code></pre>

<p>Here&rsquo;s how to run the ANOVA. Note that this is a one-way anova with 3 levels of the factor.</p>

<pre><code class="python">#pop the data into R
%Rpush cond_1 cond_2 cond_3 

#label the conditions
%R Factor &lt;- c('Cond1','Cond2','Cond3')
#create a vector of conditions
%R idata &lt;- data.frame(Factor) 

#combine data into single matrix
%R Bind &lt;- cbind(cond_1,cond_2,cond_3) 
#generate linear model
%R model &lt;- lm(Bind~1)

#load the car library. note this library must be installed.
%R library(car) 
#run anova
%R analysis &lt;- Anova(model,idata=idata,idesign=~Factor,type="III") 
#create anova summary table
%R anova_sum = summary(analysis) 

#move the data from R to python
%Rpull anova_sum 
print anova_sum
</code></pre>

<pre><code>Type III Repeated Measures MANOVA Tests:

------------------------------------------

Term: (Intercept) 

 Response transformation matrix:
       (Intercept)
cond_1           1
cond_2           1
cond_3           1

Sum of squares and products for the hypothesis:
            (Intercept)
(Intercept)   102473990

Sum of squares and products for error:
            (Intercept)
(Intercept)     78712.7

Multivariate Tests: (Intercept)
                 Df test stat approx F num Df den Df     Pr(&gt;F)    
Pillai            1    0.9992 37754.33      1     29 &lt; 2.22e-16 ***
Wilks             1    0.0008 37754.33      1     29 &lt; 2.22e-16 ***
Hotelling-Lawley  1 1301.8736 37754.33      1     29 &lt; 2.22e-16 ***
Roy               1 1301.8736 37754.33      1     29 &lt; 2.22e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor 

 Response transformation matrix:
       Factor1 Factor2
cond_1       1       0
cond_2       0       1
cond_3      -1      -1

Sum of squares and products for the hypothesis:
          Factor1   Factor2
Factor1  3679.584  19750.87
Factor2 19750.870 106016.58

Sum of squares and products for error:
         Factor1  Factor2
Factor1 40463.19 27139.59
Factor2 27139.59 51733.12

Multivariate Tests: Factor
                 Df test stat approx F num Df den Df    Pr(&gt;F)    
Pillai            1 0.7152596 35.16759      2     28 2.303e-08 ***
Wilks             1 0.2847404 35.16759      2     28 2.303e-08 ***
Hotelling-Lawley  1 2.5119704 35.16759      2     28 2.303e-08 ***
Roy               1 2.5119704 35.16759      2     28 2.303e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

                  SS num Df Error SS den Df         F    Pr(&gt;F)    
(Intercept) 34157997      1    26238     29 37754.334 &lt; 2.2e-16 ***
Factor         59964      2    43371     58    40.094 1.163e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Mauchly Tests for Sphericity

       Test statistic p-value
Factor        0.96168 0.57866


Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

        GG eps Pr(&gt;F[GG])    
Factor 0.96309  2.595e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

        HF eps   Pr(&gt;F[HF])
Factor 1.03025 1.163294e-11
</code></pre>

<p>The ANOVA table isn&rsquo;t pretty, but it works. As you can see, the ANOVA was wildly significant.</p>

<p>Next, I generate data for a two-way (2x3) repeated measures ANOVA. Condition A is the same data as above. Condition B has a different pattern (2 is lower than 1 and 3), which should produce an interaction.</p>

<pre><code class="python">random.seed(1)

cond_1a = [random.gauss(600,30) for x in range(30)] #u=600,sd=30
cond_2a = [random.gauss(650,30) for x in range(30)] #u=650,sd=30
cond_3a = [random.gauss(600,30) for x in range(30)] #u=600,sd=30

cond_1b = [random.gauss(600,30) for x in range(30)] #u=600,sd=30
cond_2b = [random.gauss(550,30) for x in range(30)] #u=550,sd=30
cond_3b = [random.gauss(650,30) for x in range(30)] #u=650,sd=30

width = 0.25
plt.bar(np.arange(1,4)-width,[np.mean(cond_1a),np.mean(cond_2a),np.mean(cond_3a)],width)
plt.bar(np.arange(1,4),[np.mean(cond_1b),np.mean(cond_2b),np.mean(cond_3b)],width,color=plt.rcParams['axes.color_cycle'][0])
plt.legend(['A','B'],loc=4)
plt.xticks([1,2,3]);
</code></pre>

<p><img src="/images/rmANOVA_2.png" /></p>

<pre><code class="python">%Rpush cond_1a cond_1b cond_2a cond_2b cond_3a cond_3b

%R Factor1 &lt;- c('A','A','A','B','B','B')
%R Factor2 &lt;- c('Cond1','Cond2','Cond3','Cond1','Cond2','Cond3')
%R idata &lt;- data.frame(Factor1, Factor2)

#make sure the vectors appear in the same order as they appear in the dataframe
%R Bind &lt;- cbind(cond_1a, cond_2a, cond_3a, cond_1b, cond_2b, cond_3b)
%R model &lt;- lm(Bind~1)

%R library(car)
%R analysis &lt;- Anova(model, idata=idata, idesign=~Factor1*Factor2, type="III")
%R anova_sum = summary(analysis)
%Rpull anova_sum

print anova_sum
</code></pre>

<pre><code>Type III Repeated Measures MANOVA Tests:

------------------------------------------

Term: (Intercept) 

 Response transformation matrix:
        (Intercept)
cond_1a           1
cond_2a           1
cond_3a           1
cond_1b           1
cond_2b           1
cond_3b           1

Sum of squares and products for the hypothesis:
            (Intercept)
(Intercept)   401981075

Sum of squares and products for error:
            (Intercept)
(Intercept)    185650.5

Multivariate Tests: (Intercept)
                 Df test stat approx F num Df den Df     Pr(&gt;F)    
Pillai            1    0.9995 62792.47      1     29 &lt; 2.22e-16 ***
Wilks             1    0.0005 62792.47      1     29 &lt; 2.22e-16 ***
Hotelling-Lawley  1 2165.2575 62792.47      1     29 &lt; 2.22e-16 ***
Roy               1 2165.2575 62792.47      1     29 &lt; 2.22e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor1 

 Response transformation matrix:
        Factor11
cond_1a        1
cond_2a        1
cond_3a        1
cond_1b       -1
cond_2b       -1
cond_3b       -1

Sum of squares and products for the hypothesis:
         Factor11
Factor11 38581.51

Sum of squares and products for error:
         Factor11
Factor11 142762.3

Multivariate Tests: Factor1
                 Df test stat approx F num Df den Df    Pr(&gt;F)   
Pillai            1 0.2127533 7.837247      1     29 0.0090091 **
Wilks             1 0.7872467 7.837247      1     29 0.0090091 **
Hotelling-Lawley  1 0.2702499 7.837247      1     29 0.0090091 **
Roy               1 0.2702499 7.837247      1     29 0.0090091 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor2 

 Response transformation matrix:
        Factor21 Factor22
cond_1a        1        0
cond_2a        0        1
cond_3a       -1       -1
cond_1b        1        0
cond_2b        0        1
cond_3b       -1       -1

Sum of squares and products for the hypothesis:
         Factor21 Factor22
Factor21 91480.01 77568.78
Factor22 77568.78 65773.02

Sum of squares and products for error:
         Factor21 Factor22
Factor21 90374.60 56539.06
Factor22 56539.06 87589.85

Multivariate Tests: Factor2
                 Df test stat approx F num Df den Df    Pr(&gt;F)    
Pillai            1 0.5235423 15.38351      2     28 3.107e-05 ***
Wilks             1 0.4764577 15.38351      2     28 3.107e-05 ***
Hotelling-Lawley  1 1.0988223 15.38351      2     28 3.107e-05 ***
Roy               1 1.0988223 15.38351      2     28 3.107e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

------------------------------------------

Term: Factor1:Factor2 

 Response transformation matrix:
        Factor11:Factor21 Factor11:Factor22
cond_1a                 1                 0
cond_2a                 0                 1
cond_3a                -1                -1
cond_1b                -1                 0
cond_2b                 0                -1
cond_3b                 1                 1

Sum of squares and products for the hypothesis:
                  Factor11:Factor21 Factor11:Factor22
Factor11:Factor21          179585.9            384647
Factor11:Factor22          384647.0            823858

Sum of squares and products for error:
                  Factor11:Factor21 Factor11:Factor22
Factor11:Factor21          92445.33          45639.49
Factor11:Factor22          45639.49          89940.37

Multivariate Tests: Factor1:Factor2
                 Df test stat approx F num Df den Df     Pr(&gt;F)    
Pillai            1  0.901764 128.5145      2     28 7.7941e-15 ***
Wilks             1  0.098236 128.5145      2     28 7.7941e-15 ***
Hotelling-Lawley  1  9.179605 128.5145      2     28 7.7941e-15 ***
Roy               1  9.179605 128.5145      2     28 7.7941e-15 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

                      SS num Df Error SS den Df          F    Pr(&gt;F)    
(Intercept)     66996846      1    30942     29 62792.4662 &lt; 2.2e-16 ***
Factor1             6430      1    23794     29     7.8372  0.009009 ** 
Factor2            26561      2    40475     58    19.0310  4.42e-07 ***
Factor1:Factor2   206266      2    45582     58   131.2293 &lt; 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Mauchly Tests for Sphericity

                Test statistic p-value
Factor2                0.96023 0.56654
Factor1:Factor2        0.99975 0.99648


Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

                 GG eps Pr(&gt;F[GG])    
Factor2         0.96175  6.876e-07 ***
Factor1:Factor2 0.99975  &lt; 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

                  HF eps   Pr(&gt;F[HF])
Factor2         1.028657 4.420005e-07
Factor1:Factor2 1.073774 2.965002e-22
</code></pre>

<p>Again, the anova table isn&rsquo;t too pretty.</p>

<p>This obviously isn&rsquo;t the most exciting post in the world, but its a nice bit of code to have in your back pocket if you&rsquo;re doing experimental analyses in python.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Complete Amnesia for Object Attributes?]]></title>
    <link href="http://www.danvatterott.com/blog/2015/12/12/complete-amnesia-for-object-attributes/"/>
    <updated>2015-12-12T13:10:09-05:00</updated>
    <id>http://www.danvatterott.com/blog/2015/12/12/complete-amnesia-for-object-attributes</id>
    <content type="html"><![CDATA[<h3>Reanalysis of Chen &amp; Wyble, 2015</h3>

<p>Chen and Wyble published an interesting paper (2015) where they demonstrate that participants cannot report attributes of attended stimuli unless the participants are previously informed that this attribute is important. For instance, you wouldn&rsquo;t remember the color of the apple if you had had just told someone the shape. I would have expected the opposite, so &hellip; cool!</p>

<p>After reading the paper (you can check it out at <a href="http://wyblelab.com/publications">http://wyblelab.com/publications</a>), I became curious whether participants might unconsciously retain some information about these forgotten attributes. Chen and Wyble posted their data to databrary.com (<a href="https://nyu.databrary.org/volume/79">https://nyu.databrary.org/volume/79</a>), so I downloaded the data and did some quick analyses that you see here! I want to commend Chen and Wyble for sharing their data. This is something everyone should start doing (including me).</p>

<p>Below, I will start by showing I can replicate Chen and Wyble&rsquo;s analyses, then I will investigate whether there&rsquo;s a trace of unconscious memory for the &ldquo;forgotten&rdquo; features.</p>

<p>EDIT[12/22/15]:
Brad Wyble recently pointed out that I overstated the claim in their paper. They do not claim participants have complete amnesia for unqueried object attributes. Rather, Chen and Wyble focus on the dramatic performance change between the first and second trial following the initial query about an object attribute. This performance change demonstrates amnesia, but not necessarily complete amnesia.</p>

<h5>References</h5>

<p>Chen, H., &amp; Wyble, B. (2015). Amnesia for Object Attributes Failure to Report Attended Information That Had Just Reached Conscious Awareness. Psychological science, 26(2),203-210.</p>

<p>Wyble, B. (2014). Amnesia for object attributes: Failure to report attended information that had just reached conscious awareness. Databrary. Retrieved November 22, 2015 from <a href="http://doi.org/10.17910/B7G010">http://doi.org/10.17910/B7G010</a></p>

<h4>Load relevant libraries and write analysis functions</h4>

<p>I&rsquo;ll start by loading the python libraries that I&rsquo;ll use throughout analyses.</p>

<pre><code class="python">import numpy as np, sys, scipy.stats, pandas as pd, os, os.path, csv #loading useful libraries

import matplotlib as mpl
import matplotlib.pyplot as plt
import pylab as pl
%matplotlib inline
pd.options.display.mpl_style = 'default' #load matplotlib for plotting
plt.style.use('ggplot') #im addicted to ggplot. so pretty.
mpl.rcParams['font.family'] = ['Bitstream Vera Sans']
</code></pre>

<p>Here are some quick functions I wrote for running different statistical tests and plotting the data. I won&rsquo;t explain this code, but encourage you to look through it later if you&rsquo;re wondering how I did any of the analyses.</p>

<pre><code class="python">def print_tests(series1, series2): #this function just presents normality and t-tests. 
    import scipy, numpy as np #the function does these tests with arcsin(sqrt(acc)) to help with normality
    normTest2 = scipy.stats.shapiro(np.arcsin(np.sqrt(series1))-np.arcsin(np.sqrt(series2)))
    Test2 = scipy.stats.ttest_rel(np.arcsin(np.sqrt(series1)), np.arcsin(np.sqrt(series2)))
    Test3 = scipy.stats.wilcoxon(np.arcsin(np.sqrt(series1)), y=np.arcsin(np.sqrt(series2)))
    print '\t normality test adj. Test value: %s P-value: %s' % (str(np.round(normTest2[0],2)), 
                                                                 str(np.round(normTest2[1],4)))
    if normTest2[1] &gt; 0.1: print '\t T-test adj. Test value: %s P-value: %s' % (str(np.round(Test2[0],2)), 
                                                                                str(np.round(Test2[1],4)))
    if normTest2[1] &lt;= 0.1: print '\t Wilcoxon. Test value: %s P-value: %s' % (str(np.round(Test3[0],2)),
                                                                               str(np.round(Test3[1],2)))

def print_tests_ChiSq(series): #this function just presents normality and t-tests. 
    import scipy, numpy as np #the function does these tests with arcsin(sqrt(acc)) to help with normality
    Test1 = scipy.stats.chisquare(series[1], f_exp = 0.25)
    Test2 = scipy.stats.chisquare(series[2], f_exp = 0.25)
    print '\t Surprise Test. Comparison to Chance: %s P-value: %s' % (str(np.round(Test1[0],4)),
                                                                      str(np.round(Test1[1],4)))
    print '\t After Surprise Test. Comparison to Chance: %s P-value: %s' % (str(np.round(Test2[0],4)),
                                                                            str(np.round(Test2[1],4)))
    x = scipy.stats.chi2_contingency([[sum(series[1]==1),sum(series[2]==1)], [sum(series[1]==0),sum(series[2]==0)]],
                                     correction=False) 
    print '\t Chi-Square Comparison: %s P-value: %s' % (str(np.round(x[0],4)),str(np.round(x[1],4)))

def Analysis_and_Plot(tableRT2, CIs): #this function plots the data and writes the results, including stats tests
    PlotFrame = pd.DataFrame([tableRT2.mean()])
    PlotFrame.columns = ['Presurprise', 'Surprise', 'Post surprise']
    PlotFrame2 = pd.DataFrame([CIs])
    PlotFrame2.columns = ['Presurprise', 'Surprise', 'Post surprise']
    PlotFrame.plot(ylim = [0, 1], yerr=PlotFrame2, kind='bar')
    plt.xticks(range(1), ['Trial Type'], rotation=0);

    print '---------------------------------'
    print 'Mean Presurprise: %s' % (str(round(np.mean(tableRT2[0]),2)))
    print 'Mean Surprise: %s' % (str(round(np.mean(tableRT2[1]),2)))
    print 'Mean Postsurprise: %s' % (str(round(np.mean(tableRT2[2]),2)))
    print 'Presurprise - Surprise: %s' % (str(round(np.mean(tableRT2[0])-np.mean(tableRT2[1]),2)))
    print 'Postsurprise - Surprise: %s' % (str(round(np.mean(tableRT2[2])-np.mean(tableRT2[1]),2)))
    print 'Postsurprise - Presurprise: %s' % (str(round(np.mean(tableRT2[0])-np.mean(tableRT2[1]),2)))
    print '---------------------------------'
    print 'Presurprise vs Surprise'
    print_tests(tableRT2[1],tableRT2[0])
    print 'Postsuprise vs Surprise'
    print_tests(tableRT2[2],tableRT2[1])
    print 'Presurprise vs Postsurprise'
    print_tests(tableRT2[0],tableRT2[2])

def Analysis_and_Plot_2(tableRT2, CIs): #this function also plots the data and prints results.
    PlotFrame = pd.DataFrame([tableRT2.mean()]) #I could probably consolidate these functions, but whatever. This works.
    PlotFrame.columns = ['Surprise', 'Postsurprise']
    PlotFrame.plot(ylim = [0, 1], kind='bar')#yerr=PlotFrame2, kind='bar')
    plt.xticks(range(1), ['Trial Type'], rotation=0);

    print '---------------------------------'
    print 'Mean Surprise: %s' % (str(round(np.mean(tableRT2[1]),2)))
    print 'Mean Postsurprise: %s' % (str(round(np.mean(tableRT2[2]),2)))
    print 'Postsurprise - Surprise: %s' % (str(round(np.mean(tableRT2[2])-np.mean(tableRT2[1]),2)))
    print '---------------------------------'
    print 'Postsurprise vs Surprise'
    print_tests_ChiSq(tableRT2)

def Analysis_and_Plot_3(tableRT2, CIs): #another plotting function...i should really consolidate these.
    PlotFrame = pd.DataFrame([tableRT2.mean()])
    PlotFrame.columns = ['Match', 'Mismatch']
    PlotFrame2 = pd.DataFrame()
    PlotFrame2['Match'] = pd.DataFrame([CIs])
    PlotFrame2['Mismatch'] = pd.DataFrame([CIs])
    PlotFrame.plot(ylim = [0, 1], yerr=PlotFrame2, kind='bar')
    plt.xticks(range(1), ['Trial Type'], rotation=0);

    #disp_tab = np.round(tableRT2,2)
    #disp_tab['Match-Mismatch'] = disp_tab[1] - disp_tab[2]
    #print disp_tab

    print '---------------------------------'
    print 'Mean match: %s' % (str(round(np.mean(tableRT2[1]),4)))
    print 'Mean mismatch: %s' % (str(round(np.mean(tableRT2[2]),4)))
    print 'Match - Mismatch: %s' % (str(round(np.mean(tableRT2[1])-np.mean(tableRT2[2]),4)))
    print '---------------------------------'
    print 'Match vs Mismatch'
    print_tests(tableRT2[1],tableRT2[2])

def OneWayConfInterval(table): #Calculates confidence intervals for a one way anova, this is Cousineau, Morey ect
    import scipy.stats, numpy
    ParticipantsMeans = []
    STEs = []
    CIs = []
    for participant in table.index:
        mean = []
        for condition in xrange(numpy.shape(table)[1]): #there's definitely a better way to do this, but this works...
            mean.append(np.array(table[table.index==participant][condition]))
        ParticipantsMeans.append(sum(mean)/len(mean))
    ConfMeans = numpy.zeros(shape=numpy.shape(table))
    for counter, participant in enumerate(table.index):
        for condition in xrange(numpy.shape(table)[1]):
            ConfMeans[counter][condition] = table[table.index==participant][condition]-\
            ParticipantsMeans[counter]+numpy.array(ParticipantsMeans).mean()
    for counter, column in enumerate(ConfMeans.T):
        STEs.append(numpy.std(column, ddof=1)/numpy.sqrt(len(column)))
        CIs.append(STEs[counter]*scipy.stats.t.isf(0.025, len(ConfMeans)-1))
    return CIs

def SimpleComparisonCI(table): #confidence interval for pairwise comparisons - masson &amp; loftus, Baguley (2012)
    import scipy.stats, math
    ttest = scipy.stats.ttest_rel(table[1], table[2])
    MeanDiff_byT = abs((table[1].mean()-table[2].mean())/ttest[0])
    CI = MeanDiff_byT*scipy.stats.t.isf(0.025, len(table)-1)*math.pow(2,0.05)/2
    return CI
</code></pre>

<h4>Experiment 1</h4>

<p>Next, load Experiment 1 data</p>

<pre><code class="python">filename = 'Exp1.csv' #looking at Exp1 data first. 

if sys.platform == 'linux2': #is this my linux laptop
    path = '/home/dan-laptop/Documents/Databrary/Wyble_PsychSci'
elif sys.platform == 'darwin': #is this my mac work comp    
    path = '/Users/danvatterott/Dropbox Encore/Dropbox/Databrary/Wyble_PsychSci'

os.chdir(path)

df = pd.read_csv(filename)

df.columns = ['Sub#', 'Block', 'Trial#', 'TarCol', 'Tar_Iden','Tar_Loc', 'Col_Resp', 'Iden_Resp', 'Loc_Resp', 
             'Col_Acc', 'Iden_Acc', 'Loc_Acc'] #naming the columns of the data file.
</code></pre>

<p>The data is loaded, lets just take a quick look at the data after loading it in.</p>

<pre><code class="python">df[0:5]
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sub#</th>
      <th>Block</th>
      <th>Trial#</th>
      <th>TarCol</th>
      <th>Tar_Iden</th>
      <th>Tar_Loc</th>
      <th>Col_Resp</th>
      <th>Iden_Resp</th>
      <th>Loc_Resp</th>
      <th>Col_Acc</th>
      <th>Iden_Acc</th>
      <th>Loc_Acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>1</td>
      <td>5</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6</td>
      <td>1</td>
      <td>6</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>


<p>I want to create a new variable.</p>

<p>Before explaining the new variable, I should explain a little about Chen and Wyble&rsquo;s experiment. Half the participants were instructed to find the letter among numbers and the other half were instructed to find the number among letters. 4 items were briefly flashed on the screen (150 ms) then participants reported where the target item had been. Each of the 4 items was a different color.</p>

<p>Participants reported target location for 155 trials. On the 156th trial, the participants reported the target location then (to their surprise) reported what specific letter/number the target was and what color it was. Even though participants knew where the target was, they had no idea what either the target&rsquo;s letter/number or color were. They had &ldquo;forgotten&rdquo; what the target was (even though they must have known what the target was when they initially located it).</p>

<p>The new variable will code whether the trial is a &ldquo;pre-surprise&rdquo; trial (trials 1-155), a surprise trial (trial 156) or the trial after the surprise (trial 157).</p>

<p>I&rsquo;ll call this variable &ldquo;TrialType&rdquo;</p>

<p>TrialType: 0=Presurprise; 1=Surprise Trial; 2=Postsurprise trials.</p>

<pre><code class="python">df['TrialType'] = np.where(df['Trial#']&lt;156, 0, np.where(df['Trial#']==156, 1, 2))
df['TrialType2'] = np.where(df['Trial#']&lt;156, 0, np.where(df['Trial#']==156, 1, np.where(df['Trial#']==157, 2, -1)))
</code></pre>

<h4>Experiment 1: Replicating Chen &amp; Wyble&rsquo;s (2015) analyses</h4>

<p>Lets just take a quick look at overall accuracy. Make sure everyone is doing ok on the task. Below I plot the mean accuracy of each participant&hellip;looks like participant 23 struggled a little. Chen and Wyble (2015) notes that no participants were excluded or replaced.</p>

<p>I might have replaced participant 23 since his/her accuracy is easily 2.5 standard deviations below the mean accuracy (I print this value below)&hellip;seems like participant 23 was doing something different in this task.</p>

<pre><code class="python">tableAcc = df.pivot_table(values='Loc_Acc', index='Sub#', aggfunc=np.mean)
#print tableAcc
print 'mean accuracy'
print np.round(np.mean(tableAcc),2)
print 'standard deviation of accuracies'
print np.round(np.std(tableAcc),2)

print '2.5 standard deviations below mean accuracy'
print np.round(np.mean(tableAcc)-2.5*np.std(tableAcc),2)

tableAcc.plot(ylim = [0, 1], kind='bar');
</code></pre>

<pre><code>mean accuracy
0.89
standard deviation of accuracies
0.07
2.5 standard deviations below mean accuracy
0.71
</code></pre>

<p><img src="/images/Wyble1.png" /></p>

<p>Lets look at participants' performance when asked to identify the target&rsquo;s location. I will plot performance as mean accuracy in the presurprise,surprise, and postsurprose trials.</p>

<p>I will also run some quick statistical tests. For these tests, I take the arcsine of the square root of the accuracies (Rao, 1960) to increase the accuracies' normality (I use adj. to indiciate that the tested data is transformed). I test whether this worked with a Shapiro-Wilk test of normality. If the p-value of the Shapiro-Wilk test is greater than 0.1, I run a t test to see if the accuracy in the two conditions is significantly different. If the p-value of the Shapiro-Wilk test is less than or equal to 0.1, then I run a Wilcoxon signed rank test since this test does not care about normality.</p>

<pre><code class="python">Loc_Acc = df.pivot_table(values='Loc_Acc', index='Sub#', columns='TrialType', aggfunc=np.mean)
CIs = np.array(OneWayConfInterval(Loc_Acc))
Analysis_and_Plot(Loc_Acc, CIs)
</code></pre>

<pre><code>---------------------------------
Mean Presurprise: 0.89
Mean Surprise: 0.8
Mean Postsurprise: 0.79
Presurprise - Surprise: 0.09
Postsurprise - Surprise: -0.01
Postsurprise - Presurprise: 0.09
---------------------------------
Presurprise vs Surprise
     normality test adj. Test value: 0.64 P-value: 0.0
     Wilcoxon. Test value: 74.0 P-value: 0.25
Postsuprise vs Surprise
     normality test adj. Test value: 0.8 P-value: 0.001
     Wilcoxon. Test value: 33.0 P-value: 0.63
Presurprise vs Postsurprise
     normality test adj. Test value: 0.94 P-value: 0.2857
     T-test adj. Test value: 0.92 P-value: 0.3695
</code></pre>

<p><img src="/images/Wyble2.png" /></p>

<p>The y-axis represents percent correct. All graphs in this post will have percent correct on the y-axis.</p>

<p>Replicating Chen and Wyble, participants perform no worse in the surprise and post surprise trials, indicating that they succesfully found the target.</p>

<p>Now lets look at participants' ability to report the target&rsquo;s color in the surprise trial and the trial immediately following the surprise test.</p>

<p>Below I plot the percent of participants that correctly identified the target&rsquo;s color in the surprise and post-surprise trials</p>

<pre><code class="python">Trial_Trimmer = df['TrialType2'] &gt; 0
Col_Acc = df[Trial_Trimmer].pivot_table(values='Col_Acc', index='Sub#', columns='TrialType2', aggfunc=np.mean)
CIs = SimpleComparisonCI(Col_Acc)
Analysis_and_Plot_2(Col_Acc, CIs)
</code></pre>

<pre><code>---------------------------------
Mean Surprise: 0.3
Mean Postsurprise: 0.7
Postsurprise - Surprise: 0.4
---------------------------------
Postsurprise vs Surprise
     Surprise Test. Comparison to Chance: 17.0 P-value: 0.5899
     After Surprise Test. Comparison to Chance: 33.0 P-value: 0.024
     Chi-Square Comparison: 6.4 P-value: 0.0114
</code></pre>

<p><img src="/images/Wyble3.png" /></p>

<p>We perfectly replicate Chen and Wyble; participants respond more accurarely in the post-surprise trial than in the surprise trial.</p>

<p>The next cell examines participants' ability to report the target&rsquo;s identity on the surprise trial and the trial immediately following the surprise trial. Remember, the participants locate the target based on whether its a letter or number, so they know the broad category of the target. Nonetheless, they cannot report the target&rsquo;s identity on the surprise trial</p>

<pre><code class="python">Trial_Trimmer = df['TrialType2'] &gt; 0
Iden_Acc = df[Trial_Trimmer].pivot_table(values='Iden_Acc', index='Sub#', columns='TrialType2', aggfunc=np.mean)
CIs = SimpleComparisonCI(Iden_Acc)
Analysis_and_Plot_2(Iden_Acc, CIs)
</code></pre>

<pre><code>---------------------------------
Mean Surprise: 0.25
Mean Postsurprise: 0.75
Postsurprise - Surprise: 0.5
---------------------------------
Postsurprise vs Surprise
     Surprise Test. Comparison to Chance: 15.0 P-value: 0.7226
     After Surprise Test. Comparison to Chance: 35.0 P-value: 0.014
     Chi-Square Comparison: 10.0 P-value: 0.0016
</code></pre>

<p><img src="/images/Wyble4.png" /></p>

<h4>Experiment 1 - Intertrial analyses</h4>

<p>So far, I&rsquo;ve perfectly replicated Chen &amp; Wyble (which is good since this is their data).</p>

<p>Now I want to see if the target&rsquo;s color or identity on the previous trial influences the current trial&rsquo;s performance in the location task. I am only examining presurprise trials, so this should be trials when the participants don&rsquo;t &ldquo;remember&rdquo; the target&rsquo;s color or identity.</p>

<p>First I want to make some variables representing whether the target&rsquo;s color and identity repeat across trials.</p>

<pre><code class="python">df['Prev_TarCol'] = df['TarCol'].shift(periods=1)
df['Prev_TarCol_match'] = np.where(df['Prev_TarCol']==df['TarCol'], 1, 2)
df['Prev_Iden'] = df['Tar_Iden'].shift(periods=1)
df['Prev_Iden_match'] = np.where(df['Prev_Iden']==df['Tar_Iden'], 1, 2)
df['Prev_Col+Iden_match'] = np.where((df['Prev_Iden_match']==1) &amp; (df['Prev_TarCol_match']==1), 1, 2)
</code></pre>

<p>Lets see what happens when the target&rsquo;s color and identity repeat.</p>

<pre><code class="python">Trial_Trimmer = df['TrialType'] == 0
ColandIden_Acc1 = df[Trial_Trimmer].pivot_table(values='Loc_Acc', index='Sub#', columns='Prev_Col+Iden_match',
                                                aggfunc=np.mean)
CIs = SimpleComparisonCI(ColandIden_Acc1)
Analysis_and_Plot_3(ColandIden_Acc1, CIs)
</code></pre>

<pre><code>---------------------------------
Mean match: 0.918
Mean mismatch: 0.8925
Match - Mismatch: 0.0255
---------------------------------
Match vs Mismatch
     normality test adj. Test value: 0.92 P-value: 0.0821
     Wilcoxon. Test value: 51.0 P-value: 0.04
</code></pre>

<p><img src="/images/Wyble5.png" /></p>

<p>Looks like a 2.5% increase in accuracy. Now, this wasn&rsquo;t really a planned comparison, so please take this result with a grain of salt.</p>

<p>As a sanity check, lets look at how repetitions in the target&rsquo;s location (the reported feature) effect performance.</p>

<p>We have to quickly create a new variable coding target location repetitions</p>

<pre><code class="python">df['Prev_Loc'] = df['Tar_Loc'].shift(periods=1)
df['Prev_Loc_match'] = np.where(df['Prev_Loc']==df['Tar_Loc'], 1, 2)
</code></pre>

<pre><code class="python">Trial_Trimmer = df['TrialType'] == 0
Loc_Acc1 = df[Trial_Trimmer].pivot_table(values='Loc_Acc', index='Sub#', columns='Prev_Loc_match', aggfunc=np.mean)
CIs = SimpleComparisonCI(Loc_Acc1)
Analysis_and_Plot_3(Loc_Acc1, CIs)
</code></pre>

<pre><code>---------------------------------
Mean match: 0.9101
Mean mismatch: 0.8883
Match - Mismatch: 0.0218
---------------------------------
Match vs Mismatch
     normality test adj. Test value: 0.93 P-value: 0.1812
     T-test adj. Test value: 2.62 P-value: 0.0168
</code></pre>

<p><img src="/images/Wyble6.png" /></p>

<p>Target location repetitions lead to a 2% increase in performance. Again, this result is robust.</p>

<p>It&rsquo;s a good sign that this effect is about the same size as repetitions in the unreported features.</p>

<h3>Replicate Experiments 1 Intertrial Analyses with Experiment 1b</h3>

<p>Experiment 1 had some evidence that participants unconsciously knew the color and identity of the target, since they performed a little better when the color and identity repeated. The effect was small, so I am not 100% confident that it&rsquo;s robust.</p>

<p>The best way to demonstrate that this effect is real would be to show that it also exists in another similar Experiment. Chen and Wyble provide a replication of Experiment 1. In this experiment, the only difference is the target and distractors appear for longer and are not masked (making them easier to see).</p>

<p>If participants response more accurately when the target color and identity repeat in Experiment 1b, then we can be a little more confident that participants are unconsciously aware of the target&rsquo;s color and identity.</p>

<pre><code class="python">filename = 'Exp1b.csv'
df = pd.read_csv(filename)
df.columns = ['Sub#', 'Block', 'Trial#', 'TarCol', 'Tar_Iden','Tar_Loc', 'Col_Resp', 'Iden_Resp', 'Loc_Resp', 
             'Col_Acc', 'Iden_Acc', 'Loc_Acc'] #naming the columns of the data file.

df['TrialType'] = np.where(df['Trial#']&lt;156, 0, np.where(df['Trial#']==156, 1, 2))
df['TrialType2'] = np.where(df['Trial#']&lt;156, 0, np.where(df['Trial#']==156, 1, np.where(df['Trial#']==157, 2, -1)))

df['Prev_TarCol'] = df['TarCol'].shift(periods=1)
df['Prev_TarCol_match'] = np.where(df['Prev_TarCol']==df['TarCol'], 1, 2)
df['Prev_Iden'] = df['Tar_Iden'].shift(periods=1)
df['Prev_Iden_match'] = np.where(df['Prev_Iden']==df['Tar_Iden'], 1, 2)
df['Prev_Col+Iden_match'] = np.where((df['Prev_Iden_match']==1) &amp; (df['Prev_TarCol_match']==1), 1, 2)

Trial_Trimmer = df['TrialType'] == 0 #only interested in pre-surprise trials
ColandIden_Acc = df[Trial_Trimmer].pivot_table(values='Loc_Acc', index='Sub#', 
                                               columns='Prev_Col+Iden_match', aggfunc=np.mean)
</code></pre>

<pre><code class="python">CIs = SimpleComparisonCI(ColandIden_Acc)
Analysis_and_Plot_3(ColandIden_Acc, CIs)
</code></pre>

<pre><code>---------------------------------
Mean match: 0.9716
Mean mismatch: 0.9644
Match - Mismatch: 0.0072
---------------------------------
Match vs Mismatch
     normality test adj. Test value: 0.93 P-value: 0.1875
     T-test adj. Test value: 2.81 P-value: 0.0112
</code></pre>

<p><img src="/images/Wyble7.png" /></p>

<p>Wow. Only a 1% change in accuracy, so again not big. Nonetheless, this result is signficant. So, Some evidence that participants perform a little better when the targets' color and identity repeat.</p>

<p>This suggests that participants retain some information about the targets' color and identity even though they cannot explicitly report these attributes.</p>

<p>Now, I would probably want to replicate this result again before trusting it, but I&rsquo;m relatively confident that participants unconsciously retain some information about the target&rsquo;s color and identity.</p>
]]></content>
  </entry>
  
</feed>
