<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Predicting Career Performance from Rookie Performance &mdash; Dan Vatterott</title>
  <meta name="author" content="Dan Vatterott">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="https://danvatterott.com/favicon.png" rel="icon">

  <link href="https://danvatterott.com/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://danvatterott.com/">Dan Vatterott</a></h1>
    <h2>Data Scientist</h2>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>


<ul class="main-navigation">
    <li><a href="/">Home</a></li>
    <li><a href="/about-me.html">About Me</a></li>
    <li><a href="/publications.html">Publications</a></li>
    <li><a href="/Vatterott_Resume.pdf">Resume</a></li>
    <li><a href="/my-reads.html">My Reads</a></li>
    <li><a href="/presentations.html">Presentations</a></li>
    <li><a href="/blog.html">Blog</a></li>
    <li><a href="/archives.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Predicting Career Performance from Rookie Performance</h1>
    <p class="meta">
<time datetime="2016-03-20T15:56:18-04:00" pubdate>Sun 20 March 2016</time>    </p>
</header>

  <div class="entry-content"><p>As a huge t-wolves fan, I've been curious all year by what we can infer from Karl-Anthony Towns' great rookie season. To answer this question, I've create a simple linear regression model that uses rookie year performance to predict career performance.</p>
<p>Many have attempted to predict NBA players' success via regression style approaches. Notable models I know of include <a href="http://laynevashro.com/basketball/predsFAQ.html">Layne Vashro's model</a> which uses combine and college performance to predict career performance. Layne Vashro's model is a quasi-poisson GLM. I tried a similar approach, but had the most success when using ws/48 and OLS. I will discuss this a little more at the end of the post.</p>
<p>A jupyter notebook of this post can be found on my <a href="https://github.com/dvatterott/jupyter_notebooks/blob/master/nba_rookie_regression.ipynb">github</a>.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="c1">#import some libraries and tell ipython we want inline figures rather than interactive figures.</span>
<span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span><span class="o">,</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span><span class="o">,</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpl</span>

<span class="w"> </span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="n">print_function</span>

<span class="w"> </span><span class="o">%</span><span class="n">matplotlib</span><span class="w"> </span><span class="n">inline</span>
<span class="w"> </span><span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">mpl_style</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;default&#39;</span><span class="w"> </span><span class="c1">#load matplotlib for plotting</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span><span class="w"> </span><span class="c1">#im addicted to ggplot. so pretty.</span>
<span class="w"> </span><span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.family&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;Bitstream Vera Sans&#39;</span><span class="p">]</span>
</code></pre></div>

<p>I collected all the data for this project from basketball-reference.com. I posted the functions for collecting the data on my <a href="https://github.com/dvatterott/nba_project">github</a>. The data is also posted there. Beware, the data collection scripts take awhile to run.  </p>
<p>This data includes per 36 stats and advanced statistics such as usage percentage. I simply took all the per 36 and advanced statistics from a player's page on basketball-reference.com.</p>
<div class="highlight"><pre><span></span><code> df = pd.read_pickle(&#39;nba_bballref_career_stats_2016_Mar_15.pkl&#39;) #here&#39;s the career data.
 rookie_df = pd.read_pickle(&#39;nba_bballref_rookie_stats_2016_Mar_15.pkl&#39;) #here&#39;s the rookie year data
</code></pre></div>

<p>The variable I am trying to predict is average <a href="http://www.basketball-reference.com/about/ws.html">WS/48</a> over a player's career. There's no perfect box-score statistic when it comes to quantifying a player's peformance, but ws/48 seems relatively solid.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="n">Games</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;G&#39;</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">50</span><span class="w"> </span><span class="c1">#only using players who played in more than 50 games.</span>
<span class="w"> </span><span class="n">Year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Year&#39;</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">1980</span><span class="w"> </span><span class="c1">#only using players after 1980 when they started keeping many important records such as games started</span>

<span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">[</span><span class="n">Games</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Year</span><span class="p">][</span><span class="s1">&#39;WS/48&#39;</span><span class="p">]</span><span class="w"> </span><span class="c1">#predicted variable</span>

<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Y</span><span class="p">);</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Bin Count&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;WS/48&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img src="https://danvatterott.com/images/regressionNBA/predictor_hist.png" /></p>
<p>The predicted variable looks pretty gaussian, so I can use ordinary least squares. This will be nice because while ols is not flexible, it's highly interpretable. At the end of the post I'll mention some more complex models that I will try.</p>
<div class="highlight"><pre><span></span><code> rook_games = rookie_df[&#39;Career Games&#39;]&gt;50
 rook_year = rookie_df[&#39;Year&#39;]&gt;1980

 #remove rookies from before 1980 and who have played less than 50 games. I also remove some features that seem irrelevant or unfair
 rookie_df_games = rookie_df[rook_games &amp; rook_year] #only players with more than 50 games.
 rookie_df_drop = rookie_df_games.drop([&#39;Year&#39;,&#39;Career Games&#39;,&#39;Name&#39;],1)
</code></pre></div>

<p>Above, I remove some predictors from the rookie data. Lets run the regression!</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="w"> </span><span class="n">X_rookie</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rookie_df_drop</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="w"> </span><span class="c1">#take data out of dataframe</span>
<span class="w"> </span><span class="n">X_rookie</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_rookie</span><span class="p">)</span><span class="w">  </span><span class="c1"># Adds a constant term to the predictor</span>

<span class="w"> </span><span class="n">estAll</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">X_rookie</span><span class="p">)</span><span class="w"> </span><span class="c1">#create ordinary least squares model</span>
<span class="w"> </span><span class="n">estAll</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">estAll</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="w"> </span><span class="c1">#fit the model</span>
<span class="w"> </span><span class="nb">print</span><span class="p">(</span><span class="n">estAll</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="w">                            </span><span class="n">OLS</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">Results</span><span class="w">                            </span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span><span class="w"> </span><span class="n">Variable</span><span class="p">:</span><span class="w">                  </span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="w">   </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                       </span><span class="mf">0.476</span>
<span class="n">Model</span><span class="p">:</span><span class="w">                            </span><span class="n">OLS</span><span class="w">   </span><span class="n">Adj</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                  </span><span class="mf">0.461</span>
<span class="n">Method</span><span class="p">:</span><span class="w">                 </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w">   </span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span><span class="w">                     </span><span class="mf">31.72</span>
<span class="n">Date</span><span class="p">:</span><span class="w">                </span><span class="n">Sun</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">Mar</span><span class="w"> </span><span class="mi">2016</span><span class="w">   </span><span class="n">Prob</span><span class="w"> </span><span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span><span class="w">          </span><span class="mf">2.56e-194</span>
<span class="n">Time</span><span class="p">:</span><span class="w">                        </span><span class="mi">15</span><span class="p">:</span><span class="mi">29</span><span class="p">:</span><span class="mi">43</span><span class="w">   </span><span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span><span class="w">                 </span><span class="mf">3303.9</span>
<span class="n">No</span><span class="o">.</span><span class="w"> </span><span class="n">Observations</span><span class="p">:</span><span class="w">                </span><span class="mi">1690</span><span class="w">   </span><span class="n">AIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">6512.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Residuals</span><span class="p">:</span><span class="w">                    </span><span class="mi">1642</span><span class="w">   </span><span class="n">BIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">6251.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Model</span><span class="p">:</span><span class="w">                          </span><span class="mi">47</span><span class="w">                                         </span>
<span class="n">Covariance</span><span class="w"> </span><span class="n">Type</span><span class="p">:</span><span class="w">            </span><span class="n">nonrobust</span><span class="w">                                         </span>
<span class="o">==============================================================================</span>
<span class="w">                 </span><span class="n">coef</span><span class="w">    </span><span class="n">std</span><span class="w"> </span><span class="n">err</span><span class="w">          </span><span class="n">t</span><span class="w">      </span><span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="w">      </span><span class="p">[</span><span class="mf">95.0</span><span class="o">%</span><span class="w"> </span><span class="n">Conf</span><span class="o">.</span><span class="w"> </span><span class="nb">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="n">const</span><span class="w">          </span><span class="mf">0.2509</span><span class="w">      </span><span class="mf">0.078</span><span class="w">      </span><span class="mf">3.223</span><span class="w">      </span><span class="mf">0.001</span><span class="w">         </span><span class="mf">0.098</span><span class="w">     </span><span class="mf">0.404</span>
<span class="n">x1</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0031</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">6.114</span><span class="w">      </span><span class="mf">0.000</span><span class="w">        </span><span class="o">-</span><span class="mf">0.004</span><span class="w">    </span><span class="o">-</span><span class="mf">0.002</span>
<span class="n">x2</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0004</span><span class="w">   </span><span class="mf">9.06e-05</span><span class="w">     </span><span class="o">-</span><span class="mf">4.449</span><span class="w">      </span><span class="mf">0.000</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">    </span><span class="o">-</span><span class="mf">0.000</span>
<span class="n">x3</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0003</span><span class="w">   </span><span class="mf">8.12e-05</span><span class="w">     </span><span class="o">-</span><span class="mf">3.525</span><span class="w">      </span><span class="mf">0.000</span><span class="w">        </span><span class="o">-</span><span class="mf">0.000</span><span class="w">    </span><span class="o">-</span><span class="mf">0.000</span>
<span class="n">x4</span><span class="w">          </span><span class="mf">1.522e-05</span><span class="w">   </span><span class="mf">4.73e-06</span><span class="w">      </span><span class="mf">3.218</span><span class="w">      </span><span class="mf">0.001</span><span class="w">      </span><span class="mf">5.94e-06</span><span class="w">  </span><span class="mf">2.45e-05</span>
<span class="n">x5</span><span class="w">             </span><span class="mf">0.0030</span><span class="w">      </span><span class="mf">0.031</span><span class="w">      </span><span class="mf">0.096</span><span class="w">      </span><span class="mf">0.923</span><span class="w">        </span><span class="o">-</span><span class="mf">0.057</span><span class="w">     </span><span class="mf">0.063</span>
<span class="n">x6</span><span class="w">             </span><span class="mf">0.0109</span><span class="w">      </span><span class="mf">0.019</span><span class="w">      </span><span class="mf">0.585</span><span class="w">      </span><span class="mf">0.559</span><span class="w">        </span><span class="o">-</span><span class="mf">0.026</span><span class="w">     </span><span class="mf">0.047</span>
<span class="n">x7</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0312</span><span class="w">      </span><span class="mf">0.094</span><span class="w">     </span><span class="o">-</span><span class="mf">0.331</span><span class="w">      </span><span class="mf">0.741</span><span class="w">        </span><span class="o">-</span><span class="mf">0.216</span><span class="w">     </span><span class="mf">0.154</span>
<span class="n">x8</span><span class="w">             </span><span class="mf">0.0161</span><span class="w">      </span><span class="mf">0.027</span><span class="w">      </span><span class="mf">0.594</span><span class="w">      </span><span class="mf">0.553</span><span class="w">        </span><span class="o">-</span><span class="mf">0.037</span><span class="w">     </span><span class="mf">0.069</span>
<span class="n">x9</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0054</span><span class="w">      </span><span class="mf">0.018</span><span class="w">     </span><span class="o">-</span><span class="mf">0.292</span><span class="w">      </span><span class="mf">0.770</span><span class="w">        </span><span class="o">-</span><span class="mf">0.041</span><span class="w">     </span><span class="mf">0.031</span>
<span class="n">x10</span><span class="w">            </span><span class="mf">0.0012</span><span class="w">      </span><span class="mf">0.007</span><span class="w">      </span><span class="mf">0.169</span><span class="w">      </span><span class="mf">0.866</span><span class="w">        </span><span class="o">-</span><span class="mf">0.013</span><span class="w">     </span><span class="mf">0.015</span>
<span class="n">x11</span><span class="w">            </span><span class="mf">0.0136</span><span class="w">      </span><span class="mf">0.023</span><span class="w">      </span><span class="mf">0.592</span><span class="w">      </span><span class="mf">0.554</span><span class="w">        </span><span class="o">-</span><span class="mf">0.031</span><span class="w">     </span><span class="mf">0.059</span>
<span class="n">x12</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0099</span><span class="w">      </span><span class="mf">0.018</span><span class="w">     </span><span class="o">-</span><span class="mf">0.538</span><span class="w">      </span><span class="mf">0.591</span><span class="w">        </span><span class="o">-</span><span class="mf">0.046</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x13</span><span class="w">            </span><span class="mf">0.0076</span><span class="w">      </span><span class="mf">0.054</span><span class="w">      </span><span class="mf">0.141</span><span class="w">      </span><span class="mf">0.888</span><span class="w">        </span><span class="o">-</span><span class="mf">0.098</span><span class="w">     </span><span class="mf">0.113</span>
<span class="n">x14</span><span class="w">            </span><span class="mf">0.0094</span><span class="w">      </span><span class="mf">0.012</span><span class="w">      </span><span class="mf">0.783</span><span class="w">      </span><span class="mf">0.433</span><span class="w">        </span><span class="o">-</span><span class="mf">0.014</span><span class="w">     </span><span class="mf">0.033</span>
<span class="n">x15</span><span class="w">            </span><span class="mf">0.0029</span><span class="w">      </span><span class="mf">0.002</span><span class="w">      </span><span class="mf">1.361</span><span class="w">      </span><span class="mf">0.174</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">     </span><span class="mf">0.007</span>
<span class="n">x16</span><span class="w">            </span><span class="mf">0.0078</span><span class="w">      </span><span class="mf">0.009</span><span class="w">      </span><span class="mf">0.861</span><span class="w">      </span><span class="mf">0.390</span><span class="w">        </span><span class="o">-</span><span class="mf">0.010</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x17</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0107</span><span class="w">      </span><span class="mf">0.019</span><span class="w">     </span><span class="o">-</span><span class="mf">0.573</span><span class="w">      </span><span class="mf">0.567</span><span class="w">        </span><span class="o">-</span><span class="mf">0.047</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x18</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0062</span><span class="w">      </span><span class="mf">0.018</span><span class="w">     </span><span class="o">-</span><span class="mf">0.342</span><span class="w">      </span><span class="mf">0.732</span><span class="w">        </span><span class="o">-</span><span class="mf">0.042</span><span class="w">     </span><span class="mf">0.029</span>
<span class="n">x19</span><span class="w">            </span><span class="mf">0.0095</span><span class="w">      </span><span class="mf">0.017</span><span class="w">      </span><span class="mf">0.552</span><span class="w">      </span><span class="mf">0.581</span><span class="w">        </span><span class="o">-</span><span class="mf">0.024</span><span class="w">     </span><span class="mf">0.043</span>
<span class="n">x20</span><span class="w">            </span><span class="mf">0.0111</span><span class="w">      </span><span class="mf">0.004</span><span class="w">      </span><span class="mf">2.853</span><span class="w">      </span><span class="mf">0.004</span><span class="w">         </span><span class="mf">0.003</span><span class="w">     </span><span class="mf">0.019</span>
<span class="n">x21</span><span class="w">            </span><span class="mf">0.0109</span><span class="w">      </span><span class="mf">0.018</span><span class="w">      </span><span class="mf">0.617</span><span class="w">      </span><span class="mf">0.537</span><span class="w">        </span><span class="o">-</span><span class="mf">0.024</span><span class="w">     </span><span class="mf">0.046</span>
<span class="n">x22</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0139</span><span class="w">      </span><span class="mf">0.006</span><span class="w">     </span><span class="o">-</span><span class="mf">2.165</span><span class="w">      </span><span class="mf">0.030</span><span class="w">        </span><span class="o">-</span><span class="mf">0.026</span><span class="w">    </span><span class="o">-</span><span class="mf">0.001</span>
<span class="n">x23</span><span class="w">            </span><span class="mf">0.0024</span><span class="w">      </span><span class="mf">0.005</span><span class="w">      </span><span class="mf">0.475</span><span class="w">      </span><span class="mf">0.635</span><span class="w">        </span><span class="o">-</span><span class="mf">0.008</span><span class="w">     </span><span class="mf">0.012</span>
<span class="n">x24</span><span class="w">            </span><span class="mf">0.0022</span><span class="w">      </span><span class="mf">0.001</span><span class="w">      </span><span class="mf">1.644</span><span class="w">      </span><span class="mf">0.100</span><span class="w">        </span><span class="o">-</span><span class="mf">0.000</span><span class="w">     </span><span class="mf">0.005</span>
<span class="n">x25</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0125</span><span class="w">      </span><span class="mf">0.012</span><span class="w">     </span><span class="o">-</span><span class="mf">1.027</span><span class="w">      </span><span class="mf">0.305</span><span class="w">        </span><span class="o">-</span><span class="mf">0.036</span><span class="w">     </span><span class="mf">0.011</span>
<span class="n">x26</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0006</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">1.782</span><span class="w">      </span><span class="mf">0.075</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">  </span><span class="mf">5.74e-05</span>
<span class="n">x27</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0011</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">1.749</span><span class="w">      </span><span class="mf">0.080</span><span class="w">        </span><span class="o">-</span><span class="mf">0.002</span><span class="w">     </span><span class="mf">0.000</span>
<span class="n">x28</span><span class="w">            </span><span class="mf">0.0012</span><span class="w">      </span><span class="mf">0.003</span><span class="w">      </span><span class="mf">0.487</span><span class="w">      </span><span class="mf">0.626</span><span class="w">        </span><span class="o">-</span><span class="mf">0.004</span><span class="w">     </span><span class="mf">0.006</span>
<span class="n">x29</span><span class="w">            </span><span class="mf">0.1824</span><span class="w">      </span><span class="mf">0.089</span><span class="w">      </span><span class="mf">2.059</span><span class="w">      </span><span class="mf">0.040</span><span class="w">         </span><span class="mf">0.009</span><span class="w">     </span><span class="mf">0.356</span>
<span class="n">x30</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0288</span><span class="w">      </span><span class="mf">0.025</span><span class="w">     </span><span class="o">-</span><span class="mf">1.153</span><span class="w">      </span><span class="mf">0.249</span><span class="w">        </span><span class="o">-</span><span class="mf">0.078</span><span class="w">     </span><span class="mf">0.020</span>
<span class="n">x31</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0128</span><span class="w">      </span><span class="mf">0.011</span><span class="w">     </span><span class="o">-</span><span class="mf">1.206</span><span class="w">      </span><span class="mf">0.228</span><span class="w">        </span><span class="o">-</span><span class="mf">0.034</span><span class="w">     </span><span class="mf">0.008</span>
<span class="n">x32</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0046</span><span class="w">      </span><span class="mf">0.008</span><span class="w">     </span><span class="o">-</span><span class="mf">0.603</span><span class="w">      </span><span class="mf">0.547</span><span class="w">        </span><span class="o">-</span><span class="mf">0.020</span><span class="w">     </span><span class="mf">0.010</span>
<span class="n">x33</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0071</span><span class="w">      </span><span class="mf">0.005</span><span class="w">     </span><span class="o">-</span><span class="mf">1.460</span><span class="w">      </span><span class="mf">0.145</span><span class="w">        </span><span class="o">-</span><span class="mf">0.017</span><span class="w">     </span><span class="mf">0.002</span>
<span class="n">x34</span><span class="w">            </span><span class="mf">0.0131</span><span class="w">      </span><span class="mf">0.012</span><span class="w">      </span><span class="mf">1.124</span><span class="w">      </span><span class="mf">0.261</span><span class="w">        </span><span class="o">-</span><span class="mf">0.010</span><span class="w">     </span><span class="mf">0.036</span>
<span class="n">x35</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0023</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">2.580</span><span class="w">      </span><span class="mf">0.010</span><span class="w">        </span><span class="o">-</span><span class="mf">0.004</span><span class="w">    </span><span class="o">-</span><span class="mf">0.001</span>
<span class="n">x36</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0077</span><span class="w">      </span><span class="mf">0.013</span><span class="w">     </span><span class="o">-</span><span class="mf">0.605</span><span class="w">      </span><span class="mf">0.545</span><span class="w">        </span><span class="o">-</span><span class="mf">0.033</span><span class="w">     </span><span class="mf">0.017</span>
<span class="n">x37</span><span class="w">            </span><span class="mf">0.0069</span><span class="w">      </span><span class="mf">0.004</span><span class="w">      </span><span class="mf">1.916</span><span class="w">      </span><span class="mf">0.055</span><span class="w">        </span><span class="o">-</span><span class="mf">0.000</span><span class="w">     </span><span class="mf">0.014</span>
<span class="n">x38</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0015</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">2.568</span><span class="w">      </span><span class="mf">0.010</span><span class="w">        </span><span class="o">-</span><span class="mf">0.003</span><span class="w">    </span><span class="o">-</span><span class="mf">0.000</span>
<span class="n">x39</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0002</span><span class="w">      </span><span class="mf">0.002</span><span class="w">     </span><span class="o">-</span><span class="mf">0.110</span><span class="w">      </span><span class="mf">0.912</span><span class="w">        </span><span class="o">-</span><span class="mf">0.005</span><span class="w">     </span><span class="mf">0.004</span>
<span class="n">x40</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0109</span><span class="w">      </span><span class="mf">0.017</span><span class="w">     </span><span class="o">-</span><span class="mf">0.632</span><span class="w">      </span><span class="mf">0.528</span><span class="w">        </span><span class="o">-</span><span class="mf">0.045</span><span class="w">     </span><span class="mf">0.023</span>
<span class="n">x41</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0142</span><span class="w">      </span><span class="mf">0.017</span><span class="w">     </span><span class="o">-</span><span class="mf">0.821</span><span class="w">      </span><span class="mf">0.412</span><span class="w">        </span><span class="o">-</span><span class="mf">0.048</span><span class="w">     </span><span class="mf">0.020</span>
<span class="n">x42</span><span class="w">            </span><span class="mf">0.0217</span><span class="w">      </span><span class="mf">0.017</span><span class="w">      </span><span class="mf">1.257</span><span class="w">      </span><span class="mf">0.209</span><span class="w">        </span><span class="o">-</span><span class="mf">0.012</span><span class="w">     </span><span class="mf">0.056</span>
<span class="n">x43</span><span class="w">            </span><span class="mf">0.0123</span><span class="w">      </span><span class="mf">0.102</span><span class="w">      </span><span class="mf">0.121</span><span class="w">      </span><span class="mf">0.904</span><span class="w">        </span><span class="o">-</span><span class="mf">0.188</span><span class="w">     </span><span class="mf">0.213</span>
<span class="n">x44</span><span class="w">            </span><span class="mf">0.0441</span><span class="w">      </span><span class="mf">0.018</span><span class="w">      </span><span class="mf">2.503</span><span class="w">      </span><span class="mf">0.012</span><span class="w">         </span><span class="mf">0.010</span><span class="w">     </span><span class="mf">0.079</span>
<span class="n">x45</span><span class="w">            </span><span class="mf">0.0406</span><span class="w">      </span><span class="mf">0.018</span><span class="w">      </span><span class="mf">2.308</span><span class="w">      </span><span class="mf">0.021</span><span class="w">         </span><span class="mf">0.006</span><span class="w">     </span><span class="mf">0.075</span>
<span class="n">x46</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0410</span><span class="w">      </span><span class="mf">0.018</span><span class="w">     </span><span class="o">-</span><span class="mf">2.338</span><span class="w">      </span><span class="mf">0.020</span><span class="w">        </span><span class="o">-</span><span class="mf">0.075</span><span class="w">    </span><span class="o">-</span><span class="mf">0.007</span>
<span class="n">x47</span><span class="w">            </span><span class="mf">0.0035</span><span class="w">      </span><span class="mf">0.003</span><span class="w">      </span><span class="mf">1.304</span><span class="w">      </span><span class="mf">0.192</span><span class="w">        </span><span class="o">-</span><span class="mf">0.002</span><span class="w">     </span><span class="mf">0.009</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span><span class="w">                       </span><span class="mf">42.820</span><span class="w">   </span><span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span><span class="w">                   </span><span class="mf">1.966</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span><span class="w">                  </span><span class="mf">0.000</span><span class="w">   </span><span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span><span class="w"> </span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">               </span><span class="mf">54.973</span>
<span class="n">Skew</span><span class="p">:</span><span class="w">                           </span><span class="mf">0.300</span><span class="w">   </span><span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                     </span><span class="mf">1.16e-12</span>
<span class="n">Kurtosis</span><span class="p">:</span><span class="w">                       </span><span class="mf">3.649</span><span class="w">   </span><span class="n">Cond</span><span class="o">.</span><span class="w"> </span><span class="n">No</span><span class="o">.</span><span class="w">                     </span><span class="mf">1.88e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="n">Standard</span><span class="w"> </span><span class="n">Errors</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">covariance</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">errors</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="n">correctly</span><span class="w"> </span><span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="n">large</span><span class="p">,</span><span class="w"> </span><span class="mf">1.88e+05</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">strong</span><span class="w"> </span><span class="n">multicollinearity</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">problems</span><span class="o">.</span>
</code></pre></div>

<p>There's a lot to look at in the regression output (especially with this many features). For an explanation of all the different parts of the regression take a look at this <a href="http://connor-johnson.com/2014/02/18/linear-regression-with-python/">post</a>. Below is a quick plot of predicted ws/48 against actual ws/48.</p>
<div class="highlight"><pre><span></span><code> plt.plot(estAll.predict(X_rookie),Y,&#39;o&#39;)
 plt.plot(np.arange(0,0.25,0.01),np.arange(0,0.25,0.01),&#39;b-&#39;)
 plt.ylabel(&#39;Career WS/48&#39;)
 plt.xlabel(&#39;Predicted WS/48&#39;);
</code></pre></div>

<p><img src="https://danvatterott.com/images/regressionNBA/regression1_predict.png" /></p>
<p>The blue line above is NOT the best-fit line. It's the identity line. I plot it to help visualize where the model fails. The model seems to primarily fail in the extremes - it tends to overestimate the worst players.</p>
<p>All in all, This model does a remarkably good job given its simplicity (linear regression), but it also leaves a lot of variance unexplained.</p>
<p>One reason this model might miss some variance is there's more than one way to be a productive basketball player. For instance, Dwight Howard and Steph Curry find very different ways to contribute. One linear regression model is unlikely to succesfully predict both players.</p>
<p>In a <a href="http://www.danvatterott.com/blog/2016/02/21/grouping-nba-players/">previous post</a>, I grouped players according to their on-court performance. These player groupings might help predict career performance.</p>
<p>Below, I will use the same player grouping I developed in my previous post, and examine how these groupings impact my ability to predict career performance.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="n">StandardScaler</span>

<span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;nba_bballref_career_stats_2016_Mar_15.pkl&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;G&#39;</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">50</span><span class="p">]</span>
<span class="w"> </span><span class="n">df_drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span><span class="s1">&#39;Name&#39;</span><span class="p">,</span><span class="s1">&#39;G&#39;</span><span class="p">,</span><span class="s1">&#39;GS&#39;</span><span class="p">,</span><span class="s1">&#39;MP&#39;</span><span class="p">,</span><span class="s1">&#39;FG&#39;</span><span class="p">,</span><span class="s1">&#39;FGA&#39;</span><span class="p">,</span><span class="s1">&#39;FG%&#39;</span><span class="p">,</span><span class="s1">&#39;3P&#39;</span><span class="p">,</span><span class="s1">&#39;2P&#39;</span><span class="p">,</span><span class="s1">&#39;FT&#39;</span><span class="p">,</span><span class="s1">&#39;TRB&#39;</span><span class="p">,</span><span class="s1">&#39;PTS&#39;</span><span class="p">,</span><span class="s1">&#39;ORtg&#39;</span><span class="p">,</span><span class="s1">&#39;DRtg&#39;</span><span class="p">,</span><span class="s1">&#39;PER&#39;</span><span class="p">,</span><span class="s1">&#39;TS%&#39;</span><span class="p">,</span><span class="s1">&#39;3PAr&#39;</span><span class="p">,</span><span class="s1">&#39;FTr&#39;</span><span class="p">,</span><span class="s1">&#39;ORB%&#39;</span><span class="p">,</span><span class="s1">&#39;DRB%&#39;</span><span class="p">,</span><span class="s1">&#39;TRB%&#39;</span><span class="p">,</span><span class="s1">&#39;AST%&#39;</span><span class="p">,</span><span class="s1">&#39;STL%&#39;</span><span class="p">,</span><span class="s1">&#39;BLK%&#39;</span><span class="p">,</span><span class="s1">&#39;TOV%&#39;</span><span class="p">,</span><span class="s1">&#39;USG%&#39;</span><span class="p">,</span><span class="s1">&#39;OWS&#39;</span><span class="p">,</span><span class="s1">&#39;DWS&#39;</span><span class="p">,</span><span class="s1">&#39;WS&#39;</span><span class="p">,</span><span class="s1">&#39;WS/48&#39;</span><span class="p">,</span><span class="s1">&#39;OBPM&#39;</span><span class="p">,</span><span class="s1">&#39;DBPM&#39;</span><span class="p">,</span><span class="s1">&#39;BPM&#39;</span><span class="p">,</span><span class="s1">&#39;VORP&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
<span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df_drop</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="w"> </span><span class="c1">#take data out of dataframe</span>
<span class="w"> </span><span class="n">ScaleModel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ScaleModel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="w"> </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="n">PCA</span>
<span class="w"> </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="n">KMeans</span>

<span class="w"> </span><span class="n">reduced_model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="w"> </span><span class="n">reduced_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reduced_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="c1">#transform data into the 5 PCA components space</span>
<span class="w"> </span><span class="n">final_fit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">reduced_data</span><span class="p">)</span><span class="w"> </span><span class="c1">#fit 6 clusters</span>
<span class="w"> </span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">final_fit</span><span class="o">.</span><span class="n">labels_</span><span class="w"> </span><span class="c1">#label each data point with its clusters</span>
</code></pre></div>

<p>See my other post for more details about this clustering procedure.</p>
<p>Let's see how WS/48 varies across the groups.</p>
<div class="highlight"><pre><span></span><code> WS_48 = [df[df[&#39;kmeans_label&#39;]==x][&#39;WS/48&#39;] for x in np.unique(df[&#39;kmeans_label&#39;])] #create a vector of ws/48. One for each cluster
 plt.boxplot(WS_48);
</code></pre></div>

<p><img src="https://danvatterott.com/images/regressionNBA/boxwhisk_ws48.png" /></p>
<p>Some groups perform better than others, but there's lots of overlap between the groups. Importantly, each group has a fair amount of variability. Each group spans at least 0.15 WS/48. This gives the regression enough room to successfully predict performance in each group.</p>
<p>Now, lets get a bit of a refresher on what the groups are. Again, my previous post has a good description of these groups.</p>
<div class="highlight"><pre><span></span><code> TS = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;TS%&#39;])*100 for x in np.unique(df[&#39;kmeans_label&#39;])] #create vectors of each stat for each cluster
 ThreeAr = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;3PAr&#39;])*100 for x in np.unique(df[&#39;kmeans_label&#39;])]
 FTr = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;FTr&#39;])*100 for x in np.unique(df[&#39;kmeans_label&#39;])]
 RBD = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;TRB%&#39;]) for x in np.unique(df[&#39;kmeans_label&#39;])]
 AST = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;AST%&#39;]) for x in np.unique(df[&#39;kmeans_label&#39;])]
 STL = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;STL%&#39;]) for x in np.unique(df[&#39;kmeans_label&#39;])]
 TOV = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;TOV%&#39;]) for x in np.unique(df[&#39;kmeans_label&#39;])]
 USG = [np.mean(df[df[&#39;kmeans_label&#39;]==x][&#39;USG%&#39;]) for x in np.unique(df[&#39;kmeans_label&#39;])]

 Data = np.array([TS,ThreeAr,FTr,RBD,AST,STL,TOV,USG])
 ind = np.arange(1,9)

 plt.figure(figsize=(16,8))
 plt.plot(ind,Data,&#39;o-&#39;,linewidth=2)
 plt.xticks(ind,(&#39;True Shooting&#39;, &#39;3 point Attempt&#39;, &#39;Free Throw Rate&#39;, &#39;Rebound&#39;, &#39;Assist&#39;,&#39;Steal&#39;,&#39;TOV&#39;,&#39;Usage&#39;),rotation=45)
 plt.legend((&#39;Group 1&#39;,&#39;Group 2&#39;,&#39;Group 3&#39;,&#39;Group 4&#39;,&#39;Group 5&#39;,&#39;Group 6&#39;))
 plt.ylabel(&#39;Percent&#39;);
</code></pre></div>

<p><img src="https://danvatterott.com/images/regressionNBA/grouping_performance.png" /></p>
<p>I've plotted the groups across a number of useful categories. For information about these categories see <a href="http://www.basketball-reference.com/about/glossary.html">basketball reference's glossary</a>.</p>
<p>Here's a quick rehash of the groupings. See my <a href="http://www.danvatterott.com/blog/2016/02/21/grouping-nba-players/">previous post</a> for more detail.</p>
<ul>
<li>**Group 1:** These are the distributors who shoot a fair number of threes, don't rebound at all, dish out assists, gather steals, and ...turn the ball over.</li>
<li>**Group 2:** These are the scorers who get to the free throw line, dish out assists, and carry a high usage.</li>
<li>**Group 3:** These are the bench players who don't score...or do much in general.</li>
<li>**Group 4:** These are the 3 point shooters who shoot tons of 3 pointers, almost no free throws, and don't rebound well.</li>
<li>**Group 5:** These are the mid-range shooters who shoot well, but don't shoot threes or draw free throws</li>
<li>**Group 6:** These are the defensive big men who shoot no threes, rebound lots, and carry a low usage.</li>
</ul>

<p>On to the regression.</p>
<div class="highlight"><pre><span></span><code> rookie_df = pd.read_pickle(&#39;nba_bballref_rookie_stats_2016_Mar_15.pkl&#39;)
 rookie_df = rookie_df.drop([&#39;Year&#39;,&#39;Career Games&#39;,&#39;Name&#39;],1)

 X = rookie_df.as_matrix() #take data out of dataframe
 ScaleRookie = StandardScaler().fit(X) #scale data
 X = ScaleRookie.transform(X) #transform data to scale

 reduced_model_rookie = PCA(n_components=10).fit(X) #create pca model of first 10 components.
</code></pre></div>

<p>You might have noticed the giant condition number in the regression above. This indicates significant <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a> of the features, which isn't surprising since I have many features that reflect the same abilities.</p>
<p>The multicollinearity doesn't prevent the regression model from making accurate predictions, but does it make the beta weight estimates irratic. With irratic beta weights, it's hard to tell whether the different clusters use different models when predicting career ws/48.</p>
<p>In the following regression, I put the predicting features through a PCA and keep only the first 10 PCA components. Using only the first 10 PCA components keeps the component score below 20, indicating that multicollinearity is not a problem. I then examine whether the different groups exhibit a different patterns of beta weights (whether different models predict success of the different groups).</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="n">cluster_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">df[&#39;Year&#39;</span><span class="o">]&gt;</span><span class="mi">1980</span><span class="err">]</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]</span><span class="w"> </span><span class="n">#limit</span><span class="w"> </span><span class="n">labels</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">players</span><span class="w"> </span><span class="k">after</span><span class="w"> </span><span class="mi">1980</span>
<span class="w"> </span><span class="n">rookie_df_drop</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cluster_labels</span><span class="w"> </span><span class="n">#label</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">clusters</span>

<span class="w"> </span><span class="n">estHold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">[</span><span class="o">]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]]</span>

<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="k">group</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="k">unique</span><span class="p">(</span><span class="n">final_fit</span><span class="p">.</span><span class="n">labels_</span><span class="p">))</span><span class="err">:</span>

<span class="w">     </span><span class="n">Grouper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]==</span><span class="k">group</span><span class="w"> </span><span class="n">#do</span><span class="w"> </span><span class="n">regression</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">group</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nc">time</span>
<span class="w">     </span><span class="n">Yearer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">&#39;Year&#39;</span><span class="o">]&gt;</span><span class="mi">1980</span>

<span class="w">     </span><span class="n">Group1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">Grouper &amp; Yearer</span><span class="o">]</span>
<span class="w">     </span><span class="n">Y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Group1</span><span class="o">[</span><span class="n">&#39;WS/48&#39;</span><span class="o">]</span><span class="w"> </span><span class="n">#get</span><span class="w"> </span><span class="n">predicted</span><span class="w"> </span><span class="k">data</span>

<span class="w">     </span><span class="n">Group1_rookie</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rookie_df_drop</span><span class="o">[</span><span class="n">rookie_df_drop[&#39;kmeans_label&#39;</span><span class="o">]==</span><span class="k">group</span><span class="err">]</span><span class="w"> </span><span class="n">#get</span><span class="w"> </span><span class="n">predictor</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">group</span>
<span class="w">     </span><span class="n">Group1_rookie</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Group1_rookie</span><span class="p">.</span><span class="k">drop</span><span class="p">(</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="w">     </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Group1_rookie</span><span class="p">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="w"> </span><span class="n">#take</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="k">out</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">dataframe</span>
<span class="w">     </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ScaleRookie</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="n">#scale</span><span class="w"> </span><span class="k">data</span>

<span class="w">     </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reduced_model_rookie</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="n">#transform</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="n">PCA</span><span class="w"> </span><span class="n">components</span><span class="w"> </span><span class="nf">space</span>

<span class="w">     </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sm</span><span class="p">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">Adds</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">constant</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predictor</span>
<span class="w">     </span><span class="n">est</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="n">#create</span><span class="w"> </span><span class="n">regression</span><span class="w"> </span><span class="n">model</span>
<span class="w">     </span><span class="n">est</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">est</span><span class="p">.</span><span class="n">fit</span><span class="p">()</span>
<span class="w">     </span><span class="n">#print</span><span class="p">(</span><span class="n">est</span><span class="p">.</span><span class="n">summary</span><span class="p">())</span>
<span class="w">     </span><span class="n">estHold</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">est</span>



<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span><span class="w"> </span><span class="n">#plot</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="n">weights</span>
<span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="mf">0.12</span>
<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="n">est</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">estHold</span><span class="p">)</span><span class="err">:</span>
<span class="w">     </span><span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="o">+</span><span class="n">width</span><span class="o">*</span><span class="n">i</span><span class="p">,</span><span class="n">est</span><span class="p">.</span><span class="n">params</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="o">[</span><span class="n">&#39;axes.color_cycle&#39;</span><span class="o">][</span><span class="n">i</span><span class="o">]</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span><span class="n">yerr</span><span class="o">=</span><span class="p">(</span><span class="n">est</span><span class="p">.</span><span class="n">conf_int</span><span class="p">()</span><span class="o">[</span><span class="n">1</span><span class="o">]-</span><span class="n">est</span><span class="p">.</span><span class="n">conf_int</span><span class="p">()</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="nf">right</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principle Components&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">((</span><span class="s1">&#39;Group 1&#39;</span><span class="p">,</span><span class="s1">&#39;Group 2&#39;</span><span class="p">,</span><span class="s1">&#39;Group 3&#39;</span><span class="p">,</span><span class="s1">&#39;Group 4&#39;</span><span class="p">,</span><span class="s1">&#39;Group 5&#39;</span><span class="p">,</span><span class="s1">&#39;Group 6&#39;</span><span class="p">))</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Beta Weights&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img src="https://danvatterott.com/images/regressionNBA/beta_weights.png" /></p>
<p>Above I plot the beta weights for each principle component across the groupings. This plot is a lot to look at, but I wanted to depict how the beta values changed across the groups. They are not drastically different, but they're also not identical. Error bars depict 95% confidence intervals.</p>
<p>Below I fit a regression to each group, but with all the features. Again, multicollinearity will be a problem, but this will not decrease the regression's accuracy, which is all I really care about.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rookie_df</span><span class="p">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="w"> </span><span class="n">#take</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="k">out</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">dataframe</span>

<span class="w"> </span><span class="n">cluster_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">df[&#39;Year&#39;</span><span class="o">]&gt;</span><span class="mi">1980</span><span class="err">]</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]</span>
<span class="w"> </span><span class="n">rookie_df_drop</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cluster_labels</span><span class="w"> </span><span class="n">#label</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">clusters</span>

<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span>

<span class="w"> </span><span class="n">estHold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">[</span><span class="o">]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]</span><span class="p">,</span><span class="err">[]]</span>

<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="k">group</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="k">unique</span><span class="p">(</span><span class="n">final_fit</span><span class="p">.</span><span class="n">labels_</span><span class="p">))</span><span class="err">:</span>

<span class="w">     </span><span class="n">Grouper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]==</span><span class="k">group</span><span class="w"> </span><span class="n">#do</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">regression</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nc">time</span>
<span class="w">     </span><span class="n">Yearer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">&#39;Year&#39;</span><span class="o">]&gt;</span><span class="mi">1980</span>

<span class="w">     </span><span class="n">Group1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">Grouper &amp; Yearer</span><span class="o">]</span>
<span class="w">     </span><span class="n">Y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Group1</span><span class="o">[</span><span class="n">&#39;WS/48&#39;</span><span class="o">]</span><span class="w"> </span><span class="n">#get</span><span class="w"> </span><span class="n">predictor</span><span class="w"> </span><span class="k">data</span>

<span class="w">     </span><span class="n">Group1_rookie</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rookie_df_drop</span><span class="o">[</span><span class="n">rookie_df_drop[&#39;kmeans_label&#39;</span><span class="o">]==</span><span class="k">group</span><span class="err">]</span>
<span class="w">     </span><span class="n">Group1_rookie</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Group1_rookie</span><span class="p">.</span><span class="k">drop</span><span class="p">(</span><span class="o">[</span><span class="n">&#39;kmeans_label&#39;</span><span class="o">]</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">#get</span><span class="w"> </span><span class="n">predicted</span><span class="w"> </span><span class="k">data</span>

<span class="w">     </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Group1_rookie</span><span class="p">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="w"> </span><span class="n">#take</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="k">out</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">dataframe</span>

<span class="w">     </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sm</span><span class="p">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">Adds</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">constant</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predictor</span>
<span class="w">     </span><span class="n">est</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="n">#fit</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">linear</span><span class="w"> </span><span class="n">regression</span><span class="w"> </span><span class="n">model</span>
<span class="w">     </span><span class="n">est</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">est</span><span class="p">.</span><span class="n">fit</span><span class="p">()</span>
<span class="w">     </span><span class="n">estHold</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">est</span>
<span class="w">     </span><span class="n">#print</span><span class="w"> </span><span class="n">est</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>

<span class="w">     </span><span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">#plot</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">regression</span><span class="s1">&#39;s prediction against actual data</span>
<span class="s1">     plt.plot(est.predict(X),Y,&#39;</span><span class="n">o</span><span class="s1">&#39;,color=plt.rcParams[&#39;</span><span class="n">axes</span><span class="p">.</span><span class="n">color_cycle</span><span class="s1">&#39;][i])</span>
<span class="s1">     plt.plot(np.arange(-0.1,0.25,0.01),np.arange(-0.1,0.25,0.01),&#39;</span><span class="o">-</span><span class="s1">&#39;)</span>
<span class="s1">     plt.title(&#39;</span><span class="k">Group</span><span class="w"> </span><span class="o">%</span><span class="n">d</span><span class="s1">&#39;%(i+1))</span>
<span class="s1">     plt.text(0.15,-0.05,&#39;</span><span class="err">$</span><span class="n">r</span><span class="o">^</span><span class="mi">2</span><span class="err">$</span><span class="o">=%</span><span class="mf">.2</span><span class="n">f</span><span class="err">&#39;</span><span class="o">%</span><span class="n">est</span><span class="p">.</span><span class="n">rsquared</span><span class="p">)</span>
<span class="w">     </span><span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="o">[</span><span class="n">0.0,0.12,0.25</span><span class="o">]</span><span class="p">)</span>
<span class="w">     </span><span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="o">[</span><span class="n">0.0,0.12,0.25</span><span class="o">]</span><span class="p">);</span>
</code></pre></div>

<p><img src="https://danvatterott.com/images/regressionNBA/model2_predictions.png" /></p>
<p>The plots above depict each regression's predictions against actual ws/48. I provide each model's r^2 in the plot too.</p>
<p>Some regressions are better than others. For instance, the regression model does a pretty awesome job predicting the bench warmers...I wonder if this is because they have shorter careers... The regression model does not do a good job predicting the 3-point shooters.</p>
<p>Now onto the fun stuff though.</p>
<p>Below, create a function for predicting a players career WS/48. First, I write a function that finds what cluster a player would belong to, and what the regression model predicts for this players career (with 95% confidence intervals).</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="k">def</span><span class="w"> </span><span class="nf">player_prediction__regressionModel</span><span class="p">(</span><span class="n">PlayerName</span><span class="p">):</span>
<span class="w">     </span><span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.sandbox.regression.predstd</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="n">wls_prediction_std</span>

<span class="w">     </span><span class="n">clust_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;nba_bballref_career_stats_2016_Mar_05.pkl&#39;</span><span class="p">)</span>
<span class="w">     </span><span class="n">clust_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clust_df</span><span class="p">[</span><span class="n">clust_df</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">PlayerName</span><span class="p">]</span>
<span class="w">     </span><span class="n">clust_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clust_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Name&#39;</span><span class="p">,</span><span class="s1">&#39;G&#39;</span><span class="p">,</span><span class="s1">&#39;GS&#39;</span><span class="p">,</span><span class="s1">&#39;MP&#39;</span><span class="p">,</span><span class="s1">&#39;FG&#39;</span><span class="p">,</span><span class="s1">&#39;FGA&#39;</span><span class="p">,</span><span class="s1">&#39;FG%&#39;</span><span class="p">,</span><span class="s1">&#39;3P&#39;</span><span class="p">,</span><span class="s1">&#39;2P&#39;</span><span class="p">,</span><span class="s1">&#39;FT&#39;</span><span class="p">,</span><span class="s1">&#39;TRB&#39;</span><span class="p">,</span><span class="s1">&#39;PTS&#39;</span><span class="p">,</span><span class="s1">&#39;ORtg&#39;</span><span class="p">,</span><span class="s1">&#39;DRtg&#39;</span><span class="p">,</span><span class="s1">&#39;PER&#39;</span><span class="p">,</span><span class="s1">&#39;TS%&#39;</span><span class="p">,</span><span class="s1">&#39;3PAr&#39;</span><span class="p">,</span><span class="s1">&#39;FTr&#39;</span><span class="p">,</span><span class="s1">&#39;ORB%&#39;</span><span class="p">,</span><span class="s1">&#39;DRB%&#39;</span><span class="p">,</span><span class="s1">&#39;TRB%&#39;</span><span class="p">,</span><span class="s1">&#39;AST%&#39;</span><span class="p">,</span><span class="s1">&#39;STL%&#39;</span><span class="p">,</span><span class="s1">&#39;BLK%&#39;</span><span class="p">,</span><span class="s1">&#39;TOV%&#39;</span><span class="p">,</span><span class="s1">&#39;USG%&#39;</span><span class="p">,</span><span class="s1">&#39;OWS&#39;</span><span class="p">,</span><span class="s1">&#39;DWS&#39;</span><span class="p">,</span><span class="s1">&#39;WS&#39;</span><span class="p">,</span><span class="s1">&#39;WS/48&#39;</span><span class="p">,</span><span class="s1">&#39;OBPM&#39;</span><span class="p">,</span><span class="s1">&#39;DBPM&#39;</span><span class="p">,</span><span class="s1">&#39;BPM&#39;</span><span class="p">,</span><span class="s1">&#39;VORP&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
<span class="w">     </span><span class="n">new_vect</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ScaleModel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">clust_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
<span class="w">     </span><span class="n">reduced_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reduced_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_vect</span><span class="p">)</span>
<span class="w">     </span><span class="n">Group</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">final_fit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">reduced_data</span><span class="p">)</span>
<span class="w">     </span><span class="n">clust_df</span><span class="p">[</span><span class="s1">&#39;kmeans_label&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="w">     </span><span class="n">Predrookie_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;nba_bballref_rookie_stats_2016_Mar_15.pkl&#39;</span><span class="p">)</span>
<span class="w">     </span><span class="n">Predrookie_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Predrookie_df</span><span class="p">[</span><span class="n">Predrookie_df</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">PlayerName</span><span class="p">]</span>
<span class="w">     </span><span class="n">Predrookie_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Predrookie_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span><span class="s1">&#39;Career Games&#39;</span><span class="p">,</span><span class="s1">&#39;Name&#39;</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
<span class="w">     </span><span class="n">predX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Predrookie_df</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="w"> </span><span class="c1">#take data out of dataframe</span>
<span class="w">     </span><span class="n">predX</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">predX</span><span class="p">,</span><span class="n">has_constant</span><span class="o">=</span><span class="s1">&#39;add&#39;</span><span class="p">)</span><span class="w">  </span><span class="c1"># Adds a constant term to the predictor</span>
<span class="w">     </span><span class="n">prstd_ols</span><span class="p">,</span><span class="w"> </span><span class="n">iv_l_ols</span><span class="p">,</span><span class="w"> </span><span class="n">iv_u_ols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wls_prediction_std</span><span class="p">(</span><span class="n">estHold</span><span class="p">[</span><span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">predX</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="w">     </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="s1">&#39;Name&#39;</span><span class="p">:</span><span class="n">PlayerName</span><span class="p">,</span><span class="s1">&#39;Group&#39;</span><span class="p">:</span><span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;Prediction&#39;</span><span class="p">:</span><span class="n">estHold</span><span class="p">[</span><span class="n">Group</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predX</span><span class="p">),</span><span class="s1">&#39;Upper_CI&#39;</span><span class="p">:</span><span class="n">iv_u_ols</span><span class="p">,</span><span class="s1">&#39;Lower_CI&#39;</span><span class="p">:</span><span class="n">iv_l_ols</span><span class="p">}</span>
</code></pre></div>

<p>Here I create a function that creates a list of all the first round draft picks from a given year.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="k">def</span><span class="w"> </span><span class="nf">gather_draftData</span><span class="p">(</span><span class="n">Year</span><span class="p">):</span>

<span class="w">     </span><span class="kn">import</span><span class="w"> </span><span class="nn">urllib2</span>
<span class="w">     </span><span class="kn">from</span><span class="w"> </span><span class="nn">bs4</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="n">BeautifulSoup</span>
<span class="w">     </span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="w">     </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="w">     </span><span class="n">draft_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">30</span>

<span class="w">     </span><span class="k">def</span><span class="w"> </span><span class="nf">convert_float</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
<span class="w">         </span><span class="k">try</span><span class="p">:</span>
<span class="w">             </span><span class="k">return</span><span class="w"> </span><span class="nb">float</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="w">         </span><span class="k">except</span><span class="w"> </span><span class="n">ValueError</span><span class="p">:</span>
<span class="w">             </span><span class="k">return</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="w">     </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;http://www.basketball-reference.com/draft/NBA_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">Year</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;.html&#39;</span>
<span class="w">     </span><span class="n">html</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">urllib2</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="w">     </span><span class="n">soup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">html</span><span class="p">,</span><span class="s2">&quot;lxml&quot;</span><span class="p">)</span>

<span class="w">     </span><span class="n">draft_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">soup</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;tbody&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;tr&#39;</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;td&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="n">draft_len</span><span class="p">)]</span>
<span class="w">     </span><span class="n">draft_nam</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">soup</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;tbody&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;tr&#39;</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;td&#39;</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="n">draft_len</span><span class="p">)]</span>

<span class="w">     </span><span class="n">draft_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">draft_num</span><span class="p">,</span><span class="n">draft_nam</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="w">     </span><span class="n">draft_df</span><span class="o">.</span><span class="n">columns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;Number&#39;</span><span class="p">,</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span>
<span class="w">     </span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="w">     </span><span class="k">return</span><span class="w"> </span><span class="n">draft_df</span>
</code></pre></div>

<p>Below I create predictions for each first-round draft pick from 2015. The spurs' first round pick, Nikola Milutinov, has yet to play so I do not create a prediction for him.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.patches</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpatches</span>

<span class="w"> </span><span class="n">draft_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gather_draftData</span><span class="p">(</span><span class="mi">2015</span><span class="p">)</span>

<span class="w"> </span><span class="n">draft_df</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">][</span><span class="mi">14</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="s1">&#39;Kelly Oubre Jr.&#39;</span><span class="w"> </span><span class="c1">#annoying name inconsistencies</span>

<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">31</span><span class="p">),</span><span class="n">draft_df</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">],</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>

<span class="w"> </span><span class="n">draft_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">draft_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="c1">#Sam Dekker has received little playing time making his prediction highly erratic</span>
<span class="w"> </span><span class="n">draft_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">draft_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="c1">#spurs&#39; 1st round pick has not played yet</span>

<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">draft_df</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]:</span>

<span class="w">     </span><span class="n">draft_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">draft_df</span><span class="p">[</span><span class="n">draft_df</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">name</span><span class="p">][</span><span class="s1">&#39;Number&#39;</span><span class="p">]</span>

<span class="w">     </span><span class="n">predict_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">player_prediction__regressionModel</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="w">     </span><span class="n">yerr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">predict_dict</span><span class="p">[</span><span class="s1">&#39;Upper_CI&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">predict_dict</span><span class="p">[</span><span class="s1">&#39;Lower_CI&#39;</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>

<span class="w">     </span><span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">draft_num</span><span class="p">,</span><span class="n">predict_dict</span><span class="p">[</span><span class="s1">&#39;Prediction&#39;</span><span class="p">],</span><span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
<span class="w">                 </span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.color_cycle&#39;</span><span class="p">][</span><span class="n">predict_dict</span><span class="p">[</span><span class="s1">&#39;Group&#39;</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">yerr</span><span class="o">=</span><span class="n">yerr</span><span class="p">);</span>

<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">right</span><span class="o">=</span><span class="mi">31</span><span class="p">)</span>
<span class="w"> </span><span class="n">patch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.color_cycle&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Group </span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">patch</span><span class="p">,</span><span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted WS/48&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Draft Position&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img src="https://danvatterott.com/images/regressionNBA/draft_2015_predict.png" /></p>
<p>The plot above is ordered by draft pick. The error bars depict 95% confidence interbals...which are a little wider than I would like. It's interesting to look at what clusters these players fit into. Lots of 3-pt shooters! It could be that rookies play a limited role in the offense - just shooting 3s.</p>
<p>As a t-wolves fan, I am relatively happy about the high prediction for Karl-Anthony Towns. His predicted ws/48 is between Marc Gasol and Elton Brand. Again, the CIs are quite wide, so the model says there's a 95% chance he is somewhere between Lebron James ever and a player that averages less than 0.1 ws/48.</p>
<p>Karl-Anthony Towns would have the highest predicted ws/48 if it were not for Kevin Looney who the model loves. Kevin Looney has not seen much playing time though, which likely makes his prediction more erratic. Keep in mind I did not use draft position as a predictor in the model.</p>
<p>Sam Dekker has a pretty huge error bar, likely because of his limited playing time this year.</p>
<p>While I fed a ton of features into this model, it's still just a linear regression. The simplicity of the model might prevent me from making more accurate predictions.</p>
<p>I've already started playing with some more complex models. If those work out well, I will post them here. I ended up sticking with a plain linear regression because my vast number of features is a little unwieldy in a more complex models. If you're interested (and the models produce better results) check back in the future.</p>
<p>For now, these models explain between 40 and 70% of the variance in career ws/48 from only a player's rookie year. Even predicting 30% of variance is pretty remarkable, so I don't want to trash on this part of the model. Explaining 65% of the variance is pretty awesome. The model gives us a pretty accurate idea of how these "bench players" will perform. For instance, the future does not look bright for players like Emmanuel Mudiay and Tyus Jones. Not to say these players are doomed. The model assumes that players will retain their grouping for the entire career. Emmanuel Mudiay and Tyus Jones might start performing more like distributors as their career progresses. This could result in a better career.</p>
<p>One nice part about this model is it tells us where the predictions are less confident. For instance, it is nice to know that we're relatively confident when predicting bench players, but not when we're predicting 3-point shooters.</p>
<p>For those curious, I output each groups regression summary below.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">estHold</span><span class="p">];</span>

<span class="w">                            </span><span class="n">OLS</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">Results</span><span class="w">                            </span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span><span class="w"> </span><span class="n">Variable</span><span class="p">:</span><span class="w">                  </span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="w">   </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                       </span><span class="mf">0.648</span>
<span class="n">Model</span><span class="p">:</span><span class="w">                            </span><span class="n">OLS</span><span class="w">   </span><span class="n">Adj</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                  </span><span class="mf">0.575</span>
<span class="n">Method</span><span class="p">:</span><span class="w">                 </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w">   </span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span><span class="w">                     </span><span class="mf">8.939</span>
<span class="n">Date</span><span class="p">:</span><span class="w">                </span><span class="n">Sun</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">Mar</span><span class="w"> </span><span class="mi">2016</span><span class="w">   </span><span class="n">Prob</span><span class="w"> </span><span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span><span class="w">           </span><span class="mf">2.33e-24</span>
<span class="n">Time</span><span class="p">:</span><span class="w">                        </span><span class="mi">10</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">28</span><span class="w">   </span><span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span><span class="w">                 </span><span class="mf">493.16</span>
<span class="n">No</span><span class="o">.</span><span class="w"> </span><span class="n">Observations</span><span class="p">:</span><span class="w">                 </span><span class="mi">212</span><span class="w">   </span><span class="n">AIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">912.3</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Residuals</span><span class="p">:</span><span class="w">                     </span><span class="mi">175</span><span class="w">   </span><span class="n">BIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">788.1</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Model</span><span class="p">:</span><span class="w">                          </span><span class="mi">36</span><span class="w">                                         </span>
<span class="n">Covariance</span><span class="w"> </span><span class="n">Type</span><span class="p">:</span><span class="w">            </span><span class="n">nonrobust</span><span class="w">                                         </span>
<span class="o">==============================================================================</span>
<span class="w">                 </span><span class="n">coef</span><span class="w">    </span><span class="n">std</span><span class="w"> </span><span class="n">err</span><span class="w">          </span><span class="n">t</span><span class="w">      </span><span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="w">      </span><span class="p">[</span><span class="mf">95.0</span><span class="o">%</span><span class="w"> </span><span class="n">Conf</span><span class="o">.</span><span class="w"> </span><span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="k">const</span><span class="w">         </span><span class="o">-</span><span class="mf">0.1072</span><span class="w">      </span><span class="mf">0.064</span><span class="w">     </span><span class="o">-</span><span class="mf">1.682</span><span class="w">      </span><span class="mf">0.094</span><span class="w">        </span><span class="o">-</span><span class="mf">0.233</span><span class="w">     </span><span class="mf">0.019</span>
<span class="n">x1</span><span class="w">             </span><span class="mf">0.0012</span><span class="w">      </span><span class="mf">0.001</span><span class="w">      </span><span class="mf">0.925</span><span class="w">      </span><span class="mf">0.356</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">     </span><span class="mf">0.004</span>
<span class="n">x2</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0005</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">2.355</span><span class="w">      </span><span class="mf">0.020</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w"> </span><span class="o">-</span><span class="mf">7.53e-05</span>
<span class="n">x3</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0005</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">1.899</span><span class="w">      </span><span class="mf">0.059</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">  </span><span class="mf">2.03e-05</span>
<span class="n">x4</span><span class="w">          </span><span class="mf">3.753e-05</span><span class="w">   </span><span class="mf">1.27e-05</span><span class="w">      </span><span class="mf">2.959</span><span class="w">      </span><span class="mf">0.004</span><span class="w">      </span><span class="mf">1.25e-05</span><span class="w">  </span><span class="mf">6.26e-05</span>
<span class="n">x5</span><span class="w">            </span><span class="o">-</span><span class="mf">0.1152</span><span class="w">      </span><span class="mf">0.088</span><span class="w">     </span><span class="o">-</span><span class="mf">1.315</span><span class="w">      </span><span class="mf">0.190</span><span class="w">        </span><span class="o">-</span><span class="mf">0.288</span><span class="w">     </span><span class="mf">0.058</span>
<span class="n">x6</span><span class="w">             </span><span class="mf">0.0240</span><span class="w">      </span><span class="mf">0.053</span><span class="w">      </span><span class="mf">0.456</span><span class="w">      </span><span class="mf">0.649</span><span class="w">        </span><span class="o">-</span><span class="mf">0.080</span><span class="w">     </span><span class="mf">0.128</span>
<span class="n">x7</span><span class="w">            </span><span class="o">-</span><span class="mf">0.4318</span><span class="w">      </span><span class="mf">0.372</span><span class="w">     </span><span class="o">-</span><span class="mf">1.159</span><span class="w">      </span><span class="mf">0.248</span><span class="w">        </span><span class="o">-</span><span class="mf">1.167</span><span class="w">     </span><span class="mf">0.303</span>
<span class="n">x8</span><span class="w">             </span><span class="mf">0.0089</span><span class="w">      </span><span class="mf">0.085</span><span class="w">      </span><span class="mf">0.105</span><span class="w">      </span><span class="mf">0.917</span><span class="w">        </span><span class="o">-</span><span class="mf">0.159</span><span class="w">     </span><span class="mf">0.177</span>
<span class="n">x9</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0479</span><span class="w">      </span><span class="mf">0.054</span><span class="w">     </span><span class="o">-</span><span class="mf">0.893</span><span class="w">      </span><span class="mf">0.373</span><span class="w">        </span><span class="o">-</span><span class="mf">0.154</span><span class="w">     </span><span class="mf">0.058</span>
<span class="n">x10</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0055</span><span class="w">      </span><span class="mf">0.021</span><span class="w">     </span><span class="o">-</span><span class="mf">0.265</span><span class="w">      </span><span class="mf">0.792</span><span class="w">        </span><span class="o">-</span><span class="mf">0.046</span><span class="w">     </span><span class="mf">0.035</span>
<span class="n">x11</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0011</span><span class="w">      </span><span class="mf">0.076</span><span class="w">     </span><span class="o">-</span><span class="mf">0.015</span><span class="w">      </span><span class="mf">0.988</span><span class="w">        </span><span class="o">-</span><span class="mf">0.152</span><span class="w">     </span><span class="mf">0.149</span>
<span class="n">x12</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0301</span><span class="w">      </span><span class="mf">0.053</span><span class="w">     </span><span class="o">-</span><span class="mf">0.569</span><span class="w">      </span><span class="mf">0.570</span><span class="w">        </span><span class="o">-</span><span class="mf">0.134</span><span class="w">     </span><span class="mf">0.074</span>
<span class="n">x13</span><span class="w">            </span><span class="mf">0.7814</span><span class="w">      </span><span class="mf">0.270</span><span class="w">      </span><span class="mf">2.895</span><span class="w">      </span><span class="mf">0.004</span><span class="w">         </span><span class="mf">0.249</span><span class="w">     </span><span class="mf">1.314</span>
<span class="n">x14</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0323</span><span class="w">      </span><span class="mf">0.028</span><span class="w">     </span><span class="o">-</span><span class="mf">1.159</span><span class="w">      </span><span class="mf">0.248</span><span class="w">        </span><span class="o">-</span><span class="mf">0.087</span><span class="w">     </span><span class="mf">0.023</span>
<span class="n">x15</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0108</span><span class="w">      </span><span class="mf">0.007</span><span class="w">     </span><span class="o">-</span><span class="mf">1.451</span><span class="w">      </span><span class="mf">0.149</span><span class="w">        </span><span class="o">-</span><span class="mf">0.025</span><span class="w">     </span><span class="mf">0.004</span>
<span class="n">x16</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0202</span><span class="w">      </span><span class="mf">0.030</span><span class="w">     </span><span class="o">-</span><span class="mf">0.676</span><span class="w">      </span><span class="mf">0.500</span><span class="w">        </span><span class="o">-</span><span class="mf">0.079</span><span class="w">     </span><span class="mf">0.039</span>
<span class="n">x17</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0461</span><span class="w">      </span><span class="mf">0.039</span><span class="w">     </span><span class="o">-</span><span class="mf">1.172</span><span class="w">      </span><span class="mf">0.243</span><span class="w">        </span><span class="o">-</span><span class="mf">0.124</span><span class="w">     </span><span class="mf">0.032</span>
<span class="n">x18</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0178</span><span class="w">      </span><span class="mf">0.040</span><span class="w">     </span><span class="o">-</span><span class="mf">0.443</span><span class="w">      </span><span class="mf">0.659</span><span class="w">        </span><span class="o">-</span><span class="mf">0.097</span><span class="w">     </span><span class="mf">0.062</span>
<span class="n">x19</span><span class="w">            </span><span class="mf">0.0450</span><span class="w">      </span><span class="mf">0.038</span><span class="w">      </span><span class="mf">1.178</span><span class="w">      </span><span class="mf">0.240</span><span class="w">        </span><span class="o">-</span><span class="mf">0.030</span><span class="w">     </span><span class="mf">0.121</span>
<span class="n">x20</span><span class="w">            </span><span class="mf">0.0354</span><span class="w">      </span><span class="mf">0.014</span><span class="w">      </span><span class="mf">2.527</span><span class="w">      </span><span class="mf">0.012</span><span class="w">         </span><span class="mf">0.008</span><span class="w">     </span><span class="mf">0.063</span>
<span class="n">x21</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0418</span><span class="w">      </span><span class="mf">0.044</span><span class="w">     </span><span class="o">-</span><span class="mf">0.947</span><span class="w">      </span><span class="mf">0.345</span><span class="w">        </span><span class="o">-</span><span class="mf">0.129</span><span class="w">     </span><span class="mf">0.045</span>
<span class="n">x22</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0224</span><span class="w">      </span><span class="mf">0.015</span><span class="w">     </span><span class="o">-</span><span class="mf">1.448</span><span class="w">      </span><span class="mf">0.150</span><span class="w">        </span><span class="o">-</span><span class="mf">0.053</span><span class="w">     </span><span class="mf">0.008</span>
<span class="n">x23</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0158</span><span class="w">      </span><span class="mf">0.008</span><span class="w">     </span><span class="o">-</span><span class="mf">2.039</span><span class="w">      </span><span class="mf">0.043</span><span class="w">        </span><span class="o">-</span><span class="mf">0.031</span><span class="w">    </span><span class="o">-</span><span class="mf">0.001</span>
<span class="n">x24</span><span class="w">            </span><span class="mf">0.0058</span><span class="w">      </span><span class="mf">0.001</span><span class="w">      </span><span class="mf">4.261</span><span class="w">      </span><span class="mf">0.000</span><span class="w">         </span><span class="mf">0.003</span><span class="w">     </span><span class="mf">0.009</span>
<span class="n">x25</span><span class="w">            </span><span class="mf">0.0577</span><span class="w">      </span><span class="mf">0.027</span><span class="w">      </span><span class="mf">2.112</span><span class="w">      </span><span class="mf">0.036</span><span class="w">         </span><span class="mf">0.004</span><span class="w">     </span><span class="mf">0.112</span>
<span class="n">x26</span><span class="w">           </span><span class="o">-</span><span class="mf">0.1913</span><span class="w">      </span><span class="mf">0.267</span><span class="w">     </span><span class="o">-</span><span class="mf">0.718</span><span class="w">      </span><span class="mf">0.474</span><span class="w">        </span><span class="o">-</span><span class="mf">0.717</span><span class="w">     </span><span class="mf">0.335</span>
<span class="n">x27</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0050</span><span class="w">      </span><span class="mf">0.093</span><span class="w">     </span><span class="o">-</span><span class="mf">0.054</span><span class="w">      </span><span class="mf">0.957</span><span class="w">        </span><span class="o">-</span><span class="mf">0.189</span><span class="w">     </span><span class="mf">0.179</span>
<span class="n">x28</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0133</span><span class="w">      </span><span class="mf">0.039</span><span class="w">     </span><span class="o">-</span><span class="mf">0.344</span><span class="w">      </span><span class="mf">0.731</span><span class="w">        </span><span class="o">-</span><span class="mf">0.090</span><span class="w">     </span><span class="mf">0.063</span>
<span class="n">x29</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0071</span><span class="w">      </span><span class="mf">0.015</span><span class="w">     </span><span class="o">-</span><span class="mf">0.480</span><span class="w">      </span><span class="mf">0.632</span><span class="w">        </span><span class="o">-</span><span class="mf">0.036</span><span class="w">     </span><span class="mf">0.022</span>
<span class="n">x30</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0190</span><span class="w">      </span><span class="mf">0.010</span><span class="w">     </span><span class="o">-</span><span class="mf">1.973</span><span class="w">      </span><span class="mf">0.050</span><span class="w">        </span><span class="o">-</span><span class="mf">0.038</span><span class="w">  </span><span class="mf">5.68e-06</span>
<span class="n">x31</span><span class="w">            </span><span class="mf">0.0221</span><span class="w">      </span><span class="mf">0.023</span><span class="w">      </span><span class="mf">0.951</span><span class="w">      </span><span class="mf">0.343</span><span class="w">        </span><span class="o">-</span><span class="mf">0.024</span><span class="w">     </span><span class="mf">0.068</span>
<span class="n">x32</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0083</span><span class="w">      </span><span class="mf">0.003</span><span class="w">     </span><span class="o">-</span><span class="mf">2.490</span><span class="w">      </span><span class="mf">0.014</span><span class="w">        </span><span class="o">-</span><span class="mf">0.015</span><span class="w">    </span><span class="o">-</span><span class="mf">0.002</span>
<span class="n">x33</span><span class="w">            </span><span class="mf">0.0386</span><span class="w">      </span><span class="mf">0.031</span><span class="w">      </span><span class="mf">1.259</span><span class="w">      </span><span class="mf">0.210</span><span class="w">        </span><span class="o">-</span><span class="mf">0.022</span><span class="w">     </span><span class="mf">0.099</span>
<span class="n">x34</span><span class="w">            </span><span class="mf">0.0153</span><span class="w">      </span><span class="mf">0.008</span><span class="w">      </span><span class="mf">1.819</span><span class="w">      </span><span class="mf">0.071</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">     </span><span class="mf">0.032</span>
<span class="n">x35</span><span class="w">        </span><span class="o">-</span><span class="mf">1.734e-05</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">0.014</span><span class="w">      </span><span class="mf">0.989</span><span class="w">        </span><span class="o">-</span><span class="mf">0.002</span><span class="w">     </span><span class="mf">0.002</span>
<span class="n">x36</span><span class="w">            </span><span class="mf">0.0033</span><span class="w">      </span><span class="mf">0.004</span><span class="w">      </span><span class="mf">0.895</span><span class="w">      </span><span class="mf">0.372</span><span class="w">        </span><span class="o">-</span><span class="mf">0.004</span><span class="w">     </span><span class="mf">0.011</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span><span class="w">                        </span><span class="mf">2.457</span><span class="w">   </span><span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span><span class="w">                   </span><span class="mf">2.144</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span><span class="w">                  </span><span class="mf">0.293</span><span class="w">   </span><span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span><span class="w"> </span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                </span><span class="mf">2.475</span>
<span class="n">Skew</span><span class="p">:</span><span class="w">                           </span><span class="mf">0.007</span><span class="w">   </span><span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                        </span><span class="mf">0.290</span>
<span class="n">Kurtosis</span><span class="p">:</span><span class="w">                       </span><span class="mf">3.529</span><span class="w">   </span><span class="n">Cond</span><span class="o">.</span><span class="w"> </span><span class="n">No</span><span class="o">.</span><span class="w">                     </span><span class="mf">1.78e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="n">Standard</span><span class="w"> </span><span class="n">Errors</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">covariance</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">errors</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correctly</span><span class="w"> </span><span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">large</span><span class="p">,</span><span class="w"> </span><span class="mf">1.78e+05</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">strong</span><span class="w"> </span><span class="n">multicollinearity</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">problems</span><span class="o">.</span>
<span class="w">                            </span><span class="n">OLS</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">Results</span><span class="w">                            </span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span><span class="w"> </span><span class="n">Variable</span><span class="p">:</span><span class="w">                  </span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="w">   </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                       </span><span class="mf">0.443</span>
<span class="n">Model</span><span class="p">:</span><span class="w">                            </span><span class="n">OLS</span><span class="w">   </span><span class="n">Adj</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                  </span><span class="mf">0.340</span>
<span class="n">Method</span><span class="p">:</span><span class="w">                 </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w">   </span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span><span class="w">                     </span><span class="mf">4.307</span>
<span class="n">Date</span><span class="p">:</span><span class="w">                </span><span class="n">Sun</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">Mar</span><span class="w"> </span><span class="mi">2016</span><span class="w">   </span><span class="n">Prob</span><span class="w"> </span><span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span><span class="w">           </span><span class="mf">1.67e-11</span>
<span class="n">Time</span><span class="p">:</span><span class="w">                        </span><span class="mi">10</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">28</span><span class="w">   </span><span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span><span class="w">                 </span><span class="mf">447.99</span>
<span class="n">No</span><span class="o">.</span><span class="w"> </span><span class="n">Observations</span><span class="p">:</span><span class="w">                 </span><span class="mi">232</span><span class="w">   </span><span class="n">AIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">822.0</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Residuals</span><span class="p">:</span><span class="w">                     </span><span class="mi">195</span><span class="w">   </span><span class="n">BIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">694.4</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Model</span><span class="p">:</span><span class="w">                          </span><span class="mi">36</span><span class="w">                                         </span>
<span class="n">Covariance</span><span class="w"> </span><span class="n">Type</span><span class="p">:</span><span class="w">            </span><span class="n">nonrobust</span><span class="w">                                         </span>
<span class="o">==============================================================================</span>
<span class="w">                 </span><span class="n">coef</span><span class="w">    </span><span class="n">std</span><span class="w"> </span><span class="n">err</span><span class="w">          </span><span class="n">t</span><span class="w">      </span><span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="w">      </span><span class="p">[</span><span class="mf">95.0</span><span class="o">%</span><span class="w"> </span><span class="n">Conf</span><span class="o">.</span><span class="w"> </span><span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="k">const</span><span class="w">         </span><span class="o">-</span><span class="mf">0.0532</span><span class="w">      </span><span class="mf">0.090</span><span class="w">     </span><span class="o">-</span><span class="mf">0.594</span><span class="w">      </span><span class="mf">0.553</span><span class="w">        </span><span class="o">-</span><span class="mf">0.230</span><span class="w">     </span><span class="mf">0.124</span>
<span class="n">x1</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0020</span><span class="w">      </span><span class="mf">0.002</span><span class="w">     </span><span class="o">-</span><span class="mf">1.186</span><span class="w">      </span><span class="mf">0.237</span><span class="w">        </span><span class="o">-</span><span class="mf">0.005</span><span class="w">     </span><span class="mf">0.001</span>
<span class="n">x2</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0006</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">1.957</span><span class="w">      </span><span class="mf">0.052</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">  </span><span class="mf">4.47e-06</span>
<span class="n">x3</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0007</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">2.559</span><span class="w">      </span><span class="mf">0.011</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">    </span><span class="o">-</span><span class="mf">0.000</span>
<span class="n">x4</span><span class="w">          </span><span class="mf">5.589e-05</span><span class="w">   </span><span class="mf">1.39e-05</span><span class="w">      </span><span class="mf">4.012</span><span class="w">      </span><span class="mf">0.000</span><span class="w">      </span><span class="mf">2.84e-05</span><span class="w">  </span><span class="mf">8.34e-05</span>
<span class="n">x5</span><span class="w">             </span><span class="mf">0.0386</span><span class="w">      </span><span class="mf">0.093</span><span class="w">      </span><span class="mf">0.414</span><span class="w">      </span><span class="mf">0.679</span><span class="w">        </span><span class="o">-</span><span class="mf">0.145</span><span class="w">     </span><span class="mf">0.222</span>
<span class="n">x6</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0721</span><span class="w">      </span><span class="mf">0.051</span><span class="w">     </span><span class="o">-</span><span class="mf">1.407</span><span class="w">      </span><span class="mf">0.161</span><span class="w">        </span><span class="o">-</span><span class="mf">0.173</span><span class="w">     </span><span class="mf">0.029</span>
<span class="n">x7</span><span class="w">            </span><span class="o">-</span><span class="mf">0.6259</span><span class="w">      </span><span class="mf">0.571</span><span class="w">     </span><span class="o">-</span><span class="mf">1.097</span><span class="w">      </span><span class="mf">0.274</span><span class="w">        </span><span class="o">-</span><span class="mf">1.751</span><span class="w">     </span><span class="mf">0.499</span>
<span class="n">x8</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0653</span><span class="w">      </span><span class="mf">0.079</span><span class="w">     </span><span class="o">-</span><span class="mf">0.822</span><span class="w">      </span><span class="mf">0.412</span><span class="w">        </span><span class="o">-</span><span class="mf">0.222</span><span class="w">     </span><span class="mf">0.091</span>
<span class="n">x9</span><span class="w">             </span><span class="mf">0.0756</span><span class="w">      </span><span class="mf">0.051</span><span class="w">      </span><span class="mf">1.485</span><span class="w">      </span><span class="mf">0.139</span><span class="w">        </span><span class="o">-</span><span class="mf">0.025</span><span class="w">     </span><span class="mf">0.176</span>
<span class="n">x10</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0046</span><span class="w">      </span><span class="mf">0.031</span><span class="w">     </span><span class="o">-</span><span class="mf">0.149</span><span class="w">      </span><span class="mf">0.881</span><span class="w">        </span><span class="o">-</span><span class="mf">0.066</span><span class="w">     </span><span class="mf">0.057</span>
<span class="n">x11</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0365</span><span class="w">      </span><span class="mf">0.066</span><span class="w">     </span><span class="o">-</span><span class="mf">0.554</span><span class="w">      </span><span class="mf">0.580</span><span class="w">        </span><span class="o">-</span><span class="mf">0.166</span><span class="w">     </span><span class="mf">0.093</span>
<span class="n">x12</span><span class="w">            </span><span class="mf">0.0679</span><span class="w">      </span><span class="mf">0.051</span><span class="w">      </span><span class="mf">1.332</span><span class="w">      </span><span class="mf">0.185</span><span class="w">        </span><span class="o">-</span><span class="mf">0.033</span><span class="w">     </span><span class="mf">0.169</span>
<span class="n">x13</span><span class="w">            </span><span class="mf">0.0319</span><span class="w">      </span><span class="mf">0.183</span><span class="w">      </span><span class="mf">0.174</span><span class="w">      </span><span class="mf">0.862</span><span class="w">        </span><span class="o">-</span><span class="mf">0.329</span><span class="w">     </span><span class="mf">0.393</span>
<span class="n">x14</span><span class="w">            </span><span class="mf">0.0106</span><span class="w">      </span><span class="mf">0.040</span><span class="w">      </span><span class="mf">0.262</span><span class="w">      </span><span class="mf">0.793</span><span class="w">        </span><span class="o">-</span><span class="mf">0.069</span><span class="w">     </span><span class="mf">0.090</span>
<span class="n">x15</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0232</span><span class="w">      </span><span class="mf">0.017</span><span class="w">     </span><span class="o">-</span><span class="mf">1.357</span><span class="w">      </span><span class="mf">0.176</span><span class="w">        </span><span class="o">-</span><span class="mf">0.057</span><span class="w">     </span><span class="mf">0.011</span>
<span class="n">x16</span><span class="w">           </span><span class="o">-</span><span class="mf">0.1121</span><span class="w">      </span><span class="mf">0.039</span><span class="w">     </span><span class="o">-</span><span class="mf">2.869</span><span class="w">      </span><span class="mf">0.005</span><span class="w">        </span><span class="o">-</span><span class="mf">0.189</span><span class="w">    </span><span class="o">-</span><span class="mf">0.035</span>
<span class="n">x17</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0675</span><span class="w">      </span><span class="mf">0.060</span><span class="w">     </span><span class="o">-</span><span class="mf">1.134</span><span class="w">      </span><span class="mf">0.258</span><span class="w">        </span><span class="o">-</span><span class="mf">0.185</span><span class="w">     </span><span class="mf">0.050</span>
<span class="n">x18</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0314</span><span class="w">      </span><span class="mf">0.059</span><span class="w">     </span><span class="o">-</span><span class="mf">0.536</span><span class="w">      </span><span class="mf">0.593</span><span class="w">        </span><span class="o">-</span><span class="mf">0.147</span><span class="w">     </span><span class="mf">0.084</span>
<span class="n">x19</span><span class="w">            </span><span class="mf">0.0266</span><span class="w">      </span><span class="mf">0.055</span><span class="w">      </span><span class="mf">0.487</span><span class="w">      </span><span class="mf">0.627</span><span class="w">        </span><span class="o">-</span><span class="mf">0.081</span><span class="w">     </span><span class="mf">0.134</span>
<span class="n">x20</span><span class="w">            </span><span class="mf">0.0259</span><span class="w">      </span><span class="mf">0.009</span><span class="w">      </span><span class="mf">2.827</span><span class="w">      </span><span class="mf">0.005</span><span class="w">         </span><span class="mf">0.008</span><span class="w">     </span><span class="mf">0.044</span>
<span class="n">x21</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0155</span><span class="w">      </span><span class="mf">0.050</span><span class="w">     </span><span class="o">-</span><span class="mf">0.307</span><span class="w">      </span><span class="mf">0.759</span><span class="w">        </span><span class="o">-</span><span class="mf">0.115</span><span class="w">     </span><span class="mf">0.084</span>
<span class="n">x22</span><span class="w">            </span><span class="mf">0.1170</span><span class="w">      </span><span class="mf">0.051</span><span class="w">      </span><span class="mf">2.281</span><span class="w">      </span><span class="mf">0.024</span><span class="w">         </span><span class="mf">0.016</span><span class="w">     </span><span class="mf">0.218</span>
<span class="n">x23</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0157</span><span class="w">      </span><span class="mf">0.014</span><span class="w">     </span><span class="o">-</span><span class="mf">1.102</span><span class="w">      </span><span class="mf">0.272</span><span class="w">        </span><span class="o">-</span><span class="mf">0.044</span><span class="w">     </span><span class="mf">0.012</span>
<span class="n">x24</span><span class="w">            </span><span class="mf">0.0021</span><span class="w">      </span><span class="mf">0.003</span><span class="w">      </span><span class="mf">0.732</span><span class="w">      </span><span class="mf">0.465</span><span class="w">        </span><span class="o">-</span><span class="mf">0.003</span><span class="w">     </span><span class="mf">0.008</span>
<span class="n">x25</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0012</span><span class="w">      </span><span class="mf">0.038</span><span class="w">     </span><span class="o">-</span><span class="mf">0.032</span><span class="w">      </span><span class="mf">0.974</span><span class="w">        </span><span class="o">-</span><span class="mf">0.077</span><span class="w">     </span><span class="mf">0.075</span>
<span class="n">x26</span><span class="w">            </span><span class="mf">0.8379</span><span class="w">      </span><span class="mf">0.524</span><span class="w">      </span><span class="mf">1.599</span><span class="w">      </span><span class="mf">0.111</span><span class="w">        </span><span class="o">-</span><span class="mf">0.196</span><span class="w">     </span><span class="mf">1.871</span>
<span class="n">x27</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0511</span><span class="w">      </span><span class="mf">0.113</span><span class="w">     </span><span class="o">-</span><span class="mf">0.454</span><span class="w">      </span><span class="mf">0.651</span><span class="w">        </span><span class="o">-</span><span class="mf">0.273</span><span class="w">     </span><span class="mf">0.171</span>
<span class="n">x28</span><span class="w">            </span><span class="mf">0.0944</span><span class="w">      </span><span class="mf">0.111</span><span class="w">      </span><span class="mf">0.852</span><span class="w">      </span><span class="mf">0.395</span><span class="w">        </span><span class="o">-</span><span class="mf">0.124</span><span class="w">     </span><span class="mf">0.313</span>
<span class="n">x29</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0018</span><span class="w">      </span><span class="mf">0.029</span><span class="w">     </span><span class="o">-</span><span class="mf">0.061</span><span class="w">      </span><span class="mf">0.951</span><span class="w">        </span><span class="o">-</span><span class="mf">0.059</span><span class="w">     </span><span class="mf">0.055</span>
<span class="n">x30</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0167</span><span class="w">      </span><span class="mf">0.017</span><span class="w">     </span><span class="o">-</span><span class="mf">0.969</span><span class="w">      </span><span class="mf">0.334</span><span class="w">        </span><span class="o">-</span><span class="mf">0.051</span><span class="w">     </span><span class="mf">0.017</span>
<span class="n">x31</span><span class="w">            </span><span class="mf">0.0377</span><span class="w">      </span><span class="mf">0.044</span><span class="w">      </span><span class="mf">0.854</span><span class="w">      </span><span class="mf">0.394</span><span class="w">        </span><span class="o">-</span><span class="mf">0.049</span><span class="w">     </span><span class="mf">0.125</span>
<span class="n">x32</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0052</span><span class="w">      </span><span class="mf">0.002</span><span class="w">     </span><span class="o">-</span><span class="mf">2.281</span><span class="w">      </span><span class="mf">0.024</span><span class="w">        </span><span class="o">-</span><span class="mf">0.010</span><span class="w">    </span><span class="o">-</span><span class="mf">0.001</span>
<span class="n">x33</span><span class="w">            </span><span class="mf">0.0132</span><span class="w">      </span><span class="mf">0.037</span><span class="w">      </span><span class="mf">0.360</span><span class="w">      </span><span class="mf">0.719</span><span class="w">        </span><span class="o">-</span><span class="mf">0.059</span><span class="w">     </span><span class="mf">0.086</span>
<span class="n">x34</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0650</span><span class="w">      </span><span class="mf">0.028</span><span class="w">     </span><span class="o">-</span><span class="mf">2.356</span><span class="w">      </span><span class="mf">0.019</span><span class="w">        </span><span class="o">-</span><span class="mf">0.119</span><span class="w">    </span><span class="o">-</span><span class="mf">0.011</span>
<span class="n">x35</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0012</span><span class="w">      </span><span class="mf">0.002</span><span class="w">     </span><span class="o">-</span><span class="mf">0.668</span><span class="w">      </span><span class="mf">0.505</span><span class="w">        </span><span class="o">-</span><span class="mf">0.005</span><span class="w">     </span><span class="mf">0.002</span>
<span class="n">x36</span><span class="w">            </span><span class="mf">0.0087</span><span class="w">      </span><span class="mf">0.008</span><span class="w">      </span><span class="mf">1.107</span><span class="w">      </span><span class="mf">0.270</span><span class="w">        </span><span class="o">-</span><span class="mf">0.007</span><span class="w">     </span><span class="mf">0.024</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span><span class="w">                        </span><span class="mf">2.161</span><span class="w">   </span><span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span><span class="w">                   </span><span class="mf">2.000</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span><span class="w">                  </span><span class="mf">0.339</span><span class="w">   </span><span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span><span class="w"> </span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                </span><span class="mf">1.942</span>
<span class="n">Skew</span><span class="p">:</span><span class="w">                           </span><span class="mf">0.222</span><span class="w">   </span><span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                        </span><span class="mf">0.379</span>
<span class="n">Kurtosis</span><span class="p">:</span><span class="w">                       </span><span class="mf">3.067</span><span class="w">   </span><span class="n">Cond</span><span class="o">.</span><span class="w"> </span><span class="n">No</span><span class="o">.</span><span class="w">                     </span><span class="mf">3.94e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="n">Standard</span><span class="w"> </span><span class="n">Errors</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">covariance</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">errors</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correctly</span><span class="w"> </span><span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">large</span><span class="p">,</span><span class="w"> </span><span class="mf">3.94e+05</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">strong</span><span class="w"> </span><span class="n">multicollinearity</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">problems</span><span class="o">.</span>
<span class="w">                            </span><span class="n">OLS</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">Results</span><span class="w">                            </span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span><span class="w"> </span><span class="n">Variable</span><span class="p">:</span><span class="w">                  </span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="w">   </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                       </span><span class="mf">0.358</span>
<span class="n">Model</span><span class="p">:</span><span class="w">                            </span><span class="n">OLS</span><span class="w">   </span><span class="n">Adj</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                  </span><span class="mf">0.270</span>
<span class="n">Method</span><span class="p">:</span><span class="w">                 </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w">   </span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span><span class="w">                     </span><span class="mf">4.050</span>
<span class="n">Date</span><span class="p">:</span><span class="w">                </span><span class="n">Sun</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">Mar</span><span class="w"> </span><span class="mi">2016</span><span class="w">   </span><span class="n">Prob</span><span class="w"> </span><span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span><span class="w">           </span><span class="mf">1.93e-11</span>
<span class="n">Time</span><span class="p">:</span><span class="w">                        </span><span class="mi">10</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">28</span><span class="w">   </span><span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span><span class="w">                 </span><span class="mf">645.12</span>
<span class="n">No</span><span class="o">.</span><span class="w"> </span><span class="n">Observations</span><span class="p">:</span><span class="w">                 </span><span class="mi">298</span><span class="w">   </span><span class="n">AIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">1216.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Residuals</span><span class="p">:</span><span class="w">                     </span><span class="mi">261</span><span class="w">   </span><span class="n">BIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">1079.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Model</span><span class="p">:</span><span class="w">                          </span><span class="mi">36</span><span class="w">                                         </span>
<span class="n">Covariance</span><span class="w"> </span><span class="n">Type</span><span class="p">:</span><span class="w">            </span><span class="n">nonrobust</span><span class="w">                                         </span>
<span class="o">==============================================================================</span>
<span class="w">                 </span><span class="n">coef</span><span class="w">    </span><span class="n">std</span><span class="w"> </span><span class="n">err</span><span class="w">          </span><span class="n">t</span><span class="w">      </span><span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="w">      </span><span class="p">[</span><span class="mf">95.0</span><span class="o">%</span><span class="w"> </span><span class="n">Conf</span><span class="o">.</span><span class="w"> </span><span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="k">const</span><span class="w">          </span><span class="mf">0.0306</span><span class="w">      </span><span class="mf">0.040</span><span class="w">      </span><span class="mf">0.763</span><span class="w">      </span><span class="mf">0.446</span><span class="w">        </span><span class="o">-</span><span class="mf">0.048</span><span class="w">     </span><span class="mf">0.110</span>
<span class="n">x1</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0013</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">1.278</span><span class="w">      </span><span class="mf">0.202</span><span class="w">        </span><span class="o">-</span><span class="mf">0.003</span><span class="w">     </span><span class="mf">0.001</span>
<span class="n">x2</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0003</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">1.889</span><span class="w">      </span><span class="mf">0.060</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">  </span><span class="mf">1.39e-05</span>
<span class="n">x3</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0002</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">1.196</span><span class="w">      </span><span class="mf">0.233</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">     </span><span class="mf">0.000</span>
<span class="n">x4</span><span class="w">          </span><span class="mf">2.388e-05</span><span class="w">   </span><span class="mf">8.83e-06</span><span class="w">      </span><span class="mf">2.705</span><span class="w">      </span><span class="mf">0.007</span><span class="w">       </span><span class="mf">6.5e-06</span><span class="w">  </span><span class="mf">4.13e-05</span>
<span class="n">x5</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0643</span><span class="w">      </span><span class="mf">0.089</span><span class="w">     </span><span class="o">-</span><span class="mf">0.724</span><span class="w">      </span><span class="mf">0.470</span><span class="w">        </span><span class="o">-</span><span class="mf">0.239</span><span class="w">     </span><span class="mf">0.111</span>
<span class="n">x6</span><span class="w">             </span><span class="mf">0.0131</span><span class="w">      </span><span class="mf">0.046</span><span class="w">      </span><span class="mf">0.286</span><span class="w">      </span><span class="mf">0.775</span><span class="w">        </span><span class="o">-</span><span class="mf">0.077</span><span class="w">     </span><span class="mf">0.103</span>
<span class="n">x7</span><span class="w">            </span><span class="o">-</span><span class="mf">0.4703</span><span class="w">      </span><span class="mf">0.455</span><span class="w">     </span><span class="o">-</span><span class="mf">1.034</span><span class="w">      </span><span class="mf">0.302</span><span class="w">        </span><span class="o">-</span><span class="mf">1.366</span><span class="w">     </span><span class="mf">0.426</span>
<span class="n">x8</span><span class="w">             </span><span class="mf">0.0194</span><span class="w">      </span><span class="mf">0.089</span><span class="w">      </span><span class="mf">0.219</span><span class="w">      </span><span class="mf">0.827</span><span class="w">        </span><span class="o">-</span><span class="mf">0.155</span><span class="w">     </span><span class="mf">0.194</span>
<span class="n">x9</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0330</span><span class="w">      </span><span class="mf">0.052</span><span class="w">     </span><span class="o">-</span><span class="mf">0.638</span><span class="w">      </span><span class="mf">0.524</span><span class="w">        </span><span class="o">-</span><span class="mf">0.135</span><span class="w">     </span><span class="mf">0.069</span>
<span class="n">x10</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0221</span><span class="w">      </span><span class="mf">0.013</span><span class="w">     </span><span class="o">-</span><span class="mf">1.754</span><span class="w">      </span><span class="mf">0.081</span><span class="w">        </span><span class="o">-</span><span class="mf">0.047</span><span class="w">     </span><span class="mf">0.003</span>
<span class="n">x11</span><span class="w">            </span><span class="mf">0.0161</span><span class="w">      </span><span class="mf">0.074</span><span class="w">      </span><span class="mf">0.216</span><span class="w">      </span><span class="mf">0.829</span><span class="w">        </span><span class="o">-</span><span class="mf">0.130</span><span class="w">     </span><span class="mf">0.162</span>
<span class="n">x12</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0228</span><span class="w">      </span><span class="mf">0.047</span><span class="w">     </span><span class="o">-</span><span class="mf">0.489</span><span class="w">      </span><span class="mf">0.625</span><span class="w">        </span><span class="o">-</span><span class="mf">0.115</span><span class="w">     </span><span class="mf">0.069</span>
<span class="n">x13</span><span class="w">            </span><span class="mf">0.2619</span><span class="w">      </span><span class="mf">0.423</span><span class="w">      </span><span class="mf">0.620</span><span class="w">      </span><span class="mf">0.536</span><span class="w">        </span><span class="o">-</span><span class="mf">0.570</span><span class="w">     </span><span class="mf">1.094</span>
<span class="n">x14</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0303</span><span class="w">      </span><span class="mf">0.027</span><span class="w">     </span><span class="o">-</span><span class="mf">1.136</span><span class="w">      </span><span class="mf">0.257</span><span class="w">        </span><span class="o">-</span><span class="mf">0.083</span><span class="w">     </span><span class="mf">0.022</span>
<span class="n">x15</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0023</span><span class="w">      </span><span class="mf">0.003</span><span class="w">     </span><span class="o">-</span><span class="mf">0.895</span><span class="w">      </span><span class="mf">0.372</span><span class="w">        </span><span class="o">-</span><span class="mf">0.007</span><span class="w">     </span><span class="mf">0.003</span>
<span class="n">x16</span><span class="w">            </span><span class="mf">0.0005</span><span class="w">      </span><span class="mf">0.023</span><span class="w">      </span><span class="mf">0.021</span><span class="w">      </span><span class="mf">0.983</span><span class="w">        </span><span class="o">-</span><span class="mf">0.045</span><span class="w">     </span><span class="mf">0.046</span>
<span class="n">x17</span><span class="w">            </span><span class="mf">0.0206</span><span class="w">      </span><span class="mf">0.040</span><span class="w">      </span><span class="mf">0.513</span><span class="w">      </span><span class="mf">0.608</span><span class="w">        </span><span class="o">-</span><span class="mf">0.059</span><span class="w">     </span><span class="mf">0.100</span>
<span class="n">x18</span><span class="w">            </span><span class="mf">0.0507</span><span class="w">      </span><span class="mf">0.040</span><span class="w">      </span><span class="mf">1.271</span><span class="w">      </span><span class="mf">0.205</span><span class="w">        </span><span class="o">-</span><span class="mf">0.028</span><span class="w">     </span><span class="mf">0.129</span>
<span class="n">x19</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0349</span><span class="w">      </span><span class="mf">0.037</span><span class="w">     </span><span class="o">-</span><span class="mf">0.942</span><span class="w">      </span><span class="mf">0.347</span><span class="w">        </span><span class="o">-</span><span class="mf">0.108</span><span class="w">     </span><span class="mf">0.038</span>
<span class="n">x20</span><span class="w">            </span><span class="mf">0.0210</span><span class="w">      </span><span class="mf">0.017</span><span class="w">      </span><span class="mf">1.252</span><span class="w">      </span><span class="mf">0.212</span><span class="w">        </span><span class="o">-</span><span class="mf">0.012</span><span class="w">     </span><span class="mf">0.054</span>
<span class="n">x21</span><span class="w">            </span><span class="mf">0.0400</span><span class="w">      </span><span class="mf">0.041</span><span class="w">      </span><span class="mf">0.964</span><span class="w">      </span><span class="mf">0.336</span><span class="w">        </span><span class="o">-</span><span class="mf">0.042</span><span class="w">     </span><span class="mf">0.122</span>
<span class="n">x22</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0239</span><span class="w">      </span><span class="mf">0.009</span><span class="w">     </span><span class="o">-</span><span class="mf">2.530</span><span class="w">      </span><span class="mf">0.012</span><span class="w">        </span><span class="o">-</span><span class="mf">0.042</span><span class="w">    </span><span class="o">-</span><span class="mf">0.005</span>
<span class="n">x23</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0140</span><span class="w">      </span><span class="mf">0.008</span><span class="w">     </span><span class="o">-</span><span class="mf">1.683</span><span class="w">      </span><span class="mf">0.094</span><span class="w">        </span><span class="o">-</span><span class="mf">0.030</span><span class="w">     </span><span class="mf">0.002</span>
<span class="n">x24</span><span class="w">            </span><span class="mf">0.0045</span><span class="w">      </span><span class="mf">0.001</span><span class="w">      </span><span class="mf">4.594</span><span class="w">      </span><span class="mf">0.000</span><span class="w">         </span><span class="mf">0.003</span><span class="w">     </span><span class="mf">0.006</span>
<span class="n">x25</span><span class="w">            </span><span class="mf">0.0264</span><span class="w">      </span><span class="mf">0.026</span><span class="w">      </span><span class="mf">1.004</span><span class="w">      </span><span class="mf">0.316</span><span class="w">        </span><span class="o">-</span><span class="mf">0.025</span><span class="w">     </span><span class="mf">0.078</span>
<span class="n">x26</span><span class="w">            </span><span class="mf">0.2730</span><span class="w">      </span><span class="mf">0.169</span><span class="w">      </span><span class="mf">1.615</span><span class="w">      </span><span class="mf">0.107</span><span class="w">        </span><span class="o">-</span><span class="mf">0.060</span><span class="w">     </span><span class="mf">0.606</span>
<span class="n">x27</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0208</span><span class="w">      </span><span class="mf">0.187</span><span class="w">     </span><span class="o">-</span><span class="mf">0.111</span><span class="w">      </span><span class="mf">0.912</span><span class="w">        </span><span class="o">-</span><span class="mf">0.389</span><span class="w">     </span><span class="mf">0.348</span>
<span class="n">x28</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0007</span><span class="w">      </span><span class="mf">0.015</span><span class="w">     </span><span class="o">-</span><span class="mf">0.051</span><span class="w">      </span><span class="mf">0.959</span><span class="w">        </span><span class="o">-</span><span class="mf">0.029</span><span class="w">     </span><span class="mf">0.028</span>
<span class="n">x29</span><span class="w">            </span><span class="mf">0.0168</span><span class="w">      </span><span class="mf">0.018</span><span class="w">      </span><span class="mf">0.917</span><span class="w">      </span><span class="mf">0.360</span><span class="w">        </span><span class="o">-</span><span class="mf">0.019</span><span class="w">     </span><span class="mf">0.053</span>
<span class="n">x30</span><span class="w">            </span><span class="mf">0.0059</span><span class="w">      </span><span class="mf">0.011</span><span class="w">      </span><span class="mf">0.524</span><span class="w">      </span><span class="mf">0.601</span><span class="w">        </span><span class="o">-</span><span class="mf">0.016</span><span class="w">     </span><span class="mf">0.028</span>
<span class="n">x31</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0196</span><span class="w">      </span><span class="mf">0.028</span><span class="w">     </span><span class="o">-</span><span class="mf">0.711</span><span class="w">      </span><span class="mf">0.478</span><span class="w">        </span><span class="o">-</span><span class="mf">0.074</span><span class="w">     </span><span class="mf">0.035</span>
<span class="n">x32</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0035</span><span class="w">      </span><span class="mf">0.004</span><span class="w">     </span><span class="o">-</span><span class="mf">0.899</span><span class="w">      </span><span class="mf">0.370</span><span class="w">        </span><span class="o">-</span><span class="mf">0.011</span><span class="w">     </span><span class="mf">0.004</span>
<span class="n">x33</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0246</span><span class="w">      </span><span class="mf">0.029</span><span class="w">     </span><span class="o">-</span><span class="mf">0.858</span><span class="w">      </span><span class="mf">0.392</span><span class="w">        </span><span class="o">-</span><span class="mf">0.081</span><span class="w">     </span><span class="mf">0.032</span>
<span class="n">x34</span><span class="w">            </span><span class="mf">0.0145</span><span class="w">      </span><span class="mf">0.005</span><span class="w">      </span><span class="mf">2.903</span><span class="w">      </span><span class="mf">0.004</span><span class="w">         </span><span class="mf">0.005</span><span class="w">     </span><span class="mf">0.024</span>
<span class="n">x35</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0017</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">1.442</span><span class="w">      </span><span class="mf">0.150</span><span class="w">        </span><span class="o">-</span><span class="mf">0.004</span><span class="w">     </span><span class="mf">0.001</span>
<span class="n">x36</span><span class="w">            </span><span class="mf">0.0069</span><span class="w">      </span><span class="mf">0.005</span><span class="w">      </span><span class="mf">1.514</span><span class="w">      </span><span class="mf">0.131</span><span class="w">        </span><span class="o">-</span><span class="mf">0.002</span><span class="w">     </span><span class="mf">0.016</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span><span class="w">                        </span><span class="mf">5.509</span><span class="w">   </span><span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span><span class="w">                   </span><span class="mf">1.845</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span><span class="w">                  </span><span class="mf">0.064</span><span class="w">   </span><span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span><span class="w"> </span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                </span><span class="mf">5.309</span>
<span class="n">Skew</span><span class="p">:</span><span class="w">                           </span><span class="mf">0.272</span><span class="w">   </span><span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                       </span><span class="mf">0.0703</span>
<span class="n">Kurtosis</span><span class="p">:</span><span class="w">                       </span><span class="mf">3.362</span><span class="w">   </span><span class="n">Cond</span><span class="o">.</span><span class="w"> </span><span class="n">No</span><span class="o">.</span><span class="w">                     </span><span class="mf">3.70e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="n">Standard</span><span class="w"> </span><span class="n">Errors</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">covariance</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">errors</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correctly</span><span class="w"> </span><span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">large</span><span class="p">,</span><span class="w"> </span><span class="mf">3.7e+05</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">strong</span><span class="w"> </span><span class="n">multicollinearity</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">problems</span><span class="o">.</span>
<span class="w">                            </span><span class="n">OLS</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">Results</span><span class="w">                            </span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span><span class="w"> </span><span class="n">Variable</span><span class="p">:</span><span class="w">                  </span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="w">   </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                       </span><span class="mf">0.304</span>
<span class="n">Model</span><span class="p">:</span><span class="w">                            </span><span class="n">OLS</span><span class="w">   </span><span class="n">Adj</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                  </span><span class="mf">0.248</span>
<span class="n">Method</span><span class="p">:</span><span class="w">                 </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w">   </span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span><span class="w">                     </span><span class="mf">5.452</span>
<span class="n">Date</span><span class="p">:</span><span class="w">                </span><span class="n">Sun</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">Mar</span><span class="w"> </span><span class="mi">2016</span><span class="w">   </span><span class="n">Prob</span><span class="w"> </span><span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span><span class="w">           </span><span class="mf">4.41e-19</span>
<span class="n">Time</span><span class="p">:</span><span class="w">                        </span><span class="mi">10</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">28</span><span class="w">   </span><span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span><span class="w">                 </span><span class="mf">1030.4</span>
<span class="n">No</span><span class="o">.</span><span class="w"> </span><span class="n">Observations</span><span class="p">:</span><span class="w">                 </span><span class="mi">486</span><span class="w">   </span><span class="n">AIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">1987.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Residuals</span><span class="p">:</span><span class="w">                     </span><span class="mi">449</span><span class="w">   </span><span class="n">BIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">1832.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Model</span><span class="p">:</span><span class="w">                          </span><span class="mi">36</span><span class="w">                                         </span>
<span class="n">Covariance</span><span class="w"> </span><span class="n">Type</span><span class="p">:</span><span class="w">            </span><span class="n">nonrobust</span><span class="w">                                         </span>
<span class="o">==============================================================================</span>
<span class="w">                 </span><span class="n">coef</span><span class="w">    </span><span class="n">std</span><span class="w"> </span><span class="n">err</span><span class="w">          </span><span class="n">t</span><span class="w">      </span><span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="w">      </span><span class="p">[</span><span class="mf">95.0</span><span class="o">%</span><span class="w"> </span><span class="n">Conf</span><span class="o">.</span><span class="w"> </span><span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="k">const</span><span class="w">          </span><span class="mf">0.1082</span><span class="w">      </span><span class="mf">0.033</span><span class="w">      </span><span class="mf">3.280</span><span class="w">      </span><span class="mf">0.001</span><span class="w">         </span><span class="mf">0.043</span><span class="w">     </span><span class="mf">0.173</span>
<span class="n">x1</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0018</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">2.317</span><span class="w">      </span><span class="mf">0.021</span><span class="w">        </span><span class="o">-</span><span class="mf">0.003</span><span class="w">    </span><span class="o">-</span><span class="mf">0.000</span>
<span class="n">x2</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0005</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">3.541</span><span class="w">      </span><span class="mf">0.000</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">    </span><span class="o">-</span><span class="mf">0.000</span>
<span class="n">x3</span><span class="w">          </span><span class="mf">4.431e-05</span><span class="w">      </span><span class="mf">0.000</span><span class="w">      </span><span class="mf">0.359</span><span class="w">      </span><span class="mf">0.720</span><span class="w">        </span><span class="o">-</span><span class="mf">0.000</span><span class="w">     </span><span class="mf">0.000</span>
<span class="n">x4</span><span class="w">           </span><span class="mf">1.71e-05</span><span class="w">   </span><span class="mf">6.08e-06</span><span class="w">      </span><span class="mf">2.813</span><span class="w">      </span><span class="mf">0.005</span><span class="w">      </span><span class="mf">5.15e-06</span><span class="w">   </span><span class="mf">2.9e-05</span>
<span class="n">x5</span><span class="w">             </span><span class="mf">0.0257</span><span class="w">      </span><span class="mf">0.044</span><span class="w">      </span><span class="mf">0.580</span><span class="w">      </span><span class="mf">0.562</span><span class="w">        </span><span class="o">-</span><span class="mf">0.061</span><span class="w">     </span><span class="mf">0.113</span>
<span class="n">x6</span><span class="w">             </span><span class="mf">0.0133</span><span class="w">      </span><span class="mf">0.029</span><span class="w">      </span><span class="mf">0.464</span><span class="w">      </span><span class="mf">0.643</span><span class="w">        </span><span class="o">-</span><span class="mf">0.043</span><span class="w">     </span><span class="mf">0.070</span>
<span class="n">x7</span><span class="w">            </span><span class="o">-</span><span class="mf">0.5271</span><span class="w">      </span><span class="mf">0.357</span><span class="w">     </span><span class="o">-</span><span class="mf">1.476</span><span class="w">      </span><span class="mf">0.141</span><span class="w">        </span><span class="o">-</span><span class="mf">1.229</span><span class="w">     </span><span class="mf">0.175</span>
<span class="n">x8</span><span class="w">             </span><span class="mf">0.0415</span><span class="w">      </span><span class="mf">0.038</span><span class="w">      </span><span class="mf">1.090</span><span class="w">      </span><span class="mf">0.277</span><span class="w">        </span><span class="o">-</span><span class="mf">0.033</span><span class="w">     </span><span class="mf">0.116</span>
<span class="n">x9</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0117</span><span class="w">      </span><span class="mf">0.029</span><span class="w">     </span><span class="o">-</span><span class="mf">0.409</span><span class="w">      </span><span class="mf">0.682</span><span class="w">        </span><span class="o">-</span><span class="mf">0.068</span><span class="w">     </span><span class="mf">0.044</span>
<span class="n">x10</span><span class="w">            </span><span class="mf">0.0031</span><span class="w">      </span><span class="mf">0.018</span><span class="w">      </span><span class="mf">0.171</span><span class="w">      </span><span class="mf">0.865</span><span class="w">        </span><span class="o">-</span><span class="mf">0.032</span><span class="w">     </span><span class="mf">0.038</span>
<span class="n">x11</span><span class="w">            </span><span class="mf">0.0253</span><span class="w">      </span><span class="mf">0.031</span><span class="w">      </span><span class="mf">0.819</span><span class="w">      </span><span class="mf">0.413</span><span class="w">        </span><span class="o">-</span><span class="mf">0.035</span><span class="w">     </span><span class="mf">0.086</span>
<span class="n">x12</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0196</span><span class="w">      </span><span class="mf">0.028</span><span class="w">     </span><span class="o">-</span><span class="mf">0.687</span><span class="w">      </span><span class="mf">0.492</span><span class="w">        </span><span class="o">-</span><span class="mf">0.076</span><span class="w">     </span><span class="mf">0.036</span>
<span class="n">x13</span><span class="w">            </span><span class="mf">0.0360</span><span class="w">      </span><span class="mf">0.067</span><span class="w">      </span><span class="mf">0.535</span><span class="w">      </span><span class="mf">0.593</span><span class="w">        </span><span class="o">-</span><span class="mf">0.096</span><span class="w">     </span><span class="mf">0.168</span>
<span class="n">x14</span><span class="w">            </span><span class="mf">0.0096</span><span class="w">      </span><span class="mf">0.021</span><span class="w">      </span><span class="mf">0.461</span><span class="w">      </span><span class="mf">0.645</span><span class="w">        </span><span class="o">-</span><span class="mf">0.031</span><span class="w">     </span><span class="mf">0.050</span>
<span class="n">x15</span><span class="w">            </span><span class="mf">0.0101</span><span class="w">      </span><span class="mf">0.009</span><span class="w">      </span><span class="mf">1.165</span><span class="w">      </span><span class="mf">0.245</span><span class="w">        </span><span class="o">-</span><span class="mf">0.007</span><span class="w">     </span><span class="mf">0.027</span>
<span class="n">x16</span><span class="w">            </span><span class="mf">0.0227</span><span class="w">      </span><span class="mf">0.015</span><span class="w">      </span><span class="mf">1.556</span><span class="w">      </span><span class="mf">0.120</span><span class="w">        </span><span class="o">-</span><span class="mf">0.006</span><span class="w">     </span><span class="mf">0.051</span>
<span class="n">x17</span><span class="w">            </span><span class="mf">0.0413</span><span class="w">      </span><span class="mf">0.034</span><span class="w">      </span><span class="mf">1.198</span><span class="w">      </span><span class="mf">0.232</span><span class="w">        </span><span class="o">-</span><span class="mf">0.026</span><span class="w">     </span><span class="mf">0.109</span>
<span class="n">x18</span><span class="w">            </span><span class="mf">0.0195</span><span class="w">      </span><span class="mf">0.031</span><span class="w">      </span><span class="mf">0.623</span><span class="w">      </span><span class="mf">0.533</span><span class="w">        </span><span class="o">-</span><span class="mf">0.042</span><span class="w">     </span><span class="mf">0.081</span>
<span class="n">x19</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0267</span><span class="w">      </span><span class="mf">0.029</span><span class="w">     </span><span class="o">-</span><span class="mf">0.906</span><span class="w">      </span><span class="mf">0.366</span><span class="w">        </span><span class="o">-</span><span class="mf">0.085</span><span class="w">     </span><span class="mf">0.031</span>
<span class="n">x20</span><span class="w">            </span><span class="mf">0.0199</span><span class="w">      </span><span class="mf">0.008</span><span class="w">      </span><span class="mf">2.652</span><span class="w">      </span><span class="mf">0.008</span><span class="w">         </span><span class="mf">0.005</span><span class="w">     </span><span class="mf">0.035</span>
<span class="n">x21</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0442</span><span class="w">      </span><span class="mf">0.033</span><span class="w">     </span><span class="o">-</span><span class="mf">1.325</span><span class="w">      </span><span class="mf">0.186</span><span class="w">        </span><span class="o">-</span><span class="mf">0.110</span><span class="w">     </span><span class="mf">0.021</span>
<span class="n">x22</span><span class="w">            </span><span class="mf">0.0232</span><span class="w">      </span><span class="mf">0.025</span><span class="w">      </span><span class="mf">0.946</span><span class="w">      </span><span class="mf">0.345</span><span class="w">        </span><span class="o">-</span><span class="mf">0.025</span><span class="w">     </span><span class="mf">0.072</span>
<span class="n">x23</span><span class="w">            </span><span class="mf">0.0085</span><span class="w">      </span><span class="mf">0.009</span><span class="w">      </span><span class="mf">0.976</span><span class="w">      </span><span class="mf">0.330</span><span class="w">        </span><span class="o">-</span><span class="mf">0.009</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x24</span><span class="w">            </span><span class="mf">0.0025</span><span class="w">      </span><span class="mf">0.001</span><span class="w">      </span><span class="mf">1.782</span><span class="w">      </span><span class="mf">0.075</span><span class="w">        </span><span class="o">-</span><span class="mf">0.000</span><span class="w">     </span><span class="mf">0.005</span>
<span class="n">x25</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0200</span><span class="w">      </span><span class="mf">0.019</span><span class="w">     </span><span class="o">-</span><span class="mf">1.042</span><span class="w">      </span><span class="mf">0.298</span><span class="w">        </span><span class="o">-</span><span class="mf">0.058</span><span class="w">     </span><span class="mf">0.018</span>
<span class="n">x26</span><span class="w">            </span><span class="mf">0.4937</span><span class="w">      </span><span class="mf">0.331</span><span class="w">      </span><span class="mf">1.491</span><span class="w">      </span><span class="mf">0.137</span><span class="w">        </span><span class="o">-</span><span class="mf">0.157</span><span class="w">     </span><span class="mf">1.144</span>
<span class="n">x27</span><span class="w">           </span><span class="o">-</span><span class="mf">0.1406</span><span class="w">      </span><span class="mf">0.074</span><span class="w">     </span><span class="o">-</span><span class="mf">1.907</span><span class="w">      </span><span class="mf">0.057</span><span class="w">        </span><span class="o">-</span><span class="mf">0.286</span><span class="w">     </span><span class="mf">0.004</span>
<span class="n">x28</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0638</span><span class="w">      </span><span class="mf">0.049</span><span class="w">     </span><span class="o">-</span><span class="mf">1.304</span><span class="w">      </span><span class="mf">0.193</span><span class="w">        </span><span class="o">-</span><span class="mf">0.160</span><span class="w">     </span><span class="mf">0.032</span>
<span class="n">x29</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0252</span><span class="w">      </span><span class="mf">0.015</span><span class="w">     </span><span class="o">-</span><span class="mf">1.690</span><span class="w">      </span><span class="mf">0.092</span><span class="w">        </span><span class="o">-</span><span class="mf">0.055</span><span class="w">     </span><span class="mf">0.004</span>
<span class="n">x30</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0217</span><span class="w">      </span><span class="mf">0.008</span><span class="w">     </span><span class="o">-</span><span class="mf">2.668</span><span class="w">      </span><span class="mf">0.008</span><span class="w">        </span><span class="o">-</span><span class="mf">0.038</span><span class="w">    </span><span class="o">-</span><span class="mf">0.006</span>
<span class="n">x31</span><span class="w">            </span><span class="mf">0.0483</span><span class="w">      </span><span class="mf">0.020</span><span class="w">      </span><span class="mf">2.387</span><span class="w">      </span><span class="mf">0.017</span><span class="w">         </span><span class="mf">0.009</span><span class="w">     </span><span class="mf">0.088</span>
<span class="n">x32</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0036</span><span class="w">      </span><span class="mf">0.002</span><span class="w">     </span><span class="o">-</span><span class="mf">2.159</span><span class="w">      </span><span class="mf">0.031</span><span class="w">        </span><span class="o">-</span><span class="mf">0.007</span><span class="w">    </span><span class="o">-</span><span class="mf">0.000</span>
<span class="n">x33</span><span class="w">            </span><span class="mf">0.0388</span><span class="w">      </span><span class="mf">0.023</span><span class="w">      </span><span class="mf">1.681</span><span class="w">      </span><span class="mf">0.094</span><span class="w">        </span><span class="o">-</span><span class="mf">0.007</span><span class="w">     </span><span class="mf">0.084</span>
<span class="n">x34</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0105</span><span class="w">      </span><span class="mf">0.011</span><span class="w">     </span><span class="o">-</span><span class="mf">0.923</span><span class="w">      </span><span class="mf">0.357</span><span class="w">        </span><span class="o">-</span><span class="mf">0.033</span><span class="w">     </span><span class="mf">0.012</span>
<span class="n">x35</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0028</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">1.966</span><span class="w">      </span><span class="mf">0.050</span><span class="w">        </span><span class="o">-</span><span class="mf">0.006</span><span class="w"> </span><span class="o">-</span><span class="mf">1.59e-06</span>
<span class="n">x36</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0017</span><span class="w">      </span><span class="mf">0.003</span><span class="w">     </span><span class="o">-</span><span class="mf">0.513</span><span class="w">      </span><span class="mf">0.608</span><span class="w">        </span><span class="o">-</span><span class="mf">0.008</span><span class="w">     </span><span class="mf">0.005</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span><span class="w">                        </span><span class="mf">5.317</span><span class="w">   </span><span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span><span class="w">                   </span><span class="mf">2.030</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span><span class="w">                  </span><span class="mf">0.070</span><span class="w">   </span><span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span><span class="w"> </span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                </span><span class="mf">5.115</span>
<span class="n">Skew</span><span class="p">:</span><span class="w">                           </span><span class="mf">0.226</span><span class="w">   </span><span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                       </span><span class="mf">0.0775</span>
<span class="n">Kurtosis</span><span class="p">:</span><span class="w">                       </span><span class="mf">3.221</span><span class="w">   </span><span class="n">Cond</span><span class="o">.</span><span class="w"> </span><span class="n">No</span><span class="o">.</span><span class="w">                     </span><span class="mf">4.51e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="n">Standard</span><span class="w"> </span><span class="n">Errors</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">covariance</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">errors</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correctly</span><span class="w"> </span><span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">large</span><span class="p">,</span><span class="w"> </span><span class="mf">4.51e+05</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">strong</span><span class="w"> </span><span class="n">multicollinearity</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">problems</span><span class="o">.</span>
<span class="w">                            </span><span class="n">OLS</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">Results</span><span class="w">                            </span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span><span class="w"> </span><span class="n">Variable</span><span class="p">:</span><span class="w">                  </span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="w">   </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                       </span><span class="mf">0.455</span>
<span class="n">Model</span><span class="p">:</span><span class="w">                            </span><span class="n">OLS</span><span class="w">   </span><span class="n">Adj</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                  </span><span class="mf">0.378</span>
<span class="n">Method</span><span class="p">:</span><span class="w">                 </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w">   </span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span><span class="w">                     </span><span class="mf">5.852</span>
<span class="n">Date</span><span class="p">:</span><span class="w">                </span><span class="n">Sun</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">Mar</span><span class="w"> </span><span class="mi">2016</span><span class="w">   </span><span class="n">Prob</span><span class="w"> </span><span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span><span class="w">           </span><span class="mf">4.77e-18</span>
<span class="n">Time</span><span class="p">:</span><span class="w">                        </span><span class="mi">10</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">28</span><span class="w">   </span><span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span><span class="w">                 </span><span class="mf">631.81</span>
<span class="n">No</span><span class="o">.</span><span class="w"> </span><span class="n">Observations</span><span class="p">:</span><span class="w">                 </span><span class="mi">289</span><span class="w">   </span><span class="n">AIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">1190.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Residuals</span><span class="p">:</span><span class="w">                     </span><span class="mi">252</span><span class="w">   </span><span class="n">BIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">1054.</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Model</span><span class="p">:</span><span class="w">                          </span><span class="mi">36</span><span class="w">                                         </span>
<span class="n">Covariance</span><span class="w"> </span><span class="n">Type</span><span class="p">:</span><span class="w">            </span><span class="n">nonrobust</span><span class="w">                                         </span>
<span class="o">==============================================================================</span>
<span class="w">                 </span><span class="n">coef</span><span class="w">    </span><span class="n">std</span><span class="w"> </span><span class="n">err</span><span class="w">          </span><span class="n">t</span><span class="w">      </span><span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="w">      </span><span class="p">[</span><span class="mf">95.0</span><span class="o">%</span><span class="w"> </span><span class="n">Conf</span><span class="o">.</span><span class="w"> </span><span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="k">const</span><span class="w">          </span><span class="mf">0.1755</span><span class="w">      </span><span class="mf">0.096</span><span class="w">      </span><span class="mf">1.827</span><span class="w">      </span><span class="mf">0.069</span><span class="w">        </span><span class="o">-</span><span class="mf">0.014</span><span class="w">     </span><span class="mf">0.365</span>
<span class="n">x1</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0031</span><span class="w">      </span><span class="mf">0.001</span><span class="w">     </span><span class="o">-</span><span class="mf">2.357</span><span class="w">      </span><span class="mf">0.019</span><span class="w">        </span><span class="o">-</span><span class="mf">0.006</span><span class="w">    </span><span class="o">-</span><span class="mf">0.001</span>
<span class="n">x2</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0005</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">2.424</span><span class="w">      </span><span class="mf">0.016</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w"> </span><span class="o">-</span><span class="mf">8.68e-05</span>
<span class="n">x3</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0003</span><span class="w">      </span><span class="mf">0.000</span><span class="w">     </span><span class="o">-</span><span class="mf">2.154</span><span class="w">      </span><span class="mf">0.032</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">  </span><span class="o">-</span><span class="mf">2.9e-05</span>
<span class="n">x4</span><span class="w">          </span><span class="mf">2.374e-05</span><span class="w">   </span><span class="mf">8.35e-06</span><span class="w">      </span><span class="mf">2.842</span><span class="w">      </span><span class="mf">0.005</span><span class="w">      </span><span class="mf">7.29e-06</span><span class="w">  </span><span class="mf">4.02e-05</span>
<span class="n">x5</span><span class="w">             </span><span class="mf">0.0391</span><span class="w">      </span><span class="mf">0.070</span><span class="w">      </span><span class="mf">0.556</span><span class="w">      </span><span class="mf">0.579</span><span class="w">        </span><span class="o">-</span><span class="mf">0.099</span><span class="w">     </span><span class="mf">0.177</span>
<span class="n">x6</span><span class="w">             </span><span class="mf">0.0672</span><span class="w">      </span><span class="mf">0.040</span><span class="w">      </span><span class="mf">1.662</span><span class="w">      </span><span class="mf">0.098</span><span class="w">        </span><span class="o">-</span><span class="mf">0.012</span><span class="w">     </span><span class="mf">0.147</span>
<span class="n">x7</span><span class="w">             </span><span class="mf">0.9503</span><span class="w">      </span><span class="mf">0.458</span><span class="w">      </span><span class="mf">2.075</span><span class="w">      </span><span class="mf">0.039</span><span class="w">         </span><span class="mf">0.048</span><span class="w">     </span><span class="mf">1.852</span>
<span class="n">x8</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0013</span><span class="w">      </span><span class="mf">0.061</span><span class="w">     </span><span class="o">-</span><span class="mf">0.021</span><span class="w">      </span><span class="mf">0.983</span><span class="w">        </span><span class="o">-</span><span class="mf">0.122</span><span class="w">     </span><span class="mf">0.119</span>
<span class="n">x9</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0270</span><span class="w">      </span><span class="mf">0.041</span><span class="w">     </span><span class="o">-</span><span class="mf">0.659</span><span class="w">      </span><span class="mf">0.510</span><span class="w">        </span><span class="o">-</span><span class="mf">0.108</span><span class="w">     </span><span class="mf">0.054</span>
<span class="n">x10</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0072</span><span class="w">      </span><span class="mf">0.017</span><span class="w">     </span><span class="o">-</span><span class="mf">0.426</span><span class="w">      </span><span class="mf">0.671</span><span class="w">        </span><span class="o">-</span><span class="mf">0.041</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x11</span><span class="w">            </span><span class="mf">0.0604</span><span class="w">      </span><span class="mf">0.056</span><span class="w">      </span><span class="mf">1.083</span><span class="w">      </span><span class="mf">0.280</span><span class="w">        </span><span class="o">-</span><span class="mf">0.049</span><span class="w">     </span><span class="mf">0.170</span>
<span class="n">x12</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0723</span><span class="w">      </span><span class="mf">0.041</span><span class="w">     </span><span class="o">-</span><span class="mf">1.782</span><span class="w">      </span><span class="mf">0.076</span><span class="w">        </span><span class="o">-</span><span class="mf">0.152</span><span class="w">     </span><span class="mf">0.008</span>
<span class="n">x13</span><span class="w">           </span><span class="o">-</span><span class="mf">1.2499</span><span class="w">      </span><span class="mf">0.392</span><span class="w">     </span><span class="o">-</span><span class="mf">3.186</span><span class="w">      </span><span class="mf">0.002</span><span class="w">        </span><span class="o">-</span><span class="mf">2.022</span><span class="w">    </span><span class="o">-</span><span class="mf">0.477</span>
<span class="n">x14</span><span class="w">            </span><span class="mf">0.0502</span><span class="w">      </span><span class="mf">0.028</span><span class="w">      </span><span class="mf">1.776</span><span class="w">      </span><span class="mf">0.077</span><span class="w">        </span><span class="o">-</span><span class="mf">0.005</span><span class="w">     </span><span class="mf">0.106</span>
<span class="n">x15</span><span class="w">            </span><span class="mf">0.0048</span><span class="w">      </span><span class="mf">0.011</span><span class="w">      </span><span class="mf">0.456</span><span class="w">      </span><span class="mf">0.649</span><span class="w">        </span><span class="o">-</span><span class="mf">0.016</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x16</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0637</span><span class="w">      </span><span class="mf">0.042</span><span class="w">     </span><span class="o">-</span><span class="mf">1.530</span><span class="w">      </span><span class="mf">0.127</span><span class="w">        </span><span class="o">-</span><span class="mf">0.146</span><span class="w">     </span><span class="mf">0.018</span>
<span class="n">x17</span><span class="w">            </span><span class="mf">0.0042</span><span class="w">      </span><span class="mf">0.038</span><span class="w">      </span><span class="mf">0.112</span><span class="w">      </span><span class="mf">0.911</span><span class="w">        </span><span class="o">-</span><span class="mf">0.070</span><span class="w">     </span><span class="mf">0.078</span>
<span class="n">x18</span><span class="w">            </span><span class="mf">0.0318</span><span class="w">      </span><span class="mf">0.038</span><span class="w">      </span><span class="mf">0.830</span><span class="w">      </span><span class="mf">0.408</span><span class="w">        </span><span class="o">-</span><span class="mf">0.044</span><span class="w">     </span><span class="mf">0.107</span>
<span class="n">x19</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0220</span><span class="w">      </span><span class="mf">0.037</span><span class="w">     </span><span class="o">-</span><span class="mf">0.602</span><span class="w">      </span><span class="mf">0.548</span><span class="w">        </span><span class="o">-</span><span class="mf">0.094</span><span class="w">     </span><span class="mf">0.050</span>
<span class="n">x20</span><span class="w">        </span><span class="o">-</span><span class="mf">4.535e-05</span><span class="w">      </span><span class="mf">0.009</span><span class="w">     </span><span class="o">-</span><span class="mf">0.005</span><span class="w">      </span><span class="mf">0.996</span><span class="w">        </span><span class="o">-</span><span class="mf">0.018</span><span class="w">     </span><span class="mf">0.018</span>
<span class="n">x21</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0176</span><span class="w">      </span><span class="mf">0.040</span><span class="w">     </span><span class="o">-</span><span class="mf">0.440</span><span class="w">      </span><span class="mf">0.660</span><span class="w">        </span><span class="o">-</span><span class="mf">0.097</span><span class="w">     </span><span class="mf">0.061</span>
<span class="n">x22</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0244</span><span class="w">      </span><span class="mf">0.021</span><span class="w">     </span><span class="o">-</span><span class="mf">1.182</span><span class="w">      </span><span class="mf">0.238</span><span class="w">        </span><span class="o">-</span><span class="mf">0.065</span><span class="w">     </span><span class="mf">0.016</span>
<span class="n">x23</span><span class="w">            </span><span class="mf">0.0135</span><span class="w">      </span><span class="mf">0.012</span><span class="w">      </span><span class="mf">1.128</span><span class="w">      </span><span class="mf">0.260</span><span class="w">        </span><span class="o">-</span><span class="mf">0.010</span><span class="w">     </span><span class="mf">0.037</span>
<span class="n">x24</span><span class="w">            </span><span class="mf">0.0024</span><span class="w">      </span><span class="mf">0.002</span><span class="w">      </span><span class="mf">1.355</span><span class="w">      </span><span class="mf">0.177</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">     </span><span class="mf">0.006</span>
<span class="n">x25</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0418</span><span class="w">      </span><span class="mf">0.026</span><span class="w">     </span><span class="o">-</span><span class="mf">1.583</span><span class="w">      </span><span class="mf">0.115</span><span class="w">        </span><span class="o">-</span><span class="mf">0.094</span><span class="w">     </span><span class="mf">0.010</span>
<span class="n">x26</span><span class="w">            </span><span class="mf">0.3619</span><span class="w">      </span><span class="mf">0.328</span><span class="w">      </span><span class="mf">1.105</span><span class="w">      </span><span class="mf">0.270</span><span class="w">        </span><span class="o">-</span><span class="mf">0.283</span><span class="w">     </span><span class="mf">1.007</span>
<span class="n">x27</span><span class="w">            </span><span class="mf">0.0090</span><span class="w">      </span><span class="mf">0.186</span><span class="w">      </span><span class="mf">0.049</span><span class="w">      </span><span class="mf">0.961</span><span class="w">        </span><span class="o">-</span><span class="mf">0.358</span><span class="w">     </span><span class="mf">0.376</span>
<span class="n">x28</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0613</span><span class="w">      </span><span class="mf">0.057</span><span class="w">     </span><span class="o">-</span><span class="mf">1.068</span><span class="w">      </span><span class="mf">0.286</span><span class="w">        </span><span class="o">-</span><span class="mf">0.174</span><span class="w">     </span><span class="mf">0.052</span>
<span class="n">x29</span><span class="w">            </span><span class="mf">0.0124</span><span class="w">      </span><span class="mf">0.016</span><span class="w">      </span><span class="mf">0.779</span><span class="w">      </span><span class="mf">0.436</span><span class="w">        </span><span class="o">-</span><span class="mf">0.019</span><span class="w">     </span><span class="mf">0.044</span>
<span class="n">x30</span><span class="w">            </span><span class="mf">0.0042</span><span class="w">      </span><span class="mf">0.011</span><span class="w">      </span><span class="mf">0.379</span><span class="w">      </span><span class="mf">0.705</span><span class="w">        </span><span class="o">-</span><span class="mf">0.018</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x31</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0108</span><span class="w">      </span><span class="mf">0.026</span><span class="w">     </span><span class="o">-</span><span class="mf">0.412</span><span class="w">      </span><span class="mf">0.681</span><span class="w">        </span><span class="o">-</span><span class="mf">0.062</span><span class="w">     </span><span class="mf">0.041</span>
<span class="n">x32</span><span class="w">            </span><span class="mf">0.0014</span><span class="w">      </span><span class="mf">0.002</span><span class="w">      </span><span class="mf">0.588</span><span class="w">      </span><span class="mf">0.557</span><span class="w">        </span><span class="o">-</span><span class="mf">0.003</span><span class="w">     </span><span class="mf">0.006</span>
<span class="n">x33</span><span class="w">            </span><span class="mf">0.0195</span><span class="w">      </span><span class="mf">0.029</span><span class="w">      </span><span class="mf">0.672</span><span class="w">      </span><span class="mf">0.502</span><span class="w">        </span><span class="o">-</span><span class="mf">0.038</span><span class="w">     </span><span class="mf">0.077</span>
<span class="n">x34</span><span class="w">            </span><span class="mf">0.0168</span><span class="w">      </span><span class="mf">0.011</span><span class="w">      </span><span class="mf">1.554</span><span class="w">      </span><span class="mf">0.121</span><span class="w">        </span><span class="o">-</span><span class="mf">0.004</span><span class="w">     </span><span class="mf">0.038</span>
<span class="n">x35</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0026</span><span class="w">      </span><span class="mf">0.002</span><span class="w">     </span><span class="o">-</span><span class="mf">1.227</span><span class="w">      </span><span class="mf">0.221</span><span class="w">        </span><span class="o">-</span><span class="mf">0.007</span><span class="w">     </span><span class="mf">0.002</span>
<span class="n">x36</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0072</span><span class="w">      </span><span class="mf">0.004</span><span class="w">     </span><span class="o">-</span><span class="mf">1.958</span><span class="w">      </span><span class="mf">0.051</span><span class="w">        </span><span class="o">-</span><span class="mf">0.014</span><span class="w">  </span><span class="mf">4.02e-05</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span><span class="w">                        </span><span class="mf">4.277</span><span class="w">   </span><span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span><span class="w">                   </span><span class="mf">1.995</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span><span class="w">                  </span><span class="mf">0.118</span><span class="w">   </span><span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span><span class="w"> </span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                </span><span class="mf">4.056</span>
<span class="n">Skew</span><span class="p">:</span><span class="w">                           </span><span class="mf">0.226</span><span class="w">   </span><span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                        </span><span class="mf">0.132</span>
<span class="n">Kurtosis</span><span class="p">:</span><span class="w">                       </span><span class="mf">3.364</span><span class="w">   </span><span class="n">Cond</span><span class="o">.</span><span class="w"> </span><span class="n">No</span><span class="o">.</span><span class="w">                     </span><span class="mf">4.24e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="n">Standard</span><span class="w"> </span><span class="n">Errors</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">covariance</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">errors</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correctly</span><span class="w"> </span><span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">large</span><span class="p">,</span><span class="w"> </span><span class="mf">4.24e+05</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">strong</span><span class="w"> </span><span class="n">multicollinearity</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">problems</span><span class="o">.</span>
<span class="w">                            </span><span class="n">OLS</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">Results</span><span class="w">                            </span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span><span class="w"> </span><span class="n">Variable</span><span class="p">:</span><span class="w">                  </span><span class="n">WS</span><span class="o">/</span><span class="mi">48</span><span class="w">   </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                       </span><span class="mf">0.476</span>
<span class="n">Model</span><span class="p">:</span><span class="w">                            </span><span class="n">OLS</span><span class="w">   </span><span class="n">Adj</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span><span class="w">                  </span><span class="mf">0.337</span>
<span class="n">Method</span><span class="p">:</span><span class="w">                 </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w">   </span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span><span class="w">                     </span><span class="mf">3.431</span>
<span class="n">Date</span><span class="p">:</span><span class="w">                </span><span class="n">Sun</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">Mar</span><span class="w"> </span><span class="mi">2016</span><span class="w">   </span><span class="n">Prob</span><span class="w"> </span><span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span><span class="w">           </span><span class="mf">1.19e-07</span>
<span class="n">Time</span><span class="p">:</span><span class="w">                        </span><span class="mi">10</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">28</span><span class="w">   </span><span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span><span class="w">                 </span><span class="mf">330.36</span>
<span class="n">No</span><span class="o">.</span><span class="w"> </span><span class="n">Observations</span><span class="p">:</span><span class="w">                 </span><span class="mi">173</span><span class="w">   </span><span class="n">AIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">586.7</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Residuals</span><span class="p">:</span><span class="w">                     </span><span class="mi">136</span><span class="w">   </span><span class="n">BIC</span><span class="p">:</span><span class="w">                            </span><span class="o">-</span><span class="mf">470.1</span>
<span class="n">Df</span><span class="w"> </span><span class="n">Model</span><span class="p">:</span><span class="w">                          </span><span class="mi">36</span><span class="w">                                         </span>
<span class="n">Covariance</span><span class="w"> </span><span class="n">Type</span><span class="p">:</span><span class="w">            </span><span class="n">nonrobust</span><span class="w">                                         </span>
<span class="o">==============================================================================</span>
<span class="w">                 </span><span class="n">coef</span><span class="w">    </span><span class="n">std</span><span class="w"> </span><span class="n">err</span><span class="w">          </span><span class="n">t</span><span class="w">      </span><span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="w">      </span><span class="p">[</span><span class="mf">95.0</span><span class="o">%</span><span class="w"> </span><span class="n">Conf</span><span class="o">.</span><span class="w"> </span><span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="k">const</span><span class="w">          </span><span class="mf">0.1822</span><span class="w">      </span><span class="mf">0.262</span><span class="w">      </span><span class="mf">0.696</span><span class="w">      </span><span class="mf">0.488</span><span class="w">        </span><span class="o">-</span><span class="mf">0.335</span><span class="w">     </span><span class="mf">0.700</span>
<span class="n">x1</span><span class="w">            </span><span class="o">-</span><span class="mf">0.0011</span><span class="w">      </span><span class="mf">0.002</span><span class="w">     </span><span class="o">-</span><span class="mf">0.491</span><span class="w">      </span><span class="mf">0.624</span><span class="w">        </span><span class="o">-</span><span class="mf">0.005</span><span class="w">     </span><span class="mf">0.003</span>
<span class="n">x2</span><span class="w">             </span><span class="mf">0.0001</span><span class="w">      </span><span class="mf">0.000</span><span class="w">      </span><span class="mf">0.310</span><span class="w">      </span><span class="mf">0.757</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">     </span><span class="mf">0.001</span>
<span class="n">x3</span><span class="w">          </span><span class="mf">6.743e-05</span><span class="w">      </span><span class="mf">0.000</span><span class="w">      </span><span class="mf">0.220</span><span class="w">      </span><span class="mf">0.827</span><span class="w">        </span><span class="o">-</span><span class="mf">0.001</span><span class="w">     </span><span class="mf">0.001</span>
<span class="n">x4</span><span class="w">          </span><span class="mf">5.819e-06</span><span class="w">   </span><span class="mf">1.63e-05</span><span class="w">      </span><span class="mf">0.357</span><span class="w">      </span><span class="mf">0.722</span><span class="w">     </span><span class="o">-</span><span class="mf">2.65e-05</span><span class="w">  </span><span class="mf">3.81e-05</span>
<span class="n">x5</span><span class="w">             </span><span class="mf">0.0618</span><span class="w">      </span><span class="mf">0.122</span><span class="w">      </span><span class="mf">0.507</span><span class="w">      </span><span class="mf">0.613</span><span class="w">        </span><span class="o">-</span><span class="mf">0.179</span><span class="w">     </span><span class="mf">0.303</span>
<span class="n">x6</span><span class="w">             </span><span class="mf">0.0937</span><span class="w">      </span><span class="mf">0.074</span><span class="w">      </span><span class="mf">1.272</span><span class="w">      </span><span class="mf">0.206</span><span class="w">        </span><span class="o">-</span><span class="mf">0.052</span><span class="w">     </span><span class="mf">0.240</span>
<span class="n">x7</span><span class="w">             </span><span class="mf">0.8422</span><span class="w">      </span><span class="mf">0.919</span><span class="w">      </span><span class="mf">0.917</span><span class="w">      </span><span class="mf">0.361</span><span class="w">        </span><span class="o">-</span><span class="mf">0.975</span><span class="w">     </span><span class="mf">2.659</span>
<span class="n">x8</span><span class="w">            </span><span class="o">-</span><span class="mf">0.1109</span><span class="w">      </span><span class="mf">0.111</span><span class="w">     </span><span class="o">-</span><span class="mf">1.001</span><span class="w">      </span><span class="mf">0.319</span><span class="w">        </span><span class="o">-</span><span class="mf">0.330</span><span class="w">     </span><span class="mf">0.108</span>
<span class="n">x9</span><span class="w">            </span><span class="o">-</span><span class="mf">0.1334</span><span class="w">      </span><span class="mf">0.075</span><span class="w">     </span><span class="o">-</span><span class="mf">1.767</span><span class="w">      </span><span class="mf">0.079</span><span class="w">        </span><span class="o">-</span><span class="mf">0.283</span><span class="w">     </span><span class="mf">0.016</span>
<span class="n">x10</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0357</span><span class="w">      </span><span class="mf">0.024</span><span class="w">     </span><span class="o">-</span><span class="mf">1.500</span><span class="w">      </span><span class="mf">0.136</span><span class="w">        </span><span class="o">-</span><span class="mf">0.083</span><span class="w">     </span><span class="mf">0.011</span>
<span class="n">x11</span><span class="w">           </span><span class="o">-</span><span class="mf">0.1373</span><span class="w">      </span><span class="mf">0.103</span><span class="w">     </span><span class="o">-</span><span class="mf">1.335</span><span class="w">      </span><span class="mf">0.184</span><span class="w">        </span><span class="o">-</span><span class="mf">0.341</span><span class="w">     </span><span class="mf">0.066</span>
<span class="n">x12</span><span class="w">           </span><span class="o">-</span><span class="mf">0.1002</span><span class="w">      </span><span class="mf">0.075</span><span class="w">     </span><span class="o">-</span><span class="mf">1.329</span><span class="w">      </span><span class="mf">0.186</span><span class="w">        </span><span class="o">-</span><span class="mf">0.249</span><span class="w">     </span><span class="mf">0.049</span>
<span class="n">x13</span><span class="w">           </span><span class="o">-</span><span class="mf">0.2963</span><span class="w">      </span><span class="mf">0.616</span><span class="w">     </span><span class="o">-</span><span class="mf">0.481</span><span class="w">      </span><span class="mf">0.631</span><span class="w">        </span><span class="o">-</span><span class="mf">1.515</span><span class="w">     </span><span class="mf">0.922</span>
<span class="n">x14</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0278</span><span class="w">      </span><span class="mf">0.047</span><span class="w">     </span><span class="o">-</span><span class="mf">0.588</span><span class="w">      </span><span class="mf">0.557</span><span class="w">        </span><span class="o">-</span><span class="mf">0.121</span><span class="w">     </span><span class="mf">0.066</span>
<span class="n">x15</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0099</span><span class="w">      </span><span class="mf">0.015</span><span class="w">     </span><span class="o">-</span><span class="mf">0.661</span><span class="w">      </span><span class="mf">0.510</span><span class="w">        </span><span class="o">-</span><span class="mf">0.040</span><span class="w">     </span><span class="mf">0.020</span>
<span class="n">x16</span><span class="w">            </span><span class="mf">0.1532</span><span class="w">      </span><span class="mf">0.106</span><span class="w">      </span><span class="mf">1.444</span><span class="w">      </span><span class="mf">0.151</span><span class="w">        </span><span class="o">-</span><span class="mf">0.057</span><span class="w">     </span><span class="mf">0.363</span>
<span class="n">x17</span><span class="w">           </span><span class="o">-</span><span class="mf">0.1569</span><span class="w">      </span><span class="mf">0.072</span><span class="w">     </span><span class="o">-</span><span class="mf">2.168</span><span class="w">      </span><span class="mf">0.032</span><span class="w">        </span><span class="o">-</span><span class="mf">0.300</span><span class="w">    </span><span class="o">-</span><span class="mf">0.014</span>
<span class="n">x18</span><span class="w">           </span><span class="o">-</span><span class="mf">0.1633</span><span class="w">      </span><span class="mf">0.068</span><span class="w">     </span><span class="o">-</span><span class="mf">2.385</span><span class="w">      </span><span class="mf">0.018</span><span class="w">        </span><span class="o">-</span><span class="mf">0.299</span><span class="w">    </span><span class="o">-</span><span class="mf">0.028</span>
<span class="n">x19</span><span class="w">            </span><span class="mf">0.1550</span><span class="w">      </span><span class="mf">0.066</span><span class="w">      </span><span class="mf">2.356</span><span class="w">      </span><span class="mf">0.020</span><span class="w">         </span><span class="mf">0.025</span><span class="w">     </span><span class="mf">0.285</span>
<span class="n">x20</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0114</span><span class="w">      </span><span class="mf">0.017</span><span class="w">     </span><span class="o">-</span><span class="mf">0.688</span><span class="w">      </span><span class="mf">0.492</span><span class="w">        </span><span class="o">-</span><span class="mf">0.044</span><span class="w">     </span><span class="mf">0.021</span>
<span class="n">x21</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0130</span><span class="w">      </span><span class="mf">0.076</span><span class="w">     </span><span class="o">-</span><span class="mf">0.170</span><span class="w">      </span><span class="mf">0.865</span><span class="w">        </span><span class="o">-</span><span class="mf">0.164</span><span class="w">     </span><span class="mf">0.138</span>
<span class="n">x22</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0202</span><span class="w">      </span><span class="mf">0.024</span><span class="w">     </span><span class="o">-</span><span class="mf">0.857</span><span class="w">      </span><span class="mf">0.393</span><span class="w">        </span><span class="o">-</span><span class="mf">0.067</span><span class="w">     </span><span class="mf">0.026</span>
<span class="n">x23</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0203</span><span class="w">      </span><span class="mf">0.028</span><span class="w">     </span><span class="o">-</span><span class="mf">0.737</span><span class="w">      </span><span class="mf">0.462</span><span class="w">        </span><span class="o">-</span><span class="mf">0.075</span><span class="w">     </span><span class="mf">0.034</span>
<span class="n">x24</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0023</span><span class="w">      </span><span class="mf">0.004</span><span class="w">     </span><span class="o">-</span><span class="mf">0.608</span><span class="w">      </span><span class="mf">0.544</span><span class="w">        </span><span class="o">-</span><span class="mf">0.010</span><span class="w">     </span><span class="mf">0.005</span>
<span class="n">x25</span><span class="w">            </span><span class="mf">0.0546</span><span class="w">      </span><span class="mf">0.048</span><span class="w">      </span><span class="mf">1.141</span><span class="w">      </span><span class="mf">0.256</span><span class="w">        </span><span class="o">-</span><span class="mf">0.040</span><span class="w">     </span><span class="mf">0.149</span>
<span class="n">x26</span><span class="w">           </span><span class="o">-</span><span class="mf">1.0180</span><span class="w">      </span><span class="mf">0.714</span><span class="w">     </span><span class="o">-</span><span class="mf">1.426</span><span class="w">      </span><span class="mf">0.156</span><span class="w">        </span><span class="o">-</span><span class="mf">2.430</span><span class="w">     </span><span class="mf">0.394</span>
<span class="n">x27</span><span class="w">            </span><span class="mf">0.3371</span><span class="w">      </span><span class="mf">0.203</span><span class="w">      </span><span class="mf">1.664</span><span class="w">      </span><span class="mf">0.098</span><span class="w">        </span><span class="o">-</span><span class="mf">0.064</span><span class="w">     </span><span class="mf">0.738</span>
<span class="n">x28</span><span class="w">            </span><span class="mf">0.1286</span><span class="w">      </span><span class="mf">0.140</span><span class="w">      </span><span class="mf">0.916</span><span class="w">      </span><span class="mf">0.361</span><span class="w">        </span><span class="o">-</span><span class="mf">0.149</span><span class="w">     </span><span class="mf">0.406</span>
<span class="n">x29</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0561</span><span class="w">      </span><span class="mf">0.035</span><span class="w">     </span><span class="o">-</span><span class="mf">1.607</span><span class="w">      </span><span class="mf">0.110</span><span class="w">        </span><span class="o">-</span><span class="mf">0.125</span><span class="w">     </span><span class="mf">0.013</span>
<span class="n">x30</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0535</span><span class="w">      </span><span class="mf">0.020</span><span class="w">     </span><span class="o">-</span><span class="mf">2.645</span><span class="w">      </span><span class="mf">0.009</span><span class="w">        </span><span class="o">-</span><span class="mf">0.093</span><span class="w">    </span><span class="o">-</span><span class="mf">0.013</span>
<span class="n">x31</span><span class="w">            </span><span class="mf">0.1169</span><span class="w">      </span><span class="mf">0.051</span><span class="w">      </span><span class="mf">2.305</span><span class="w">      </span><span class="mf">0.023</span><span class="w">         </span><span class="mf">0.017</span><span class="w">     </span><span class="mf">0.217</span>
<span class="n">x32</span><span class="w">            </span><span class="mf">0.0039</span><span class="w">      </span><span class="mf">0.004</span><span class="w">      </span><span class="mf">1.030</span><span class="w">      </span><span class="mf">0.305</span><span class="w">        </span><span class="o">-</span><span class="mf">0.004</span><span class="w">     </span><span class="mf">0.011</span>
<span class="n">x33</span><span class="w">            </span><span class="mf">0.0179</span><span class="w">      </span><span class="mf">0.055</span><span class="w">      </span><span class="mf">0.324</span><span class="w">      </span><span class="mf">0.746</span><span class="w">        </span><span class="o">-</span><span class="mf">0.091</span><span class="w">     </span><span class="mf">0.127</span>
<span class="n">x34</span><span class="w">            </span><span class="mf">0.0081</span><span class="w">      </span><span class="mf">0.013</span><span class="w">      </span><span class="mf">0.632</span><span class="w">      </span><span class="mf">0.529</span><span class="w">        </span><span class="o">-</span><span class="mf">0.017</span><span class="w">     </span><span class="mf">0.033</span>
<span class="n">x35</span><span class="w">            </span><span class="mf">0.0013</span><span class="w">      </span><span class="mf">0.006</span><span class="w">      </span><span class="mf">0.229</span><span class="w">      </span><span class="mf">0.819</span><span class="w">        </span><span class="o">-</span><span class="mf">0.010</span><span class="w">     </span><span class="mf">0.013</span>
<span class="n">x36</span><span class="w">           </span><span class="o">-</span><span class="mf">0.0068</span><span class="w">      </span><span class="mf">0.007</span><span class="w">     </span><span class="o">-</span><span class="mf">1.045</span><span class="w">      </span><span class="mf">0.298</span><span class="w">        </span><span class="o">-</span><span class="mf">0.020</span><span class="w">     </span><span class="mf">0.006</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span><span class="w">                        </span><span class="mf">2.969</span><span class="w">   </span><span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span><span class="w">                   </span><span class="mf">2.098</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span><span class="w">                  </span><span class="mf">0.227</span><span class="w">   </span><span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span><span class="w"> </span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                </span><span class="mf">2.526</span>
<span class="n">Skew</span><span class="p">:</span><span class="w">                           </span><span class="mf">0.236</span><span class="w">   </span><span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span><span class="w">                        </span><span class="mf">0.283</span>
<span class="n">Kurtosis</span><span class="p">:</span><span class="w">                       </span><span class="mf">3.357</span><span class="w">   </span><span class="n">Cond</span><span class="o">.</span><span class="w"> </span><span class="n">No</span><span class="o">.</span><span class="w">                     </span><span class="mf">6.96e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="n">Standard</span><span class="w"> </span><span class="n">Errors</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">covariance</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">errors</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correctly</span><span class="w"> </span><span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">large</span><span class="p">,</span><span class="w"> </span><span class="mf">6.96e+05</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">strong</span><span class="w"> </span><span class="n">multicollinearity</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">problems</span><span class="o">.</span>
</code></pre></div></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Dan Vatterott
    </span>
  </span>
<time datetime="2016-03-20T15:56:18-04:00" pubdate>Sun 20 March 2016</time>  <span class="categories">
    <a class='category' href='https://danvatterott.com/category/nba.html'>nba</a>
  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">


<!-- <section> -->
    <section style="max-width: fit-content; margin-inline: auto;">

            <span class="fa-stack fa-lg">
                <a href="mailto:dvatterott@gmail.com"><i class="fa fa-envelope fa-1x"></i></a>
            </span>
            <span class="fa-stack fa-lg">
                <a href="http://www.linkedin.com/in/dan-vatterott"><i class="fa fa-linkedin fa-1x"></i></a>
            </span>

            <span class="fa-stack fa-lg">
                <a href="https://twitter.com/dvatterott"><i class="fa fa-twitter fa-1x"></i></a>
            </span>

            <span class="fa-stack fa-lg">
                <a href="https://github.com/dvatterott"><i class="fa fa-github fa-1x"></i></a>
            </span>
            <span class="fa-stack fa-lg">
                <a href="https://scholar.google.com/citations?hl=en&user=-S7mhDQAAAAJ&hl"><i class="fa fa-graduation-cap fa-1x"></i></a>
            </span>

            <!-- <h1>Social</h1>
                 <ul>
                 <li><a href="dvatterott@gmail.com" target="_blank">email</a></li>
                 <li><a href="http://www.linkedin.com/in/dan-vatterott" target="_blank">linkedin</a></li>
                 <li><a href="https://twitter.com/dvatterott" target="_blank">twitter</a></li>
                 <li><a href="https://github.com/dvatterott" target="_blank">github</a></li>
                 <li><a href="https://scholar.google.com/citations?hl=en&user=-S7mhDQAAAAJ&hl" target="_blank">google-scholar</a></li>


                 </ul> -->
    </section>


  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://danvatterott.com/modeling-the-relative-speed-of-hot-wheels-cars.html">Modeling the relative speed of Hot Wheels Cars</a>
      </li>
      <li class="post">
          <a href="https://danvatterott.com/data-onboarding-checklist.html">Data Onboarding Checklist</a>
      </li>
      <li class="post">
          <a href="https://danvatterott.com/posting-collections-as-hive-tables.html">Posting Collections as Hive Tables</a>
      </li>
      <li class="post">
          <a href="https://danvatterott.com/balancing-model-weights-in-pyspark.html">Balancing Model Weights in PySpark</a>
      </li>
      <li class="post">
          <a href="https://danvatterott.com/creating-a-cdf-in-pyspark.html">Creating a CDF in PySpark</a>
      </li>
    </ul>
  </section>
</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  Dan Vatterott &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="https://danvatterott.com/theme/js/modernizr-2.0.js"></script>
  <script src="https://danvatterott.com/theme/js/ender.js"></script>
  <script src="https://danvatterott.com/theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'danvatterott';
    var disqus_identifier = '/predicting-career-performance-from-rookie-performance.html';
    var disqus_url = 'https://danvatterott.com/predicting-career-performance-from-rookie-performance.html';
    var disqus_title = 'Predicting Career Performance from Rookie Performance';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>