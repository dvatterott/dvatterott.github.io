<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>PCA Tutorial &mdash; Dan Vatterott</title>
  <meta name="author" content="Dan Vatterott">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">Dan Vatterott</a></h1>
    <h2>Data Scientist</h2>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>


<ul class="main-navigation">
    <li><a href="/">Home</a></li>
    <li><a href="/about-me.html">About Me</a></li>
    <li><a href="/publications.html">Publications</a></li>
    <li><a href="/Vatterott_Resume.pdf">Resume</a></li>
    <li><a href="/my-reads.html">My Reads</a></li>
    <li><a href="/presentations.html">Presentations</a></li>
    <li><a href="/blog.html">Blog</a></li>
    <li><a href="/archives.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">PCA Tutorial</h1>
    <p class="meta">
<time datetime="2016-11-06T13:33:50-05:00" pubdate>Sun 06 November 2016</time>    </p>
</header>

  <div class="entry-content"><p><a href="http://setosa.io/ev/principal-component-analysis/">Principal Component Analysis</a> (PCA) is an important method for dimensionality reduction and data cleaning. I have used PCA in the past on this blog for estimating the latent variables that underlie player statistics. For example, I might have two features: average number of offensive rebounds and average number of defensive rebounds. The two features are highly correlated because a latent variable, the player's <em>rebounding ability</em>, explains common variance in the two features. PCA is a method for extracting these latent variables that explain common variance across features.</p>
<p>In this tutorial I generate fake data in order to help gain insight into the mechanics underlying PCA.</p>
<p>Below I create my first feature by sampling from a normal distribution. I create a second feature by adding a noisy normal distribution to the first feature multiplied by two. Because I generated the data here, I know it's composed to two latent variables, and PCA should be able to identify these latent variables.</p>
<p>I generate the data and plot it below.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span><span class="o">,</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="w"> </span><span class="o">%</span><span class="n">matplotlib</span><span class="w"> </span><span class="n">inline</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="c1">#make sure we&#39;re all working with the same numbers</span>

<span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,[</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="o">*</span><span class="mi">2</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">8.0</span><span class="p">,[</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">])]</span>
<span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Raw Data&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">30</span><span class="p">,</span><span class="mi">30</span><span class="p">]);</span>
</code></pre></div>

<p><img src="/images/PCA/original_data.png" /></p>
<p>The first step before doing PCA is to normalize the data. This centers each feature (each feature will have a mean of 0) and divides data by its standard deviation (changing the standard deviation to 1). Normalizing the data puts all features on the same scale. Having features on the same scale is important because features might be more or less variable because of measurement rather than the latent variables producing the feature. For example, in basketball, points are often accumulated in sets of 2s and 3s, while rebounds are accumulated one at a time. The nature of basketball puts points and rebounds on a different scales, but this doesn't mean that the latent variables <em>scoring ability</em> and <em>rebounding ability</em> are more or less variable.</p>
<p>Below I normalize and plot the data.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">stats</span>

<span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">mstats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Standardized Data&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]);</span>
</code></pre></div>

<p><img src="/images/PCA/stand_data.png" /></p>
<p>After standardizing the data, I need to find the <a href="http://setosa.io/ev/eigenvectors-and-eigenvalues/">eigenvectors and eigenvalues</a>. The eigenvectors point in the direction of a component and eigenvalues represent the amount of variance explained by the component. Below, I plot the standardized data with the eigenvectors ploted with their eigenvalues as the vectors distance from the origin.</p>
<p>As you can see, the blue eigenvector is longer and points in the direction with the most variability. The purple eigenvector is shorter and points in the direction with less variability.</p>
<p>As expected, one component explains far more variability than the other component (becaus both my features share variance from a single latent gaussian distribution).</p>
<div class="highlight"><pre><span></span><code> C = np.dot(X,np.transpose(X))/(np.shape(X)[1]-1);
 [V,PC] = np.linalg.eig(C)

 plt.plot(X[0],X[1],&#39;o&#39;)
 plt.plot([0,PC[0,0]*V[0]],[0,PC[1,0]*V[0]],&#39;o-&#39;)
 plt.plot([0,PC[0,1]*V[1]],[0,PC[1,1]*V[1]],&#39;o-&#39;)
 plt.xlabel(&#39;Feature 1&#39;)
 plt.ylabel(&#39;Feature 2&#39;)
 plt.title(&#39;Standardized Data with Eigenvectors&#39;)
 plt.axis([-4,4,-4,4]);
</code></pre></div>

<p><img src="/images/PCA/eigen_data.png" /></p>
<p>Next I order the eigenvectors according to the magnitude of their eigenvalues. This orders the components so that the components that explain more variability occur first. I then transform the data so that they're axis aligned. This means the first component explain variability on the x-axis and the second component explains variance on the y-axis.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="n">indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">V</span><span class="p">)</span>
<span class="w"> </span><span class="n">V</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">V</span><span class="o">[</span><span class="n">indices</span><span class="o">]</span>
<span class="w"> </span><span class="n">PC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PC</span><span class="o">[</span><span class="n">indices,:</span><span class="o">]</span>

<span class="w"> </span><span class="n">X_rotated</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">PC</span><span class="p">)</span>

<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_rotated</span><span class="p">.</span><span class="n">T</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">,</span><span class="n">X_rotated</span><span class="p">.</span><span class="n">T</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="p">,</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="o">[</span><span class="n">0,PC[1,0</span><span class="o">]*</span><span class="n">V</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="o">[</span><span class="n">0,0</span><span class="o">]</span><span class="p">,</span><span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="o">[</span><span class="n">0,0</span><span class="o">]</span><span class="p">,</span><span class="o">[</span><span class="n">0,PC[1,1</span><span class="o">]*</span><span class="n">V</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Data Projected into PC space&#39;</span><span class="p">)</span>
<span class="w"> </span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="o">[</span><span class="n">-4,4,-4,4</span><span class="o">]</span><span class="p">);</span>
</code></pre></div>

<p><img src="/images/PCA/trans_data.png" /></p>
<p>Finally, just to make sure the PCA was done correctly, I will call PCA from the sklearn library, run it, and make sure it produces the same results as my analysis.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="n">PCA</span>

<span class="w"> </span><span class="n">pca</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PCA</span><span class="p">()</span><span class="w"> </span><span class="c1">#create PCA object</span>
<span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="w"> </span><span class="c1">#pull out principle components</span>

<span class="w"> </span><span class="nb">print</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">X_rotated</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">test</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="w"> </span><span class="nb">print</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">X_rotated</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">test</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">)</span>
<span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Dan Vatterott
    </span>
  </span>
<time datetime="2016-11-06T13:33:50-05:00" pubdate>Sun 06 November 2016</time>  <span class="categories">
    <a class='category' href='/category/python.html'>python</a>
  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">


<!-- <section> -->
    <section style="max-width: fit-content; margin-inline: auto;">

            <span class="fa-stack fa-lg">
                <a href="mailto:dvatterott@gmail.com"><i class="fa fa-envelope fa-1x"></i></a>
            </span>
            <span class="fa-stack fa-lg">
                <a href="http://www.linkedin.com/in/dan-vatterott"><i class="fa fa-linkedin fa-1x"></i></a>
            </span>

            <span class="fa-stack fa-lg">
                <a href="https://twitter.com/dvatterott"><i class="fa fa-twitter fa-1x"></i></a>
            </span>

            <span class="fa-stack fa-lg">
                <a href="https://github.com/dvatterott"><i class="fa fa-github fa-1x"></i></a>
            </span>
            <span class="fa-stack fa-lg">
                <a href="https://scholar.google.com/citations?hl=en&user=-S7mhDQAAAAJ&hl"><i class="fa fa-graduation-cap fa-1x"></i></a>
            </span>

            <!-- <h1>Social</h1>
                 <ul>
                 <li><a href="dvatterott@gmail.com" target="_blank">email</a></li>
                 <li><a href="http://www.linkedin.com/in/dan-vatterott" target="_blank">linkedin</a></li>
                 <li><a href="https://twitter.com/dvatterott" target="_blank">twitter</a></li>
                 <li><a href="https://github.com/dvatterott" target="_blank">github</a></li>
                 <li><a href="https://scholar.google.com/citations?hl=en&user=-S7mhDQAAAAJ&hl" target="_blank">google-scholar</a></li>


                 </ul> -->
    </section>


  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/modeling-the-relative-speed-of-hot-wheels-cars.html">Modeling the relative speed of Hot Wheels Cars</a>
      </li>
      <li class="post">
          <a href="/data-onboarding-checklist.html">Data Onboarding Checklist</a>
      </li>
      <li class="post">
          <a href="/posting-collections-as-hive-tables.html">Posting Collections as Hive Tables</a>
      </li>
      <li class="post">
          <a href="/balancing-model-weights-in-pyspark.html">Balancing Model Weights in PySpark</a>
      </li>
      <li class="post">
          <a href="/creating-a-cdf-in-pyspark.html">Creating a CDF in PySpark</a>
      </li>
    </ul>
  </section>
</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  Dan Vatterott &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'danvatterott';
    var disqus_identifier = '/pca-tutorial.html';
    var disqus_url = '/pca-tutorial.html';
    var disqus_title = 'PCA Tutorial';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>